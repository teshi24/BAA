@inproceedings{christen_exogenous_2020,
	address = {Budapest, Hungary},
	title = {Exogenous {Data} for {Load} {Forecasting}: {A} {Review}:},
	copyright = {All rights reserved},
	isbn = {978-989-758-475-6},
	shorttitle = {Exogenous {Data} for {Load} {Forecasting}},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0010213204890500},
	doi = {10.5220/0010213204890500},
	language = {en},
	urldate = {2020-11-16},
	booktitle = {Proceedings of the 12th {International} {Joint} {Conference} on {Computational} {Intelligence}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Christen, Ramón and Mazzola, Luca and Denzler, Alexander and Portmann, Edy},
	year = {2020},
	keywords = {phd.mono.cited},
	pages = {489--500},
}
@InProceedings{Gottfrois2024, % was before: 10.1007/978-3-031-72384-1_66,
	author = "Gottfrois, Philippe and Gröger, Fabian and Andriambololoniaina, Faly Herizo and Amruthalingam, Ludovic and Gonzalez-Jimenez, Alvaro and Hsu, Christophe and Kessy, Agnes and Lionetti, Simone and Mavura, Daudi and Ng'ambi, Wingston and Ngongonda, Dingase Faith and Pouly, Marc and Rakotoarisaona, Mendrika Fifaliana and Rapelanoro Rabenja, Fahafahantsoa and Traoré, Ibrahima and Navarini, Alexander A.",
	title = "PASSION for Dermatology: Bridging the Diversity Gap with Pigmented Skin Images from Sub-Saharan Africa",
	booktitle = "Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024",
	year = "2024",
	publisher = "Springer Nature Switzerland",
	address = "Cham",
	pages = "703--712", 
	isbn = "978-3-031-72384-1"
}
@article{Diaz2022,
	author = "Diaz, Michael and Lucke-Wold, Brandon and Batchu, Sai and Kleinberg, Giona",
	year = "2022",
	month = "01",
	pages = "42-47",
	title = "Racial underrepresentation in dermatological datasets leads to biased machine learning models and inequitable healthcare",
	volume = "3"
}
% Link: https://www.researchgate.net/publication/366481389_Racial_underrepresentation_in_dermatological_datasets_leads_to_biased_machine_learning_models_and_inequitable_healthcare
% Abstract: Objective: Clinical applications of machine learning are promising as a tool to improve patient outcomes through assisting diagnoses, treatment, and analyzing risk factors for screening. Possible clinical applications are especially prominent in dermatology as many diseases and conditions present visually. This allows a machine learning model to analyze and diagnose conditions using patient images and data from electronic health records (EHRs) after training on clinical datasets but could also introduce bias. Despite promising applications, artificial intelligence has the capacity to exacerbate existing demographic disparities in healthcare if models are trained on biased datasets. Methods: Through systematic literature review of available literature, we highlight the extent of bias present in clinical datasets as well as the implications it could have on healthcare if not addressed. Results: We find the implications are worsened in dermatological models. Despite the severity and complexity of melanoma and other dermatological diseases as well as differing disease presentations based on skin-color, many imaging datasets underrepresent certain demographic groups causing machine learning models to train on images of primarily fair-skinned individuals leaving minorities behind. Conclusion: In order to address this disparity, research first needs to be done investigating the extent of the bias present and the implications it may have on equitable healthcare.
@online{BAD2021,
	title = "Lower socioeconomic status linked with more severe skin disease, including melanoma",
	url = "https://www.skinhealthinfo.org.uk/lower-socioeconomic-status-linked-with-more-severe-skin-disease-including-melanoma/",
	abstract = "\%",
	titleaddon = "BAD Patient Hub",
	author = "{British Association of Dermatologists (BAD)}",
	urldate = "2025-02-17",
	date = "2021-07-07",
	langid = "english",
	note = "Research was presented at the BAD's Annual Meeting."
}
@article{Mehrabi_2021,
	title = "A Survey on Bias and Fairness in Machine Learning",
	url = "https://dl.acm.org/doi/10.1145/34576072",
	doi = "10.1145/3457607",
	abstract = "With the widespread use of artificial intelligence ({AI}) systems and applications in
	our everyday lives, accounting for fairness has gained significant importance in designing
	and engineering of such systems. {AI} systems can be used in many sensitive ...",
	journaltitle = "{ACM} Computing Surveys ({CSUR})",
	author = "Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram",
	urldate = "2025-02-28",
	date = "2021-07-13",
	note = "Publisher: {ACMPUB}27New York, {NY}, {USA}"
}
%% mehrabi citations
@article{M9_Baeza-Yates_2018,
	title = {Bias on the web},
	volume = {61},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3209581},
	doi = {10.1145/3209581},
	abstract = {Bias in Web data and use taints the algorithms behind Web-based applications, delivering equally biased results.},
	pages = {54--61},
	number = {6},
	journaltitle = {Commun. {ACM}},
	author = {Baeza-Yates, Ricardo},
	urldate = {2025-03-16},
	date = {2018-05-23},
	annotation = {Mehrabi 9
	},
}

@inproceedings{M24_Buolamwini_2018,
	title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	shorttitle = {Gender Shades},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, {IJB}-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for {IJB}-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	eventtitle = {Conference on Fairness, Accountability and Transparency},
	pages = {77--91},
	booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	publisher = {{PMLR}},
	author = {Buolamwini, Joy and Gebru, Timnit},
	urldate = {2025-03-16},
	date = {2018-01-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	annotation = {Mehrabi 24, demographic (skin type and gender)
	},
}

@article{M38_Clarke_2005,
	title = {The Phantom Menace: Omitted Variable Bias in Econometric Research},
	volume = {22},
	issn = {0738-8942},
	url = {https://doi.org/10.1080/07388940500339183},
	doi = {10.1080/07388940500339183},
	shorttitle = {The Phantom Menace},
	abstract = {Quantitative political science is awash in control variables. The justification for these bloated specifications is usually the fear of omitted variable bias. A key underlying assumption is that the danger posed by omitted variable bias can be ameliorated by the inclusion of relevant control variables. Unfortunately, as this article demonstrates, there is nothing in the mathematics of regression analysis that supports this conclusion. The inclusion of additional control variables may increase or decrease the bias, and we cannot know for sure which is the case in any particular situation. A brief discussion of alternative strategies for achieving experimental control follows the main result.},
	pages = {341--352},
	number = {4},
	journaltitle = {Conflict Management and Peace Science},
	author = {Clarke, Kevin A.},
	urldate = {2025-03-16},
	date = {2005-09-01},
	note = {Publisher: {SAGE} Publications Ltd},
	annotation = {Mehrabi 38, difficultis regarding ommitted variable and overcoming methods
	},
}

@inproceedings{M44_Danks_2017,
	location = {Melbourne, Australia},
	title = {Algorithmic Bias in Autonomous Systems},
	isbn = {978-0-9992411-0-3},
	url = {https://www.ijcai.org/proceedings/2017/654},
	doi = {10.24963/ijcai.2017/654},
	abstract = {Algorithms play a key role in the functioning of autonomous systems, and so concerns have periodically been raised about the possibility of algorithmic bias. However, debates in this area have been hampered by different meanings and uses of the term, “bias.” It is sometimes used as a purely descriptive term, sometimes as a pejorative term, and such variations can promote confusion and hamper discussions about when and how to respond to algorithmic bias. In this paper, we first provide a taxonomy of different types and sources of algorithmic bias, with a focus on their different impacts on the proper functioning of autonomous systems. We then use this taxonomy to distinguish between algorithmic biases that are neutral or unobjectionable, and those that are problematic in some way and require a response. In some cases, there are technological or algorithmic adjustments that developers can use to compensate for problematic bias. In other cases, however, responses require adjustments by the agent, whether human or autonomous system, who uses the results of the algorithm. There is no “one size fits all” solution to algorithmic bias.},
	eventtitle = {Twenty-Sixth International Joint Conference on Artificial Intelligence},
	pages = {4691--4697},
	booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Danks, David and London, Alex John},
	urldate = {2025-03-16},
	date = {2017-08},
	langid = {english},
	annotation = {Mehrabi 44
	},
}

@article{M53_Friedman_1996,
	title = {Bias in computer systems},
	volume = {14},
	issn = {1046-8188},
	url = {https://dl.acm.org/doi/10.1145/230538.230561},
	doi = {10.1145/230538.230561},
	abstract = {From an analysis of actual cases, three categories of bias in computer systems have been developed: preexisting, technical, and emergent. Preexisting bias has its roots in social institutions, practices, and attitudes. Technical bias arises from technical constraints of considerations. Emergent bias arises in a context of use. Although others have pointed to bias inparticular computer systems and have noted the general problem, we know of no comparable work that examines this phenomenon comprehensively and which offers a framework for understanding and remedying it. We conclude by suggesting that freedom from bias should by counted amoung the select set of criteria—including reliability, accuracy, and efficiency—according to which the quality of systems in use in society should be judged.},
	pages = {330--347},
	number = {3},
	journaltitle = {{ACM} Trans. Inf. Syst.},
	author = {Friedman, Batya and Nissenbaum, Helen},
	urldate = {2025-03-16},
	date = {1996-07-01},
	annotation = {Mehrabi 53
	},
}

@article{M64_Hargittai_2007,
	title = {Whose Space? Differences among Users and Non-Users of Social Network Sites},
	volume = {13},
	issn = {1083-6101},
	url = {https://doi.org/10.1111/j.1083-6101.2007.00396.x},
	doi = {10.1111/j.1083-6101.2007.00396.x},
	shorttitle = {Whose Space?},
	abstract = {Are there systematic differences between people who use social network sites and those who stay away, despite a familiarity with them? Based on data from a survey administered to a diverse group of young adults, this article looks at the predictors of {SNS} usage, with particular focus on Facebook, {MySpace}, Xanga, and Friendster. Findings suggest that use of such sites is not randomly distributed across a group of highly wired users. A person’s gender, race and ethnicity, and parental educational background are all associated with use, but in most cases only when the aggregate concept of social network sites is disaggregated by service. Additionally, people with more experience and autonomy of use are more likely to be users of such sites. Unequal participation based on user background suggests that differential adoption of such services may be contributing to digital inequality.},
	pages = {276--297},
	number = {1},
	journaltitle = {Journal of Computer-Mediated Communication},
	author = {Hargittai, Eszter},
	urldate = {2025-03-16},
	date = {2007-10-01},
	annotation = {Mehrabi 64
	},
}

@article{M93_Lerman_2014,
	title = {Leveraging Position Bias to Improve Peer Recommendation},
	volume = {9},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0098914},
	doi = {10.1371/journal.pone.0098914},
	abstract = {With the advent of social media and peer production, the amount of new online content has grown dramatically. To identify interesting items in the vast stream of new content, providers must rely on peer recommendation to aggregate opinions of their many users. Due to human cognitive biases, the presentation order strongly affects how people allocate attention to the available content. Moreover, we can manipulate attention through the presentation order of items to change the way peer recommendation works. We experimentally evaluate this effect using Amazon Mechanical Turk. We find that different policies for ordering content can steer user attention so as to improve the outcomes of peer recommendation.},
	pages = {e98914},
	number = {6},
	journaltitle = {{PLOS} {ONE}},
	author = {Lerman, Kristina and Hogg, Tad},
	urldate = {2025-03-16},
	date = {2014-06-11},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Arithmetic, Attention, Intelligence, Permutation, Psychological attitudes, Social influence, Social media, Social systems},
	annotation = {Mehrabi 93
	},
}

@article{M114_Mustard_2003,
	title = {Reexamining Criminal Behavior: The Importance of Omitted Variable Bias},
	volume = {85},
	issn = {0034-6535},
	url = {https://doi.org/10.1162/rest.2003.85.1.205},
	doi = {10.1162/rest.2003.85.1.205},
	shorttitle = {Reexamining Criminal Behavior},
	abstract = {Recently many papers have used the arrest rate to measure punishments in crime-rate regressions. However, arrest rates account for only a portion of the criminal sanction. Conviction rates and time served are theoretically important, but rarely used, and excluding them generates omitted variable bias if they are correlated with the arrest rate. This paper uses the most complete set of conviction and sentencing data to show that arrest rates are negatively correlated with these normally excluded variables. Consequently, previous estimates of arrest-rate effects are understated by as much as 50\%. Also, conviction rates, but not sentence lengths, have significant explanatory power in standard crime-rate regressions.},
	pages = {205--211},
	number = {1},
	journaltitle = {The Review of Economics and Statistics},
	author = {Mustard, David B.},
	urldate = {2025-03-16},
	date = {2003-02-01},
	annotation = {Mehrabi 114
	},
}

@article{M117_Ciampaglia_2018,
	title = {How algorithmic popularity bias hinders or promotes quality},
	volume = {8},
	rights = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-34203-2},
	doi = {10.1038/s41598-018-34203-2},
	abstract = {Algorithms that favor popular items are used to help us select among many choices, from top-ranked search engine results to highly-cited scientific papers. The goal of these algorithms is to identify high-quality items such as reliable news, credible information sources, and important discoveries–in short, high-quality content should rank at the top. Prior work has shown that choosing what is popular may amplify random fluctuations and lead to sub-optimal rankings. Nonetheless, it is often assumed that recommending what is popular will help high-quality content “bubble up” in practice. Here we identify the conditions in which popularity may be a viable proxy for quality content by studying a simple model of a cultural market endowed with an intrinsic notion of quality. A parameter representing the cognitive cost of exploration controls the trade-off between quality and popularity. Below and above a critical exploration cost, popularity bias is more likely to hinder quality. But we find a narrow intermediate regime of user attention where an optimal balance exists: choosing what is popular can help promote high-quality items to the top. These findings clarify the effects of algorithmic popularity bias on quality outcomes, and may inform the design of more principled mechanisms for techno-social cultural markets.},
	pages = {15951},
	number = {1},
	journaltitle = {Sci Rep},
	author = {Ciampaglia, Giovanni Luca and Nematzadeh, Azadeh and Menczer, Filippo and Flammini, Alessandro},
	urldate = {2025-03-16},
	date = {2018-10-29},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Complex networks, Statistics},
	annotation = {Mehrabi 117
	}
}

@article{M120_Olteanu_2019,
	title = {Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries},
	volume = {2},
	issn = {2624-909X},
	url = {https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2019.00013/full},
	doi = {10.3389/fdata.2019.00013},
	shorttitle = {Social Data},
	abstract = {{\textless}p{\textgreater}Social data in digital form—including user-generated content, expressed or implicit relations between people, and behavioral traces—are at the core of popular applications and platforms, driving the research agenda of many researchers. The promises of social data are many, including understanding “what the world thinks” about a social issue, brand, celebrity, or other entity, as well as enabling better decision-making in a variety of fields including public policy, healthcare, and economics. Many academics and practitioners have warned against the naïve usage of social data. There are biases and inaccuracies occurring at the source of the data, but also introduced during processing. There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked. This paper recognizes the rigor with which these issues are addressed by different researchers varies across a wide range. We identify a variety of menaces in the practices around social data use, and organize them in a framework that helps to identify them.{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}disp-quote{\textgreater}{\textless}p{\textgreater}“{\textless}italic{\textgreater}For your own sanity, you have to remember that not all problems can be solved. Not all problems can be solved, but all problems can be illuminated.” –Ursula Franklin{\textless}/italic{\textgreater}{\textless}xref ref-type="fn" rid="fn0001"{\textgreater}$^{\textrm{1}}${\textless}/xref{\textgreater}{\textless}/p{\textgreater}{\textless}/disp-quote{\textgreater}{\textless}/p{\textgreater}},
	journaltitle = {Front. Big Data},
	author = {Olteanu, Alexandra and Castillo, Carlos and Diaz, Fernando and Kıcıman, Emre},
	urldate = {2025-03-16},
	date = {2019-07-11},
	note = {Publisher: Frontiers},
	keywords = {biases, Ethics, Evaluation, Social Media, User data},
	annotation = {Mehrabi 120
	}
}

@article{M131_Riegg_2008,
	title = {Causal Inference and Omitted Variable Bias in Financial Aid Research: Assessing Solutions},
	volume = {31},
	issn = {1090-7009},
	url = {https://muse.jhu.edu/pub/1/article/232773},
	shorttitle = {Causal Inference and Omitted Variable Bias in Financial Aid Research},
	abstract = {, This article highlights the problem of omitted variable bias in research on the causal effect of financial aid on college‑going. I first describe the problem of self‑selection and the resulting bias from omitted variables. I then assess and explore the strengths and weaknesses of random assignment, multivariate regression, proxy variables, fixed effects, difference‑in‑differences, regression discontinuity, and instrumental variables techniques in addressing the problem. I focus on the intuition, assumptions, and applications of each method in the context of the same research question, providing practical guidance for researchers interested in implementing these approaches.},
	pages = {329--354},
	number = {3},
	journaltitle = {The Review of Higher Education},
	author = {Riegg, Stephanie K.},
	urldate = {2025-03-16},
	date = {2008},
	note = {Publisher: Johns Hopkins University Press},
	annotation = {Mehrabi 131
	},
}

@inproceedings{M144_Suresh_2021,
	location = {New York, {NY}, {USA}},
	title = {A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle},
	isbn = {978-1-4503-8553-4},
	url = {https://dl.acm.org/doi/10.1145/3465416.3483305},
	doi = {10.1145/3465416.3483305},
	series = {{EAAMO} '21},
	abstract = {As machine learning ({ML}) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the {ML} life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.},
	pages = {1--9},
	booktitle = {Proceedings of the 1st {ACM} Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
	publisher = {Association for Computing Machinery},
	author = {Suresh, Harini and Guttag, John},
	urldate = {2025-03-16},
	date = {2021-11-04},
	annotation = {Mehrabi 144
	}
}

@article{M151_Wang_2014,
	title = {Why Amazon's Ratings Might Mislead You: The Story of Herding Effects},
	volume = {2},
	issn = {2167-6461},
	url = {https://www.liebertpub.com/doi/full/10.1089/big.2014.0063},
	doi = {10.1089/big.2014.0063},
	shorttitle = {Why Amazon's Ratings Might Mislead You},
	abstract = {Our society is increasingly relying on digitalized, aggregated opinions of individuals to make decisions (e.g., product recommendation based on collective ratings). One key requirement of harnessing this “wisdom of crowd” is the independency of individuals' opinions; yet, in real settings, collective opinions are rarely simple aggregations of independent minds. Recent experimental studies document that disclosing prior collective ratings distorts individuals' decision making as well as their perceptions of quality and value, highlighting a fundamental discrepancy between our perceived values from collective ratings and products' intrinsic values. Here we present a mechanistic framework to describe herding effects of prior collective ratings on subsequent individual decision making. Using large-scale longitudinal customer rating datasets, we find that our method successfully captures the dynamics of ratings growth, helping us separate social influence bias from inherent values. Leveraging the proposed framework, we quantitatively characterize the herding effects existing in product rating systems and promote strategies to untangle manipulations and social biases.},
	pages = {196--204},
	number = {4},
	journaltitle = {Big Data},
	author = {Wang, Ting and Wang, Dashun},
	urldate = {2025-03-16},
	date = {2014-12},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	annotation = {Mehrabi 151
	}
}


% Mehrabi Fairness
@article{M15_Berk_2017,
	title = {Fairness in Criminal Justice Risk Assessments: The State of the Art},
	volume = {50},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124118782533},
	doi = {10.1177/0049124118782533},
	shorttitle = {Fairness in Criminal Justice Risk Assessments},
	abstract = {Objectives:Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this article, we seek to clarify the trade-offs between different kinds of fairness and between fairness and accuracy.Methods:We draw on the existing literatures in criminology, computer science, and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments.Results:We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy.Conclusions:Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging trade-offs. These lessons apply to applications well beyond criminology where assessments of risk can be used by decision makers. Examples include mortgage lending, employment, college admissions, child welfare, and medical diagnoses.},
	pages = {3--44},
	number = {1},
	journaltitle = {Sociological Methods \& Research},
	author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Kearns, Michael and Roth, Aaron},
	urldate = {2025-03-16},
	date = {2017},
	note = {Publisher: {SAGE} Publications Inc},
	annotation = {Mehrabi 15
	},
}

@article{M34_Chouldechova_2017,
	title = {Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments},
	volume = {5},
	issn = {2167-6461},
	url = {https://www.liebertpub.com/doi/abs/10.1089/big.2016.0047},
	doi = {10.1089/big.2016.0047},
	shorttitle = {Fair Prediction with Disparate Impact},
	abstract = {Recidivism prediction instruments ({RPIs}) provide decision-makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. Although such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This article discusses several fairness criteria that have recently been applied to assess the fairness of {RPIs}. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when an {RPI} fails to satisfy the criterion of error rate balance.},
	pages = {153--163},
	number = {2},
	journaltitle = {Big Data},
	author = {Chouldechova, Alexandra},
	urldate = {2025-03-16},
	date = {2017-06},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	annotation = {Mehrabi 34
	},
}

@inproceedings{M41_Corbett-Davies_2017,
	location = {New York, {NY}, {USA}},
	title = {Algorithmic Decision Making and the Cost of Fairness},
	isbn = {978-1-4503-4887-4},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098095},
	doi = {10.1145/3097983.3098095},
	series = {{KDD} '17},
	abstract = {Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally differ, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-off can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.},
	pages = {797--806},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
	urldate = {2025-03-16},
	date = {2017-08-04},
	annotation = {Mehrabi 41
	},
}

@inproceedings{M48_Dwork_2012,
	location = {New York, {NY}, {USA}},
	title = {Fairness through awareness},
	isbn = {978-1-4503-1115-1},
	url = {https://dl.acm.org/doi/10.1145/2090236.2090255},
	doi = {10.1145/2090236.2090255},
	series = {{ITCS} '12},
	abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	pages = {214--226},
	booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
	publisher = {Association for Computing Machinery},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	urldate = {2025-03-16},
	date = {2012-01-08},
	annotation = {Mehrabi 48
	},
}

@inproceedings{M50_Farnadi_2018,
	location = {New York, {NY}, {USA}},
	title = {Fairness in Relational Domains},
	isbn = {978-1-4503-6012-8},
	url = {https://dl.acm.org/doi/10.1145/3278721.3278733},
	doi = {10.1145/3278721.3278733},
	series = {{AIES} '18},
	abstract = {{AI} and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic ({PSL}), to incorporate our definition of relational fairness. We refer to this fairness-aware framework {FairPSL}. {FairPSL} makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori({MAP}) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.},
	pages = {108--114},
	booktitle = {Proceedings of the 2018 {AAAI}/{ACM} Conference on {AI}, Ethics, and Society},
	publisher = {Association for Computing Machinery},
	author = {Farnadi, Golnoosh and Babaki, Behrouz and Getoor, Lise},
	urldate = {2025-03-16},
	date = {2018-12-27},
	annotation = {Mehrabi 50
	},
}

@article{M61_Grgic-Hlaca_2016,
	title = {The Case for Process Fairness in Learning: Feature Selection for Fair Decision Making},
	abstract = {Machine learning methods are increasingly being used to inform, or sometimes even directly to make, important decisions about humans. A number of recent works have focussed on the fairness of the outcomes of such decisions, particularly on avoiding decisions that affect users of different sensitive groups (e.g., race, gender) disparately. In this paper, we propose to consider the fairness of the process of decision making. Process fairness can be measured by estimating the degree to which people consider various features to be fair to use when making an important legal decision. We examine the task of predicting whether or not a prisoner is likely to commit a crime again once released by analyzing the dataset considered by {ProPublica} relating to the {COMPAS} system. We introduce new measures of people’s discomfort with using various features, show how these measures can be estimated, and consider the effect of removing the uncomfortable features on prediction accuracy and on outcome fairness. Our empirical analysis suggests that process fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.},
	author = {Grgic-Hlača, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
	date = {2016},
	langid = {english},
	annotation = {Mehrabi 61
	},
}

@inproceedings{M63_Hardt_2016,
	title = {Equality of Opportunity in Supervised Learning},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html},
	abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
	urldate = {2025-03-16},
	date = {2016},
	annotation = {Mehrabi 63
	},
}

@inproceedings{M79_Kearns_2018,
	title = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
	url = {https://proceedings.mlr.press/v80/kearns18a.html},
	shorttitle = {Preventing Fairness Gerrymandering},
	abstract = {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.},
	eventtitle = {International Conference on Machine Learning},
	pages = {2564--2572},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	urldate = {2025-03-16},
	date = {2018-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	annotation = {Mehrabi 79
	},
}

@inproceedings{M80_Kearns_2019,
	location = {New York, {NY}, {USA}},
	title = {An Empirical Study of Rich Subgroup Fairness for Machine Learning},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287592},
	doi = {10.1145/3287560.3287592},
	series = {{FAT}* '19},
	abstract = {Kearns, Neel, Roth, and Wu [{ICML} 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded {VC} dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [{ICML} 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.},
	pages = {100--109},
	booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	publisher = {Association for Computing Machinery},
	author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	urldate = {2025-03-16},
	date = {2019-01-29},
	annotation = {Mehrabi 80
	},
}

@inproceedings{M87_Kusner_2017,
	title = {Counterfactual Fairness},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
	abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.  In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.  Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it  the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
	urldate = {2025-03-16},
	date = {2017},
	annotation = {Mehrabi 87
	},
}

@inproceedings{M149_Verma_2018,
	location = {New York, {NY}, {USA}},
	title = {Fairness definitions explained},
	isbn = {978-1-4503-5746-3},
	url = {https://dl.acm.org/doi/10.1145/3194770.3194776},
	doi = {10.1145/3194770.3194776},
	series = {{FairWare} '18},
	abstract = {Algorithm fairness has started to attract the attention of researchers in {AI}, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
	pages = {1--7},
	booktitle = {Proceedings of the International Workshop on Software Fairness},
	publisher = {Association for Computing Machinery},
	author = {Verma, Sahil and Rubin, Julia},
	urldate = {2025-03-16},
	date = {2018-05-29},
	annotation = {Mehrabi 149
	}
}
@article{Chakraborty_2024,
	title = {Biases in dermatology: A primer},
	volume = {90},
	issn = {0378-6323},
	url = {https://ijdvl.com/biases-in-dermatology-a-primer/},
	doi = {10.25259/IJDVL_126_2023},
	shorttitle = {Biases in dermatology},
	abstract = {Biases in dermatology: A primer},
	pages = {250--254},
	number = {2},
	journaltitle = {Indian J Dermatol Venereol Leprol},
	author = {Chakraborty, Atreyo},
	urldate = {2025-03-10},
	date = {2024-02-20},
	langid = {english},
	note = {Publisher: Scientific Scholar},
	annotation = {0 citations (but from 2024), list of lots of biases}
}


@article{Young_2020,
	title = {Artificial Intelligence in Dermatology: A Primer},
	volume = {140},
	issn = {0022-202X},
	url = {https://www.sciencedirect.com/science/article/pii/S0022202X2031201X},
	doi = {10.1016/j.jid.2020.02.026},
	shorttitle = {Artificial Intelligence in Dermatology},
	abstract = {Artificial intelligence is becoming increasingly important in dermatology, with studies reporting accuracy matching or exceeding dermatologists for the diagnosis of skin lesions from clinical and dermoscopic images. However, real-world clinical validation is currently lacking. We review dermatological applications of deep learning, the leading artificial intelligence technology for image analysis, and discuss its current capabilities, potential failure modes, and challenges surrounding performance assessment and interpretability. We address the following three primary applications: (i) teledermatology, including triage for referral to dermatologists; (ii) augmenting clinical assessment during face-to-face visits; and (iii) dermatopathology. We discuss equity and ethical issues related to future clinical adoption and recommend specific standardization of metrics for reporting model performance.},
	pages = {1504--1512},
	number = {8},
	journaltitle = {Journal of Investigative Dermatology},
	author = {Young, Albert T. and Xiong, Mulin and Pfau, Jacob and Keiser, Michael J. and Wei, Maria L.},
	urldate = {2025-03-23},
	date = {2020-08-01},
	annotation = {209 citations
	}
}
@inproceedings{Montoya_2025,
	location = {Cham},
	title = {Towards Fairness in {AI} for Melanoma Detection: Systemic Review and Recommendations},
	isbn = {978-3-031-84460-7},
	doi = {10.1007/978-3-031-84460-7_21},
	shorttitle = {Towards Fairness in {AI} for Melanoma Detection},
	abstract = {Early and accurate melanoma detection is crucial for improving patient outcomes. Recent advancements in artificial intelligence ({AI}) have shown promise in this area, but the technology’s effectiveness across diverse skin tones remains a critical challenge. This study conducts a systematic review and preliminary analysis of {AI}-based melanoma detection research published between 2013 and 2024, focusing on deep learning methodologies, datasets, and skin tone representation. Our findings indicate that while {AI} can enhance melanoma detection, there is a significant bias towards lighter skin tones. To address this, we propose including skin hue in addition to skin tone as represented by the L’Oreal Color Chart Map for a more comprehensive skin tone assessment technique. This research highlights the need for diverse datasets and robust evaluation metrics to develop {AI} models that are equitable and effective for all patients. By adopting best practices outlined in a {PRISMA}-Equity framework tailored for healthcare and melanoma detection, we can work towards reducing disparities in melanoma outcomes.},
	pages = {320--341},
	booktitle = {Advances in Information and Communication},
	publisher = {Springer Nature Switzerland},
	author = {Montoya, Laura N. and Roberts, Jennafer Shae and Hidalgo, Belén Sánchez},
	editor = {Arai, Kohei},
	date = {2025},
	langid = {english},
	keywords = {Artificial intelligence, Computer vision, Deep learning, Dermatological imaging, Hue, Machine learning, Melanoma, Skin cancer, Skin tone, Systemic literature review},
	annotation = {2025
	},
}
@online{Jayawickrama_2021,
	title = {Community Detection Algorithms},
	url = {https://medium.com/data-science/community-detection-algorithms-9bd8951e7dae},
	abstract = {Many of you are familiar with networks, right? You might be using social media sites such as Facebook, Instagram, Twitter, etc. They are…},
	titleaddon = {Medium},
	author = {Jayawickrama, Thamindu Dilshan},
	urldate = {2025-03-24},
	date = {2021-02-01},
	langid = {english},
}



@online{Mester_2022,
	title = {Statistical Bias Types explained (with examples) - part1},
	url = {https://data36.com/statistical-bias-types-explained/},
	abstract = {Being aware of the different statistical bias types is a must, if you want to become a data scientist. Here are the most important ones.},
	titleaddon = {Data36},
	author = {Mester, Tomi},
	urldate = {2025-03-08},
	date = {2022-05-16},
	langid = {american},
	},
}

@online{Mester_2017,
	title = {Statistical Bias Types explained - part2 (with examples)},
	url = {https://data36.com/statistical-bias-types-examples-part2/},
	abstract = {It’s time to continue our discourse about Statistical Bias Types. What can go wrong during the analysis and the presentation part?},
	titleaddon = {Data36},
	author = {Mester, Tomi},
	urldate = {2025-03-22},
	date = {2017-08-28},
	langid = {american},
}

@online{HP_2022,
	title = {Sampling — Statistical approach in Machine learning},
	url = {https://medium.com/analytics-vidhya/sampling-statistical-approach-in-machine-learning-4903c40ebf86},
	abstract = {When we have a big dataset and excited to get started with analyzing it and building your machine learning model. Our machine gives an…},
	titleaddon = {Analytics Vidhya},
	author = {{HP}, Suresha},
	urldate = {2025-03-28},
	date = {2022-11-01},
	langid = {english},
}

@article{Delgado-Rodriguez_2004,
	title = {Bias},
	volume = {58},
	rights = {Copyright 2004 Journal of Epidemiology and Community Health},
	issn = {0143-005X, 1470-2738},
	url = {https://jech.bmj.com/content/58/8/635},
	doi = {10.1136/jech.2003.008466},
	abstract = {The concept of bias is the lack of internal validity or incorrect assessment of the association between an exposure and an effect in the target population in which the statistic estimated has an expectation that does not equal the true value. Biases can be classified by the research stage in which they occur or by the direction of change in a estimate. The most important biases are those produced in the definition and selection of the study population, data collection, and the association between different determinants of an effect in the population. A definition of the most common biases occurring in these stages is given.},
	pages = {635--641},
	number = {8},
	journaltitle = {Journal of Epidemiology \& Community Health},
	author = {Delgado-Rodríguez, M. and Llorca, J.},
	urldate = {2025-03-29},
	date = {2004-08-01},
	langid = {english},
	pmid = {15252064},
	note = {Publisher: {BMJ} Publishing Group Ltd
	Section: Continuing professional education},
	keywords = {bias, confounding, information bias, selection bias},
}
