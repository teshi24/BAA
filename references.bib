@InProceedings{Gottfrois2024, % was before: 10.1007/978-3-031-72384-1_66,
	author = "Gottfrois, Philippe and Gröger, Fabian and Andriambololoniaina, Faly Herizo and Amruthalingam, Ludovic and Gonzalez-Jimenez, Alvaro and Hsu, Christophe and Kessy, Agnes and Lionetti, Simone and Mavura, Daudi and Ng'ambi, Wingston and Ngongonda, Dingase Faith and Pouly, Marc and Rakotoarisaona, Mendrika Fifaliana and Rapelanoro Rabenja, Fahafahantsoa and Traoré, Ibrahima and Navarini, Alexander A.",
	title = "PASSION for Dermatology: Bridging the Diversity Gap with Pigmented Skin Images from Sub-Saharan Africa",
	booktitle = "Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024",
	year = "2024",
	publisher = "Springer Nature Switzerland",
	address = "Cham",
	pages = "703--712", 
	isbn = "978-3-031-72384-1"
}
@article{Diaz2022,
	author = "Diaz, Michael and Lucke-Wold, Brandon and Batchu, Sai and Kleinberg, Giona",
	year = "2022",
	month = "01",
	pages = "42-47",
	title = "Racial underrepresentation in dermatological datasets leads to biased machine learning models and inequitable healthcare",
	volume = "3"
}
% Link: https://www.researchgate.net/publication/366481389_Racial_underrepresentation_in_dermatological_datasets_leads_to_biased_machine_learning_models_and_inequitable_healthcare
% Abstract: Objective: Clinical applications of machine learning are promising as a tool to improve patient outcomes through assisting diagnoses, treatment, and analyzing risk factors for screening. Possible clinical applications are especially prominent in dermatology as many diseases and conditions present visually. This allows a machine learning model to analyze and diagnose conditions using patient images and data from electronic health records (EHRs) after training on clinical datasets but could also introduce bias. Despite promising applications, artificial intelligence has the capacity to exacerbate existing demographic disparities in healthcare if models are trained on biased datasets. Methods: Through systematic literature review of available literature, we highlight the extent of bias present in clinical datasets as well as the implications it could have on healthcare if not addressed. Results: We find the implications are worsened in dermatological models. Despite the severity and complexity of melanoma and other dermatological diseases as well as differing disease presentations based on skin-color, many imaging datasets underrepresent certain demographic groups causing machine learning models to train on images of primarily fair-skinned individuals leaving minorities behind. Conclusion: In order to address this disparity, research first needs to be done investigating the extent of the bias present and the implications it may have on equitable healthcare.
@online{BAD2021,
	title = "Lower socioeconomic status linked with more severe skin disease, including melanoma",
	url = "https://www.skinhealthinfo.org.uk/lower-socioeconomic-status-linked-with-more-severe-skin-disease-including-melanoma/",
	abstract = "\%",
	titleaddon = "BAD Patient Hub",
	author = "{British Association of Dermatologists (BAD)}",
	urldate = "2025-02-17",
	date = "2021-07-07",
	langid = "english",
	note = "Research was presented at the BAD's Annual Meeting."
}
@article{Mehrabi_2021,
	title = "A Survey on Bias and Fairness in Machine Learning",
	url = "https://dl.acm.org/doi/10.1145/34576072",
	doi = "10.1145/3457607",
	abstract = "With the widespread use of artificial intelligence ({AI}) systems and applications in
	our everyday lives, accounting for fairness has gained significant importance in designing
	and engineering of such systems. {AI} systems can be used in many sensitive ...",
	journaltitle = "{ACM} Computing Surveys ({CSUR})",
	author = "Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram",
	urldate = "2025-02-28",
	date = "2021-07-13",
	note = "Publisher: {ACMPUB}27New York, {NY}, {USA}"
}
%% mehrabi citations
@article{M9_Baeza-Yates_2018,
	title = {Bias on the web},
	volume = {61},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3209581},
	doi = {10.1145/3209581},
	abstract = {Bias in Web data and use taints the algorithms behind Web-based applications, delivering equally biased results.},
	pages = {54--61},
	number = {6},
	journaltitle = {Commun. {ACM}},
	author = {Baeza-Yates, Ricardo},
	urldate = {2025-03-16},
	date = {2018-05-23},
	annotation = {Mehrabi 9
	},
}

@inproceedings{M24_Buolamwini_2018,
	title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	shorttitle = {Gender Shades},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, {IJB}-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for {IJB}-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	eventtitle = {Conference on Fairness, Accountability and Transparency},
	pages = {77--91},
	booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	publisher = {{PMLR}},
	author = {Buolamwini, Joy and Gebru, Timnit},
	urldate = {2025-03-16},
	date = {2018-01-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	annotation = {Mehrabi 24, demographic (skin type and gender)
	},
}

@article{M38_Clarke_2005,
	title = {The Phantom Menace: Omitted Variable Bias in Econometric Research},
	volume = {22},
	issn = {0738-8942},
	url = {https://doi.org/10.1080/07388940500339183},
	doi = {10.1080/07388940500339183},
	shorttitle = {The Phantom Menace},
	abstract = {Quantitative political science is awash in control variables. The justification for these bloated specifications is usually the fear of omitted variable bias. A key underlying assumption is that the danger posed by omitted variable bias can be ameliorated by the inclusion of relevant control variables. Unfortunately, as this article demonstrates, there is nothing in the mathematics of regression analysis that supports this conclusion. The inclusion of additional control variables may increase or decrease the bias, and we cannot know for sure which is the case in any particular situation. A brief discussion of alternative strategies for achieving experimental control follows the main result.},
	pages = {341--352},
	number = {4},
	journaltitle = {Conflict Management and Peace Science},
	author = {Clarke, Kevin A.},
	urldate = {2025-03-16},
	date = {2005-09-01},
	note = {Publisher: {SAGE} Publications Ltd},
	annotation = {Mehrabi 38, difficultis regarding ommitted variable and overcoming methods
	},
}

@inproceedings{M44_Danks_2017,
	location = {Melbourne, Australia},
	title = {Algorithmic Bias in Autonomous Systems},
	isbn = {978-0-9992411-0-3},
	url = {https://www.ijcai.org/proceedings/2017/654},
	doi = {10.24963/ijcai.2017/654},
	abstract = {Algorithms play a key role in the functioning of autonomous systems, and so concerns have periodically been raised about the possibility of algorithmic bias. However, debates in this area have been hampered by different meanings and uses of the term, “bias.” It is sometimes used as a purely descriptive term, sometimes as a pejorative term, and such variations can promote confusion and hamper discussions about when and how to respond to algorithmic bias. In this paper, we first provide a taxonomy of different types and sources of algorithmic bias, with a focus on their different impacts on the proper functioning of autonomous systems. We then use this taxonomy to distinguish between algorithmic biases that are neutral or unobjectionable, and those that are problematic in some way and require a response. In some cases, there are technological or algorithmic adjustments that developers can use to compensate for problematic bias. In other cases, however, responses require adjustments by the agent, whether human or autonomous system, who uses the results of the algorithm. There is no “one size fits all” solution to algorithmic bias.},
	eventtitle = {Twenty-Sixth International Joint Conference on Artificial Intelligence},
	pages = {4691--4697},
	booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Danks, David and London, Alex John},
	urldate = {2025-03-16},
	date = {2017-08},
	langid = {english},
	annotation = {Mehrabi 44
	},
}

@article{M53_Friedman_1996,
	title = {Bias in computer systems},
	volume = {14},
	issn = {1046-8188},
	url = {https://dl.acm.org/doi/10.1145/230538.230561},
	doi = {10.1145/230538.230561},
	abstract = {From an analysis of actual cases, three categories of bias in computer systems have been developed: preexisting, technical, and emergent. Preexisting bias has its roots in social institutions, practices, and attitudes. Technical bias arises from technical constraints of considerations. Emergent bias arises in a context of use. Although others have pointed to bias inparticular computer systems and have noted the general problem, we know of no comparable work that examines this phenomenon comprehensively and which offers a framework for understanding and remedying it. We conclude by suggesting that freedom from bias should by counted amoung the select set of criteria—including reliability, accuracy, and efficiency—according to which the quality of systems in use in society should be judged.},
	pages = {330--347},
	number = {3},
	journaltitle = {{ACM} Trans. Inf. Syst.},
	author = {Friedman, Batya and Nissenbaum, Helen},
	urldate = {2025-03-16},
	date = {1996-07-01},
	annotation = {Mehrabi 53
	},
}

@article{M64_Hargittai_2007,
	title = {Whose Space? Differences among Users and Non-Users of Social Network Sites},
	volume = {13},
	issn = {1083-6101},
	url = {https://doi.org/10.1111/j.1083-6101.2007.00396.x},
	doi = {10.1111/j.1083-6101.2007.00396.x},
	shorttitle = {Whose Space?},
	abstract = {Are there systematic differences between people who use social network sites and those who stay away, despite a familiarity with them? Based on data from a survey administered to a diverse group of young adults, this article looks at the predictors of {SNS} usage, with particular focus on Facebook, {MySpace}, Xanga, and Friendster. Findings suggest that use of such sites is not randomly distributed across a group of highly wired users. A person’s gender, race and ethnicity, and parental educational background are all associated with use, but in most cases only when the aggregate concept of social network sites is disaggregated by service. Additionally, people with more experience and autonomy of use are more likely to be users of such sites. Unequal participation based on user background suggests that differential adoption of such services may be contributing to digital inequality.},
	pages = {276--297},
	number = {1},
	journaltitle = {Journal of Computer-Mediated Communication},
	author = {Hargittai, Eszter},
	urldate = {2025-03-16},
	date = {2007-10-01},
	annotation = {Mehrabi 64
	},
}

@article{M93_Lerman_2014,
	title = {Leveraging Position Bias to Improve Peer Recommendation},
	volume = {9},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0098914},
	doi = {10.1371/journal.pone.0098914},
	abstract = {With the advent of social media and peer production, the amount of new online content has grown dramatically. To identify interesting items in the vast stream of new content, providers must rely on peer recommendation to aggregate opinions of their many users. Due to human cognitive biases, the presentation order strongly affects how people allocate attention to the available content. Moreover, we can manipulate attention through the presentation order of items to change the way peer recommendation works. We experimentally evaluate this effect using Amazon Mechanical Turk. We find that different policies for ordering content can steer user attention so as to improve the outcomes of peer recommendation.},
	pages = {e98914},
	number = {6},
	journaltitle = {{PLOS} {ONE}},
	author = {Lerman, Kristina and Hogg, Tad},
	urldate = {2025-03-16},
	date = {2014-06-11},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Arithmetic, Attention, Intelligence, Permutation, Psychological attitudes, Social influence, Social media, Social systems},
	annotation = {Mehrabi 93
	},
}

@article{M114_Mustard_2003,
	title = {Reexamining Criminal Behavior: The Importance of Omitted Variable Bias},
	volume = {85},
	issn = {0034-6535},
	url = {https://doi.org/10.1162/rest.2003.85.1.205},
	doi = {10.1162/rest.2003.85.1.205},
	shorttitle = {Reexamining Criminal Behavior},
	abstract = {Recently many papers have used the arrest rate to measure punishments in crime-rate regressions. However, arrest rates account for only a portion of the criminal sanction. Conviction rates and time served are theoretically important, but rarely used, and excluding them generates omitted variable bias if they are correlated with the arrest rate. This paper uses the most complete set of conviction and sentencing data to show that arrest rates are negatively correlated with these normally excluded variables. Consequently, previous estimates of arrest-rate effects are understated by as much as 50\%. Also, conviction rates, but not sentence lengths, have significant explanatory power in standard crime-rate regressions.},
	pages = {205--211},
	number = {1},
	journaltitle = {The Review of Economics and Statistics},
	author = {Mustard, David B.},
	urldate = {2025-03-16},
	date = {2003-02-01},
	annotation = {Mehrabi 114
	},
}

@article{M117_Ciampaglia_2018,
	title = {How algorithmic popularity bias hinders or promotes quality},
	volume = {8},
	rights = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-34203-2},
	doi = {10.1038/s41598-018-34203-2},
	abstract = {Algorithms that favor popular items are used to help us select among many choices, from top-ranked search engine results to highly-cited scientific papers. The goal of these algorithms is to identify high-quality items such as reliable news, credible information sources, and important discoveries–in short, high-quality content should rank at the top. Prior work has shown that choosing what is popular may amplify random fluctuations and lead to sub-optimal rankings. Nonetheless, it is often assumed that recommending what is popular will help high-quality content “bubble up” in practice. Here we identify the conditions in which popularity may be a viable proxy for quality content by studying a simple model of a cultural market endowed with an intrinsic notion of quality. A parameter representing the cognitive cost of exploration controls the trade-off between quality and popularity. Below and above a critical exploration cost, popularity bias is more likely to hinder quality. But we find a narrow intermediate regime of user attention where an optimal balance exists: choosing what is popular can help promote high-quality items to the top. These findings clarify the effects of algorithmic popularity bias on quality outcomes, and may inform the design of more principled mechanisms for techno-social cultural markets.},
	pages = {15951},
	number = {1},
	journaltitle = {Sci Rep},
	author = {Ciampaglia, Giovanni Luca and Nematzadeh, Azadeh and Menczer, Filippo and Flammini, Alessandro},
	urldate = {2025-03-16},
	date = {2018-10-29},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Complex networks, Statistics},
	annotation = {Mehrabi 117
	}
}

@article{M120_Olteanu_2019,
	title = {Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries},
	volume = {2},
	issn = {2624-909X},
	url = {https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2019.00013/full},
	doi = {10.3389/fdata.2019.00013},
	shorttitle = {Social Data},
	abstract = {{\textless}p{\textgreater}Social data in digital form—including user-generated content, expressed or implicit relations between people, and behavioral traces—are at the core of popular applications and platforms, driving the research agenda of many researchers. The promises of social data are many, including understanding “what the world thinks” about a social issue, brand, celebrity, or other entity, as well as enabling better decision-making in a variety of fields including public policy, healthcare, and economics. Many academics and practitioners have warned against the naïve usage of social data. There are biases and inaccuracies occurring at the source of the data, but also introduced during processing. There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked. This paper recognizes the rigor with which these issues are addressed by different researchers varies across a wide range. We identify a variety of menaces in the practices around social data use, and organize them in a framework that helps to identify them.{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}disp-quote{\textgreater}{\textless}p{\textgreater}“{\textless}italic{\textgreater}For your own sanity, you have to remember that not all problems can be solved. Not all problems can be solved, but all problems can be illuminated.” –Ursula Franklin{\textless}/italic{\textgreater}{\textless}xref ref-type="fn" rid="fn0001"{\textgreater}$^{\textrm{1}}${\textless}/xref{\textgreater}{\textless}/p{\textgreater}{\textless}/disp-quote{\textgreater}{\textless}/p{\textgreater}},
	journaltitle = {Front. Big Data},
	author = {Olteanu, Alexandra and Castillo, Carlos and Diaz, Fernando and Kıcıman, Emre},
	urldate = {2025-03-16},
	date = {2019-07-11},
	note = {Publisher: Frontiers},
	keywords = {biases, Ethics, Evaluation, Social Media, User data},
	annotation = {Mehrabi 120
	}
}

@article{M131_Riegg_2008,
	title = {Causal Inference and Omitted Variable Bias in Financial Aid Research: Assessing Solutions},
	volume = {31},
	issn = {1090-7009},
	url = {https://muse.jhu.edu/pub/1/article/232773},
	shorttitle = {Causal Inference and Omitted Variable Bias in Financial Aid Research},
	abstract = {, This article highlights the problem of omitted variable bias in research on the causal effect of financial aid on college‑going. I first describe the problem of self‑selection and the resulting bias from omitted variables. I then assess and explore the strengths and weaknesses of random assignment, multivariate regression, proxy variables, fixed effects, difference‑in‑differences, regression discontinuity, and instrumental variables techniques in addressing the problem. I focus on the intuition, assumptions, and applications of each method in the context of the same research question, providing practical guidance for researchers interested in implementing these approaches.},
	pages = {329--354},
	number = {3},
	journaltitle = {The Review of Higher Education},
	author = {Riegg, Stephanie K.},
	urldate = {2025-03-16},
	date = {2008},
	note = {Publisher: Johns Hopkins University Press},
	annotation = {Mehrabi 131
	},
}

@inproceedings{M144_Suresh_2021,
	location = {New York, {NY}, {USA}},
	title = {A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle},
	isbn = {978-1-4503-8553-4},
	url = {https://dl.acm.org/doi/10.1145/3465416.3483305},
	doi = {10.1145/3465416.3483305},
	series = {{EAAMO} '21},
	abstract = {As machine learning ({ML}) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the {ML} life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.},
	pages = {1--9},
	booktitle = {Proceedings of the 1st {ACM} Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
	publisher = {Association for Computing Machinery},
	author = {Suresh, Harini and Guttag, John},
	urldate = {2025-03-16},
	date = {2021-11-04},
	annotation = {Mehrabi 144
	}
}

@article{M151_Wang_2014,
	title = {Why Amazon's Ratings Might Mislead You: The Story of Herding Effects},
	volume = {2},
	issn = {2167-6461},
	url = {https://www.liebertpub.com/doi/full/10.1089/big.2014.0063},
	doi = {10.1089/big.2014.0063},
	shorttitle = {Why Amazon's Ratings Might Mislead You},
	abstract = {Our society is increasingly relying on digitalized, aggregated opinions of individuals to make decisions (e.g., product recommendation based on collective ratings). One key requirement of harnessing this “wisdom of crowd” is the independency of individuals' opinions; yet, in real settings, collective opinions are rarely simple aggregations of independent minds. Recent experimental studies document that disclosing prior collective ratings distorts individuals' decision making as well as their perceptions of quality and value, highlighting a fundamental discrepancy between our perceived values from collective ratings and products' intrinsic values. Here we present a mechanistic framework to describe herding effects of prior collective ratings on subsequent individual decision making. Using large-scale longitudinal customer rating datasets, we find that our method successfully captures the dynamics of ratings growth, helping us separate social influence bias from inherent values. Leveraging the proposed framework, we quantitatively characterize the herding effects existing in product rating systems and promote strategies to untangle manipulations and social biases.},
	pages = {196--204},
	number = {4},
	journaltitle = {Big Data},
	author = {Wang, Ting and Wang, Dashun},
	urldate = {2025-03-16},
	date = {2014-12},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	annotation = {Mehrabi 151
	}
}


% Mehrabi Fairness
@article{M15_Berk_2017,
	title = {Fairness in Criminal Justice Risk Assessments: The State of the Art},
	volume = {50},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124118782533},
	doi = {10.1177/0049124118782533},
	shorttitle = {Fairness in Criminal Justice Risk Assessments},
	abstract = {Objectives:Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this article, we seek to clarify the trade-offs between different kinds of fairness and between fairness and accuracy.Methods:We draw on the existing literatures in criminology, computer science, and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments.Results:We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy.Conclusions:Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging trade-offs. These lessons apply to applications well beyond criminology where assessments of risk can be used by decision makers. Examples include mortgage lending, employment, college admissions, child welfare, and medical diagnoses.},
	pages = {3--44},
	number = {1},
	journaltitle = {Sociological Methods \& Research},
	author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Kearns, Michael and Roth, Aaron},
	urldate = {2025-03-16},
	date = {2017},
	note = {Publisher: {SAGE} Publications Inc},
	annotation = {Mehrabi 15
	},
}

@article{M34_Chouldechova_2017,
	title = {Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments},
	volume = {5},
	issn = {2167-6461},
	url = {https://www.liebertpub.com/doi/abs/10.1089/big.2016.0047},
	doi = {10.1089/big.2016.0047},
	shorttitle = {Fair Prediction with Disparate Impact},
	abstract = {Recidivism prediction instruments ({RPIs}) provide decision-makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. Although such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This article discusses several fairness criteria that have recently been applied to assess the fairness of {RPIs}. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when an {RPI} fails to satisfy the criterion of error rate balance.},
	pages = {153--163},
	number = {2},
	journaltitle = {Big Data},
	author = {Chouldechova, Alexandra},
	urldate = {2025-03-16},
	date = {2017-06},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	annotation = {Mehrabi 34
	},
}

@inproceedings{M41_Corbett-Davies_2017,
	location = {New York, {NY}, {USA}},
	title = {Algorithmic Decision Making and the Cost of Fairness},
	isbn = {978-1-4503-4887-4},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098095},
	doi = {10.1145/3097983.3098095},
	series = {{KDD} '17},
	abstract = {Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally differ, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-off can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.},
	pages = {797--806},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
	urldate = {2025-03-16},
	date = {2017-08-04},
	annotation = {Mehrabi 41
	},
}

@inproceedings{M48_Dwork_2012,
	location = {New York, {NY}, {USA}},
	title = {Fairness through awareness},
	isbn = {978-1-4503-1115-1},
	url = {https://dl.acm.org/doi/10.1145/2090236.2090255},
	doi = {10.1145/2090236.2090255},
	series = {{ITCS} '12},
	abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	pages = {214--226},
	booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
	publisher = {Association for Computing Machinery},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	urldate = {2025-03-16},
	date = {2012-01-08},
	annotation = {Mehrabi 48
	},
}

@inproceedings{M50_Farnadi_2018,
	location = {New York, {NY}, {USA}},
	title = {Fairness in Relational Domains},
	isbn = {978-1-4503-6012-8},
	url = {https://dl.acm.org/doi/10.1145/3278721.3278733},
	doi = {10.1145/3278721.3278733},
	series = {{AIES} '18},
	abstract = {{AI} and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic ({PSL}), to incorporate our definition of relational fairness. We refer to this fairness-aware framework {FairPSL}. {FairPSL} makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori({MAP}) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.},
	pages = {108--114},
	booktitle = {Proceedings of the 2018 {AAAI}/{ACM} Conference on {AI}, Ethics, and Society},
	publisher = {Association for Computing Machinery},
	author = {Farnadi, Golnoosh and Babaki, Behrouz and Getoor, Lise},
	urldate = {2025-03-16},
	date = {2018-12-27},
	annotation = {Mehrabi 50
	},
}

@article{M61_Grgic-Hlaca_2016,
	title = {The Case for Process Fairness in Learning: Feature Selection for Fair Decision Making},
	abstract = {Machine learning methods are increasingly being used to inform, or sometimes even directly to make, important decisions about humans. A number of recent works have focussed on the fairness of the outcomes of such decisions, particularly on avoiding decisions that affect users of different sensitive groups (e.g., race, gender) disparately. In this paper, we propose to consider the fairness of the process of decision making. Process fairness can be measured by estimating the degree to which people consider various features to be fair to use when making an important legal decision. We examine the task of predicting whether or not a prisoner is likely to commit a crime again once released by analyzing the dataset considered by {ProPublica} relating to the {COMPAS} system. We introduce new measures of people’s discomfort with using various features, show how these measures can be estimated, and consider the effect of removing the uncomfortable features on prediction accuracy and on outcome fairness. Our empirical analysis suggests that process fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.},
	author = {Grgic-Hlača, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
	date = {2016},
	langid = {english},
	annotation = {Mehrabi 61
	},
}

@inproceedings{M63_Hardt_2016,
	title = {Equality of Opportunity in Supervised Learning},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html},
	abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
	urldate = {2025-03-16},
	date = {2016},
	annotation = {Mehrabi 63
	},
}

@inproceedings{M79_Kearns_2018,
	title = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
	url = {https://proceedings.mlr.press/v80/kearns18a.html},
	shorttitle = {Preventing Fairness Gerrymandering},
	abstract = {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.},
	eventtitle = {International Conference on Machine Learning},
	pages = {2564--2572},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	urldate = {2025-03-16},
	date = {2018-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	annotation = {Mehrabi 79
	},
}

@inproceedings{M80_Kearns_2019,
	location = {New York, {NY}, {USA}},
	title = {An Empirical Study of Rich Subgroup Fairness for Machine Learning},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287592},
	doi = {10.1145/3287560.3287592},
	series = {{FAT}* '19},
	abstract = {Kearns, Neel, Roth, and Wu [{ICML} 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded {VC} dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [{ICML} 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.},
	pages = {100--109},
	booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	publisher = {Association for Computing Machinery},
	author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	urldate = {2025-03-16},
	date = {2019-01-29},
	annotation = {Mehrabi 80
	},
}

@inproceedings{M87_Kusner_2017,
	title = {Counterfactual Fairness},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
	abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.  In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.  Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it  the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
	urldate = {2025-03-16},
	date = {2017},
	annotation = {Mehrabi 87
	},
}

@inproceedings{M149_Verma_2018,
	location = {New York, {NY}, {USA}},
	title = {Fairness definitions explained},
	isbn = {978-1-4503-5746-3},
	url = {https://dl.acm.org/doi/10.1145/3194770.3194776},
	doi = {10.1145/3194770.3194776},
	series = {{FairWare} '18},
	abstract = {Algorithm fairness has started to attract the attention of researchers in {AI}, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
	pages = {1--7},
	booktitle = {Proceedings of the International Workshop on Software Fairness},
	publisher = {Association for Computing Machinery},
	author = {Verma, Sahil and Rubin, Julia},
	urldate = {2025-03-16},
	date = {2018-05-29},
	annotation = {Mehrabi 149
	}
}
@article{Chakraborty_2024,
	title = {Biases in dermatology: A primer},
	volume = {90},
	issn = {0378-6323},
	url = {https://ijdvl.com/biases-in-dermatology-a-primer/},
	doi = {10.25259/IJDVL_126_2023},
	shorttitle = {Biases in dermatology},
	abstract = {Biases in dermatology: A primer},
	pages = {250--254},
	number = {2},
	journaltitle = {Indian J Dermatol Venereol Leprol},
	author = {Chakraborty, Atreyo},
	urldate = {2025-03-10},
	date = {2024-02-20},
	langid = {english},
	note = {Publisher: Scientific Scholar},
	annotation = {0 citations (but from 2024), list of lots of biases}
}
@article{Young_2020,
	title = {Artificial Intelligence in Dermatology: A Primer},
	volume = {140},
	issn = {0022-202X},
	url = {https://www.sciencedirect.com/science/article/pii/S0022202X2031201X},
	doi = {10.1016/j.jid.2020.02.026},
	shorttitle = {Artificial Intelligence in Dermatology},
	abstract = {Artificial intelligence is becoming increasingly important in dermatology, with studies reporting accuracy matching or exceeding dermatologists for the diagnosis of skin lesions from clinical and dermoscopic images. However, real-world clinical validation is currently lacking. We review dermatological applications of deep learning, the leading artificial intelligence technology for image analysis, and discuss its current capabilities, potential failure modes, and challenges surrounding performance assessment and interpretability. We address the following three primary applications: (i) teledermatology, including triage for referral to dermatologists; (ii) augmenting clinical assessment during face-to-face visits; and (iii) dermatopathology. We discuss equity and ethical issues related to future clinical adoption and recommend specific standardization of metrics for reporting model performance.},
	pages = {1504--1512},
	number = {8},
	journaltitle = {Journal of Investigative Dermatology},
	author = {Young, Albert T. and Xiong, Mulin and Pfau, Jacob and Keiser, Michael J. and Wei, Maria L.},
	urldate = {2025-03-23},
	date = {2020-08-01},
	annotation = {209 citations
	}
}
@inproceedings{Montoya_2025,
	location = {Cham},
	title = {Towards Fairness in {AI} for Melanoma Detection: Systemic Review and Recommendations},
	isbn = {978-3-031-84460-7},
	doi = {10.1007/978-3-031-84460-7_21},
	shorttitle = {Towards Fairness in {AI} for Melanoma Detection},
	abstract = {Early and accurate melanoma detection is crucial for improving patient outcomes. Recent advancements in artificial intelligence ({AI}) have shown promise in this area, but the technology’s effectiveness across diverse skin tones remains a critical challenge. This study conducts a systematic review and preliminary analysis of {AI}-based melanoma detection research published between 2013 and 2024, focusing on deep learning methodologies, datasets, and skin tone representation. Our findings indicate that while {AI} can enhance melanoma detection, there is a significant bias towards lighter skin tones. To address this, we propose including skin hue in addition to skin tone as represented by the L’Oreal Color Chart Map for a more comprehensive skin tone assessment technique. This research highlights the need for diverse datasets and robust evaluation metrics to develop {AI} models that are equitable and effective for all patients. By adopting best practices outlined in a {PRISMA}-Equity framework tailored for healthcare and melanoma detection, we can work towards reducing disparities in melanoma outcomes.},
	pages = {320--341},
	booktitle = {Advances in Information and Communication},
	publisher = {Springer Nature Switzerland},
	author = {Montoya, Laura N. and Roberts, Jennafer Shae and Hidalgo, Belén Sánchez},
	editor = {Arai, Kohei},
	date = {2025},
	langid = {english},
	keywords = {Artificial intelligence, Computer vision, Deep learning, Dermatological imaging, Hue, Machine learning, Melanoma, Skin cancer, Skin tone, Systemic literature review},
	annotation = {2025
	},
}
@online{Jayawickrama_2021,
	title = {Community Detection Algorithms},
	url = {https://medium.com/data-science/community-detection-algorithms-9bd8951e7dae},
	abstract = {Many of you are familiar with networks, right? You might be using social media sites such as Facebook, Instagram, Twitter, etc. They are…},
	titleaddon = {Medium},
	author = {Jayawickrama, Thamindu Dilshan},
	urldate = {2025-03-24},
	date = {2021-02-01},
	langid = {english},
}



@online{Mester_2022,
	title = {Statistical Bias Types explained (with examples) - part1},
	url = {https://data36.com/statistical-bias-types-explained/},
	abstract = {Being aware of the different statistical bias types is a must, if you want to become a data scientist. Here are the most important ones.},
	titleaddon = {Data36},
	author = {Mester, Tomi},
	urldate = {2025-03-08},
	date = {2022-05-16},
	langid = {american},
	},
}

@online{Mester_2017,
	title = {Statistical Bias Types explained - part2 (with examples)},
	url = {https://data36.com/statistical-bias-types-examples-part2/},
	abstract = {It’s time to continue our discourse about Statistical Bias Types. What can go wrong during the analysis and the presentation part?},
	titleaddon = {Data36},
	author = {Mester, Tomi},
	urldate = {2025-03-22},
	date = {2017-08-28},
	langid = {american},
}

@online{HP_2022,
	title = {Sampling — Statistical approach in Machine learning},
	url = {https://medium.com/analytics-vidhya/sampling-statistical-approach-in-machine-learning-4903c40ebf86},
	abstract = {When we have a big dataset and excited to get started with analyzing it and building your machine learning model. Our machine gives an…},
	titleaddon = {Analytics Vidhya},
	author = {{HP}, Suresha},
	urldate = {2025-03-28},
	date = {2022-11-01},
	langid = {english},
}

@article{Delgado-Rodriguez_2004,
	title = {Bias},
	volume = {58},
	rights = {Copyright 2004 Journal of Epidemiology and Community Health},
	issn = {0143-005X, 1470-2738},
	url = {https://jech.bmj.com/content/58/8/635},
	doi = {10.1136/jech.2003.008466},
	abstract = {The concept of bias is the lack of internal validity or incorrect assessment of the association between an exposure and an effect in the target population in which the statistic estimated has an expectation that does not equal the true value. Biases can be classified by the research stage in which they occur or by the direction of change in a estimate. The most important biases are those produced in the definition and selection of the study population, data collection, and the association between different determinants of an effect in the population. A definition of the most common biases occurring in these stages is given.},
	pages = {635--641},
	number = {8},
	journaltitle = {Journal of Epidemiology \& Community Health},
	author = {Delgado-Rodríguez, M. and Llorca, J.},
	urldate = {2025-03-29},
	date = {2004-08-01},
	langid = {english},
	pmid = {15252064},
	note = {Publisher: {BMJ} Publishing Group Ltd
	Section: Continuing professional education},
	keywords = {bias, confounding, information bias, selection bias},
}


@inproceedings{M24_Buolamwini_2018,
	title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	shorttitle = {Gender Shades},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, {IJB}-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for {IJB}-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	eventtitle = {Conference on Fairness, Accountability and Transparency},
	pages = {77--91},
	booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	publisher = {{PMLR}},
	author = {Buolamwini, Joy and Gebru, Timnit},
	urldate = {2025-03-16},
	date = {2018-01-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	annotation = {Mehrabi 24, demographic (skin type and gender)
	},
}

@article{M98_Manrai_2016,
	title = {Genetic Misdiagnoses and the Potential for Health Disparities},
	volume = {375},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMsa1507092},
	doi = {10.1056/NEJMsa1507092},
	pages = {655--665},
	number = {7},
	journaltitle = {N Engl J Med},
	author = {Manrai, Arjun K. and Funke, Birgit H. and Rehm, Heidi L. and Olesen, Morten S. and Maron, Bradley A. and Szolovits, Peter and Margulies, David M. and Loscalzo, Joseph and Kohane, Isaac S.},
	urldate = {2025-03-22},
	date = {2016-08-18},
	langid = {english},
	annotation = {Mehrabi 98
	},
	file = {Full Text PDF:files/847/Manrai et al. - 2016 - Genetic Misdiagnoses and the Potential for Health .pdf:application/pdf},
}



@article{M150_Vickers_2014,
	title = {An Overview of {EMPaCT} and Fundamental Issues Affecting Minority Participation in Cancer Clinical Trials},
	volume = {120},
	issn = {0008-543X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4133979/},
	doi = {10.1002/cncr.28569},
	pages = {1087--1090},
	number = {0},
	journaltitle = {Cancer},
	author = {Vickers, Selwyn M. and Fouad, Mona N.},
	urldate = {2025-03-17},
	date = {2014-04-01},
	pmid = {24643645},
	pmcid = {PMC4133979},
	annotation = {Mehrabi 150
	},
}


@misc{M142_Shankar_2017,
	title = {No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World},
	url = {http://arxiv.org/abs/1711.08536},
	doi = {10.48550/arXiv.1711.08536},
	shorttitle = {No Classification without Representation},
	abstract = {Modern machine learning systems such as image classifiers rely heavily on large scale data sets for training. Such data sets are costly to create, thus in practice a small number of freely available, open source data sets are widely used. We suggest that examining the geo-diversity of open data sets is critical before adopting a data set for use cases in the developing world. We analyze two large, publicly available image data sets to assess geo-diversity and find that these data sets appear to exhibit an observable amerocentric and eurocentric representation bias. Further, we analyze classifiers trained on these data sets to assess the impact of these training distributions and find strong differences in the relative performance on images from different locales. These results emphasize the need to ensure geo-representation when constructing data sets for use in the developing world.},
	number = {{arXiv}:1711.08536},
	publisher = {{arXiv}},
	author = {Shankar, Shreya and Halpern, Yoni and Breck, Eric and Atwood, James and Wilson, Jimbo and Sculley, D.},
	urldate = {2025-04-03},
	date = {2017-11-22},
	eprinttype = {arxiv},
	eprint = {1711.08536 [stat]},
	keywords = {Statistics - Machine Learning},
	annotation = {Mehrabi 142Comment: Presented at {NIPS} 2017 Workshop on Machine Learning for the Developing World
	},
}

@article{M54_Fry_2017,
	title = {Comparison of Sociodemographic and Health-Related Characteristics of {UK} Biobank Participants With Those of the General Population},
	volume = {186},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/aje/kwx246},
	doi = {10.1093/aje/kwx246},
	abstract = {The {UK} Biobank cohort is a population-based cohort of 500,000 participants recruited in the United Kingdom ({UK}) between 2006 and 2010. Approximately 9.2 million individuals aged 40–69 years who lived within 25 miles (40 km) of one of 22 assessment centers in England, Wales, and Scotland were invited to enter the cohort, and 5.5\% participated in the baseline assessment. The representativeness of the {UK} Biobank cohort was investigated by comparing demographic characteristics between nonresponders and responders. Sociodemographic, physical, lifestyle, and health-related characteristics of the cohort were compared with nationally representative data sources. {UK} Biobank participants were more likely to be older, to be female, and to live in less socioeconomically deprived areas than nonparticipants. Compared with the general population, participants were less likely to be obese, to smoke, and to drink alcohol on a daily basis and had fewer self-reported health conditions. At age 70–74 years, rates of all-cause mortality and total cancer incidence were 46.2\% and 11.8\% lower, respectively, in men and 55.5\% and 18.1\% lower, respectively, in women than in the general population of the same age. {UK} Biobank is not representative of the sampling population; there is evidence of a “healthy volunteer” selection bias. Nonetheless, valid assessment of exposure-disease relationships may be widely generalizable and does not require participants to be representative of the population at large.},
	pages = {1026--1034},
	number = {9},
	journaltitle = {American Journal of Epidemiology},
	author = {Fry, Anna and Littlejohns, Thomas J and Sudlow, Cathie and Doherty, Nicola and Adamska, Ligia and Sprosen, Tim and Collins, Rory and Allen, Naomi E},
	urldate = {2025-04-03},
	date = {2017-11-01},
	annotation = {Mehrabi 54}
}

@inproceedings{M30_Chen_2019,
	location = {New York, {NY}, {USA}},
	title = {Fairness Under Unawareness: Assessing Disparity When Protected Class Is Unobserved},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287594},
	doi = {10.1145/3287560.3287594},
	series = {{FAT}* '19},
	shorttitle = {Fairness Under Unawareness},
	abstract = {Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.},
	pages = {339--348},
	booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	publisher = {Association for Computing Machinery},
	author = {Chen, Jiahao and Kallus, Nathan and Mao, Xiaojie and Svacha, Geoffry and Udell, Madeleine},
	urldate = {2025-04-03},
	date = {2019-01-29},
	annotation = {Mehrabi 30
	}
}

@misc{M167_Zhao_2017,
	title = {Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints},
	url = {http://arxiv.org/abs/1707.09457},
	doi = {10.48550/arXiv.1707.09457},
	shorttitle = {Men Also Like Shopping},
	abstract = {Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33\% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68\% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5\% and 40.5\% for multilabel classification and visual semantic role labeling, respectively.},
	number = {{arXiv}:1707.09457},
	publisher = {{arXiv}},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	urldate = {2025-04-03},
	date = {2017-07-29},
	eprinttype = {arxiv},
	eprint = {1707.09457 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	annotation = {Mehrabi 167 Comment: 11 pages, published in {EMNLP} 2017
	},
}

@inproceedings{M20_Bolukbasi_2016,
	title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
	shorttitle = {Man is to Computer Programmer as Woman is to Homemaker?},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female.  Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
	urldate = {2025-04-03},
	date = {2016},
	annotation = {Mehrabi 20
	},
}

@misc{M168_Zhao_2018,
	title = {Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods},
	url = {http://arxiv.org/abs/1804.06876},
	doi = {10.48550/arXiv.1804.06876},
	shorttitle = {Gender Bias in Coreference Resolution},
	abstract = {We introduce a new benchmark, {WinoBias}, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in {WinoBias} without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at http://winobias.org.},
	number = {{arXiv}:1804.06876},
	publisher = {{arXiv}},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	urldate = {2025-04-03},
	date = {2018-04-18},
	eprinttype = {arxiv},
	eprint = {1804.06876 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annotation = {Mehrabi 168, Comment: {NAACL} '18 Camera Ready
	},
}

@article{M62_Hajian_2013,
	title = {A Methodology for Direct and Indirect Discrimination Prevention in Data Mining},
	volume = {25},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/6175897},
	doi = {10.1109/TKDE.2012.72},
	abstract = {Data mining is an increasingly important technology for extracting useful knowledge hidden in large collections of data. There are, however, negative social perceptions about data mining, among which potential privacy invasion and potential discrimination. The latter consists of unfairly treating people on the basis of their belonging to a specific group. Automated data collection and data mining techniques such as classification rule mining have paved the way to making automated decisions, like loan granting/denial, insurance premium computation, etc. If the training data sets are biased in what regards discriminatory (sensitive) attributes like gender, race, religion, etc., discriminatory decisions may ensue. For this reason, antidiscrimination techniques including discrimination discovery and prevention have been introduced in data mining. Discrimination can be either direct or indirect. Direct discrimination occurs when decisions are made based on sensitive attributes. Indirect discrimination occurs when decisions are made based on nonsensitive attributes which are strongly correlated with biased sensitive ones. In this paper, we tackle discrimination prevention in data mining and propose new techniques applicable for direct or indirect discrimination prevention individually or both at the same time. We discuss how to clean training data sets and outsourced data sets in such a way that direct and/or indirect discriminatory decision rules are converted to legitimate (nondiscriminatory) classification rules. We also propose new metrics to evaluate the utility of the proposed approaches and we compare these approaches. The experimental evaluations demonstrate that the proposed techniques are effective at removing direct and/or indirect discrimination biases in the original data set while preserving data quality.},
	pages = {1445--1459},
	number = {7},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Hajian, Sara and Domingo-Ferrer, Josep},
	urldate = {2025-04-03},
	date = {2013-07},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Antidiscrimination, Data engineering, data mining, Data mining, direct and indirect discrimination prevention, Itemsets, Knowledge engineering, privacy, rule generalization, rule protection, Training, Training data},
	annotation = {Mehrabi 62
	},
}


@article{Pala_2020,
	title = {Teledermatology: idea, benefits and risks of modern age – a systematic review based on melanoma},
	volume = {37},
	issn = {1642-395X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7262815/},
	doi = {10.5114/ada.2020.94834},
	shorttitle = {Teledermatology},
	abstract = {Telemedicine may be described as a modern technology supporting health care at a distance. Dermatology, as a visually-dependent specialty, is particularly suited for this kind of the health care model. This has been proven in a number of recent studies, which emphasized feasibility and reliability of teledermatology. Many patients in the world still do not have access to appropriate dermatological care, while skin cancers morbidity is on an upward trend. Technological development has enabled clinicians to care for diverse patient populations in need of skin expertise without increasing their overhead costs. Teledermatology has been used for various purposes: health care workers can use this technology to provide clinical services to patients, to monitor patient health, to consult with other health care providers and to provide patients with access to educational resources. It seems that teledermatology might be the answer to numerous issues concerning diagnosing, screening and managing cancers as well as pigmented skin lesions.},
	pages = {159--167},
	number = {2},
	journaltitle = {Postepy Dermatol Alergol},
	author = {Pala, Paulina and Bergler-Czop, Beata S. and Gwiżdż, Jakub M.},
	urldate = {2025-04-11},
	date = {2020-04},
	pmid = {32489348},
	pmcid = {PMC7262815},
	file = {PubMed Central Full Text PDF:files/989/Pala et al. - 2020 - Teledermatology idea, benefits and risks of moder.pdf:application/pdf},
}


@article{Aleid_2024,
	title = {Prevalence and Socio-Demographic and Hygiene Factors Influencing Impetigo in Saudi Arabian Children: A Cross-Sectional Investigation},
	volume = {17},
	url = {https://www.tandfonline.com/doi/abs/10.2147/CCID.S472228},
	doi = {10.2147/CCID.S472228},
	shorttitle = {Prevalence and Socio-Demographic and Hygiene Factors Influencing Impetigo in Saudi Arabian Children},
	abstract = {To determine the prevalence of impetigo among children in Saudi Arabia as well as to identify socio-demographic factors associated with impetigo. This cross-sectional study conducted in Saudi Arabia examined impetigo prevalence and associated factors among children aged 2 to 15. Data collection occurred between June 2022 and November 2023, involving structured interviews with the parents or legal guardians of the participating children. A pre-designed questionnaire was used, which included questions related to personal hygiene practices (such as, frequency of handwashing, bathing routines, and use of communal facilities), environmental conditions, and the child’s impetigo diagnosis history. The study encompassed a total of 1200 participants, with a predominant representation of female (79.3\%). Participants exhibited a diverse age distribution, with the highest proportion falling within the 18–24 age group (33.7\%). Importantly, a statistically significant association was identified between the occurrence of impetigo in children and their personal hygiene scores (p {\textless} 0.001). Children with a confirmed impetigo diagnosis exhibited lower mean personal hygiene scores (2.6 ± 0.723) in contrast to those without such diagnoses (3.75 ± 0.911). Socio-demographic factors, including child’s gender, parental education level, employment status, and geographic location, emerge as significant determinants of impetigo occurrence. Additionally, there is a strong correlation between proper personal hygiene practices and a reduced incidence of impetigo.},
	pages = {2635--2648},
	journaltitle = {Clinical, Cosmetic and Investigational Dermatology},
	author = {Aleid, Ali M and Nukaly, Houriah Y and Almunahi, Lina K and Albwah, Ahood A and AL-Balawi, Rahaf Masoud D and AlRashdi, Mohsen H and Alkhars, Ola A and Alrasheeday, Awatif M and Alshammari, Bushra and Alabbasi, Yasmine and Al Mutair, Abbas},
	urldate = {2025-04-13},
	date = {2024-12-31},
	pmid = {39606277},
	note = {Publisher: Dove Medical Press
	eprint: https://www.tandfonline.com/doi/pdf/10.2147/{CCID}.S472228},
	keywords = {impetigo, personal hygiene, skin infections, Staphylococcus aureus, Staphylococcus pyogenes},
	file = {Full Text PDF:files/991/Aleid et al. - 2024 - Prevalence and Socio-Demographic and Hygiene Facto.pdf:application/pdf},
}
@article{Romani_2017,
	title = {The Epidemiology of Scabies and Impetigo in Relation to Demographic and Residential Characteristics: Baseline Findings from the Skin Health Intervention Fiji Trial},
	volume = {97},
	issn = {0002-9637},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5590570/},
	doi = {10.4269/ajtmh.16-0753},
	shorttitle = {The Epidemiology of Scabies and Impetigo in Relation to Demographic and Residential Characteristics},
	abstract = {Scabies and associated impetigo are under-recognized causes of morbidity in many developing countries. To strengthen the evidence base for scabies control we undertook a trial of mass treatment for scabies. We report on the occurrence and predictors of scabies and impetigo in participants at baseline. Participants were recruited in six island communities and were examined for the presence of scabies and impetigo. In addition to descriptive analyses, logistic regression models were fit to assess the association between demographic variables and outcome of interest. The study enrolled 2051 participants. Scabies prevalence was 36.4\% (95\% confidence interval [{CI}] 34.3–38.5), highest in children 5–9 years (55.7\%). Impetigo prevalence was 23.4\% (95\% {CI} 21.5–25.2) highest in children aged 10–14 (39.0\%). People with scabies were 2.8× more likely to have impetigo. The population attributable risk of scabies as a cause of impetigo was 36.3\% and 71.0\% in children aged less than five years. Households with four or more people sharing the same room were more likely to have scabies and impetigo (odds ratios [{OR}] 1.6, 95\% {CI} 1.2–2.2 and {OR} 2.3, 95\% {CI} 1.6–3.2 respectively) compared to households with rooms occupied by a single individual. This study confirms the high burden of scabies and impetigo in Fiji and the association between these two conditions, particularly in young children. Overcrowding, young age, and clinical distribution of lesion are important risk factors for scabies and impetigo. Further studies are needed to investigate whether the decline of endemic scabies would translate into a definite reduction of the burden of associated complications.},
	pages = {845--850},
	number = {3},
	journaltitle = {Am J Trop Med Hyg},
	author = {Romani, Lucia and Whitfeld, Margot J. and Koroivueta, Josefa and Kama, Mike and Wand, Handan and Tikoduadua, Lisi and Tuicakau, Meciusela and Koroi, Aminiasi and Ritova, Raijieli and Andrews, Ross and Kaldor, John M. and Steer, Andrew C.},
	urldate = {2025-04-13},
	date = {2017-09-07},
	pmid = {28722612},
	pmcid = {PMC5590570},
	file = {PubMed Central Full Text PDF:files/994/Romani et al. - 2017 - The Epidemiology of Scabies and Impetigo in Relati.pdf:application/pdf},
}

@online{Balde_2023,
	title = {Why you should use stratified split},
	url = {https://medium.com/@becaye-balde/why-you-should-use-stratified-split-bddb6dadd34e},
	abstract = {When the dataset is imbalanced, a random split might result in a training set that is not representative of the data. That is why we use…},
	titleaddon = {Medium},
	author = {Baldé, Becaye},
	urldate = {2025-04-14},
	date = {2023-04-14},
	langid = {english},
	file = {Snapshot:files/996/why-you-should-use-stratified-split-bddb6dadd34e.html:text/html},
}

@online{Taylor_2023,
	title = {Unbiased and Biased Estimators},
	url = {https://www.thoughtco.com/what-is-an-unbiased-estimator-3126502},
	abstract = {An unbiased estimator is a statistic with an expected value that matches its corresponding population parameter.},
	titleaddon = {{ThoughtCo}},
	author = {Taylor, Courtney},
	urldate = {2025-04-05},
	date = {2023-04-05},
	langid = {english},
	note = {Section: {ThoughtCo}},
	file = {Snapshot:files/982/what-is-an-unbiased-estimator-3126502.html:text/html},
}


@inproceedings{Hardt_2016,
	title = {Equality of Opportunity in Supervised Learning},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html},
	abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
	urldate = {2025-03-16},
	date = {2016},
	annotation = {Mehrabi 63
	},
}


@inproceedings{Valentim_2019,
	title = {The Impact of Data Preparation on the Fairness of Software Systems},
	url = {https://ieeexplore.ieee.org/abstract/document/8987497},
	doi = {10.1109/ISSRE.2019.00046},
	abstract = {Machine learning models are widely adopted in scenarios that directly affect people. The development of software systems based on these models raises societal and legal concerns, as their decisions may lead to the unfair treatment of individuals based on attributes like race or gender. Data preparation is key in any machine learning pipeline, but its effect on fairness is yet to be studied in detail. In this paper, we evaluate how the fairness and effectiveness of the learned models are affected by the removal of the sensitive attribute, the encoding of the categorical attributes, and instance selection methods (including cross-validators and random undersampling). We used the Adult Income and the German Credit Data datasets, which are widely studied and known to have fairness concerns. We applied each data preparation technique individually to analyse the difference in predictive performance and fairness, using statistical parity difference, disparate impact, and the normalised prejudice index. The results show that fairness is affected by transformations made to the training data, particularly in imbalanced datasets. Removing the sensitive attribute is insufficient to eliminate all the unfairness in the predictions, as expected, but it is key to achieve fairer models. Additionally, the standard random undersampling with respect to the true labels is sometimes more prejudicial than performing no random undersampling.},
	eventtitle = {2019 {IEEE} 30th International Symposium on Software Reliability Engineering ({ISSRE})},
	pages = {391--401},
	booktitle = {2019 {IEEE} 30th International Symposium on Software Reliability Engineering ({ISSRE})},
	author = {Valentim, Inês and Lourenço, Nuno and Antunes, Nuno},
	urldate = {2025-05-19},
	date = {2019-10},
	note = {{ISSN}: 2332-6549},
	keywords = {Data Preparation, Fairness, Machine Learning}
}


@misc{Sabato_2024,
	title = {Fairness and Unfairness in Binary and Multiclass Classification: Quantifying, Calculating, and Bounding},
	url = {http://arxiv.org/abs/2206.03234},
	doi = {10.48550/arXiv.2206.03234},
	shorttitle = {Fairness and Unfairness in Binary and Multiclass Classification},
	abstract = {We propose a new interpretable measure of unfairness, that allows providing a quantitative analysis of classifier fairness, beyond a dichotomous fair/unfair distinction. We show how this measure can be calculated when the classifier's conditional confusion matrices are known. We further propose methods for auditing classifiers for their fairness when the confusion matrices cannot be obtained or even estimated. Our approach lower-bounds the unfairness of a classifier based only on aggregate statistics, which may be provided by the owner of the classifier or collected from freely available data. We use the equalized odds criterion, which we generalize to the multiclass case. We report experiments on data sets representing diverse applications, which demonstrate the effectiveness and the wide range of possible uses of the proposed methodology. An implementation of the procedures proposed in this paper and as the code for running the experiments are provided in https://github.com/sivansabato/unfairness.},
	number = {{arXiv}:2206.03234},
	publisher = {{arXiv}},
	author = {Sabato, Sivan and Treister, Eran and Yom-Tov, Elad},
	urldate = {2025-05-23},
	date = {2024-04-05},
	eprinttype = {arxiv},
	eprint = {2206.03234 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:files/1006/Sabato et al. - 2024 - Fairness and Unfairness in Binary and Multiclass C.pdf:application/pdf;Snapshot:files/1005/2206.html:text/html},
}


@article{Nezami_2024,
	title = {Assessing Disparities in Predictive Modeling Outcomes for College Student Success: The Impact of Imputation Techniques on Model Performance and Fairness},
	volume = {14},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7102},
	url = {https://www.mdpi.com/2227-7102/14/2/136},
	doi = {10.3390/educsci14020136},
	shorttitle = {Assessing Disparities in Predictive Modeling Outcomes for College Student Success},
	abstract = {The education sector has been quick to recognize the power of predictive analytics to enhance student success rates. However, there are challenges to widespread adoption, including the lack of accessibility and the potential perpetuation of inequalities. These challenges present in different stages of modeling, including data preparation, model development, and evaluation. These steps can introduce additional bias to the system if not appropriately performed. Substantial incompleteness in responses is a common problem in nationally representative education data at a large scale. This can lead to missing data and can potentially impact the representativeness and accuracy of the results. While many education-related studies address the challenges of missing data, little is known about the impact of handling missing values on the fairness of predictive outcomes in practice. In this paper, we aim to assess the disparities in predictive modeling outcomes for college student success and investigate the impact of imputation techniques on model performance and fairness using various notions. We conduct a prospective evaluation to provide a less biased estimation of future performance and fairness than an evaluation of historical data. Our comprehensive analysis of a real large-scale education dataset reveals key insights on modeling disparities and the impact of imputation techniques on the fairness of the predictive outcome under different testing scenarios. Our results indicate that imputation introduces bias if the testing set follows the historical distribution. However, if the injustice in society is addressed and, consequently, the upcoming batch of observations is equalized, the model would be less biased.},
	pages = {136},
	number = {2},
	journaltitle = {Education Sciences},
	author = {Nezami, Nazanin and Haghighat, Parian and Gándara, Denisa and Anahideh, Hadis},
	urldate = {2025-05-23},
	date = {2024-02},
	langid = {english},
	note = {Number: 2
	Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {education analytics, fairness of predictive outcomes, imputation, machine learning, prospective evaluation},
	file = {Full Text PDF:files/1008/Nezami et al. - 2024 - Assessing Disparities in Predictive Modeling Outco.pdf:application/pdf},
}


@misc{Putzel_2022,
	title = {Blackbox Post-Processing for Multiclass Fairness},
	url = {http://arxiv.org/abs/2201.04461},
	doi = {10.48550/arXiv.2201.04461},
	abstract = {Applying standard machine learning approaches for classification can produce unequal results across different demographic groups. When then used in real-world settings, these inequities can have negative societal impacts. This has motivated the development of various approaches to fair classification with machine learning models in recent years. In this paper, we consider the problem of modifying the predictions of a blackbox machine learning classifier in order to achieve fairness in a multiclass setting. To accomplish this, we extend the 'post-processing' approach in Hardt et al. 2016, which focuses on fairness for binary classification, to the setting of fair multiclass classification. We explore when our approach produces both fair and accurate predictions through systematic synthetic experiments and also evaluate discrimination-fairness tradeoffs on several publicly available real-world application datasets. We find that overall, our approach produces minor drops in accuracy and enforces fairness when the number of individuals in the dataset is high relative to the number of classes and protected groups.},
	number = {{arXiv}:2201.04461},
	publisher = {{arXiv}},
	author = {Putzel, Preston and Lee, Scott},
	urldate = {2025-05-23},
	date = {2022-01-12},
	eprinttype = {arxiv},
	eprint = {2201.04461 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1011/Putzel und Lee - 2022 - Blackbox Post-Processing for Multiclass Fairness.pdf:application/pdf;Snapshot:files/1010/2201.html:text/html},
}

@misc{Chen_2024,
	title = {Unmasking Bias in {AI}: A Systematic Review of Bias Detection and Mitigation Strategies in Electronic Health Record-based Models},
	url = {http://arxiv.org/abs/2310.19917},
	doi = {10.48550/arXiv.2310.19917},
	shorttitle = {Unmasking Bias in {AI}},
	abstract = {Objectives: Leveraging artificial intelligence ({AI}) in conjunction with electronic health records ({EHRs}) holds transformative potential to improve healthcare. Yet, addressing bias in {AI}, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to detect and mitigate diverse forms of bias in {AI} models developed using {EHR} data. Methods: We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses ({PRISMA}) guidelines, analyzing articles from {PubMed}, Web of Science, and {IEEE} published between January 1, 2010, and Dec 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the {AI} model development process, and analyzed metrics for bias assessment. Results: Of the 450 articles retrieved, 20 met our criteria, revealing six major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The {AI} models were primarily developed for predictive tasks in healthcare settings. Four studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Sixty proposed various strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance (e.g., accuracy, {AUROC}) and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling, reweighting, and transformation. Discussion: This review highlights the varied and evolving nature of strategies to address bias in {EHR}-based {AI} models, emphasizing the urgent needs for the establishment of standardized, generalizable, and interpretable methodologies to foster the creation of ethical {AI} systems that promote fairness and equity in healthcare.},

	number = {{arXiv}:2310.19917},
	publisher = {{arXiv}},
	author = {Chen, Feng and Wang, Liqin and Hong, Julie and Jiang, Jiaqi and Zhou, Li},
	urldate = {2025-05-26},
	date = {2024-07-01},
	eprinttype = {arxiv},
	eprint = {2310.19917 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods},
	annotation = {Comment: Published in {JAMIA} Volume 31, Issue 5, May 2024},
	file = {Preprint PDF:files/1027/Chen et al. - 2024 - Unmasking Bias in AI A Systematic Review of Bias .pdf:application/pdf;Snapshot:files/1026/2310.html:text/html},
}


@inproceedings{Petersen_2009,
	title = {The waterfall model in large-scale development},
	url = {https://urn.kb.se/resolve?urn=urn:nbn:se:bth-8073},
	abstract = {Waterfall development is still a widely used way of working in software development companies. Many problems have been reported related to the model. Commonly accepted problems are for example to c ...},
	eventtitle = {10th International Conference on Product-Focused Software Process Improvement},
	publisher = {Springer},
	author = {Petersen, Kai and Wohlin, Claes and Baca, Dejan},
	urldate = {2025-05-26},
	date = {2009},
}

@online{Alake_2021,
	title = {How to Read Research Papers: A Pragmatic Approach for {ML} Practitioners},
	url = {https://developer.nvidia.com/blog/how-to-read-research-papers-a-pragmatic-approach-for-ml-practitioners/},
	shorttitle = {How to Read Research Papers},
	abstract = {This post presents a systematic approach to reading research papers, a useful skill for machine learning practitioners.},
	titleaddon = {{NVIDIA} Technical Blog},
	author = {Alake, Richmond},
	urldate = {2025-05-26},
	date = {2021-12-15},
	langid = {american},
}

@misc{Barr_2025,
	title = {A Review of Fairness and A Practical Guide to Selecting Context-Appropriate Fairness Metrics in Machine Learning},
	url = {http://arxiv.org/abs/2411.06624},
	doi = {10.48550/arXiv.2411.06624},
	abstract = {Recent regulatory proposals for artificial intelligence emphasize fairness requirements for machine learning models. However, precisely defining the appropriate measure of fairness is challenging due to philosophical, cultural and political contexts. Biases can infiltrate machine learning models in complex ways depending on the model's context, rendering a single common metric of fairness insufficient. This ambiguity highlights the need for criteria to guide the selection of context-aware measures, an issue of increasing importance given the proliferation of ever tighter regulatory requirements. To address this, we developed a flowchart to guide the selection of contextually appropriate fairness measures. Twelve criteria were used to formulate the flowchart. This included consideration of model assessment criteria, model selection criteria, and data bias. We also review fairness literature in the context of machine learning and link it to core regulatory instruments to assist policymakers, {AI} developers, researchers, and other stakeholders in appropriately addressing fairness concerns and complying with relevant regulatory requirements.},
	number = {{arXiv}:2411.06624},
	publisher = {{arXiv}},
	author = {Barr, Caleb J. S. and Erdelyi, Olivia and Docherty, Paul D. and Grace, Randolph C.},
	urldate = {2025-05-26},
	date = {2025-02-11},
	eprinttype = {arxiv},
	eprint = {2411.06624 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annotation = {Comment: 24 pages, 5 figures, 1 table},
}

@misc{Agarwal_2018,
	title = {A Reductions Approach to Fair Classification},
	url = {http://arxiv.org/abs/1803.02453},
	doi = {10.48550/arXiv.1803.02453},
	abstract = {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.},
	number = {{arXiv}:1803.02453},
	publisher = {{arXiv}},
	author = {Agarwal, Alekh and Beygelzimer, Alina and Dudík, Miroslav and Langford, John and Wallach, Hanna},
	urldate = {2025-06-03},
	date = {2018-07-16},
	eprinttype = {arxiv},
	eprint = {1803.02453 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:files/1036/Agarwal et al. - 2018 - A Reductions Approach to Fair Classification.pdf:application/pdf;Snapshot:files/1035/1803.html:text/html},
}


@online{Fairlearn_nodate,
	title = {{API} Docs — Fairlearn 0.13.0.dev0 documentation},
	url = {https://fairlearn.org/main/api_reference/index.html},
	author = {{Fairlearn contributors}},
	urldate = {2025-06-03},
	file = {API Docs — Fairlearn 0.13.0.dev0 documentation:files/1038/index.html:text/html},
}


@online{Sklearn_nodate,
	title = {{StratifiedKFold}},
	url = {https://scikit-learn/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html},
	abstract = {Gallery examples: Recursive feature elimination with cross-validation {GMM} covariances Receiver Operating Characteristic ({ROC}) with cross validation Test with permutations the significance of a clas...},
	titleaddon = {scikit-learn},
	author = {scikit-learn developers},
	urldate = {2025-06-04},
	langid = {english},
	file = {Snapshot:files/1040/sklearn.model_selection.StratifiedKFold.html:text/html},
}


@article{Hameed_2020,
	title = {Multi-class multi-level classification algorithm for skin lesions classification using machine learning techniques},
	volume = {141},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419306797},
	doi = {10.1016/j.eswa.2019.112961},
	abstract = {Skin diseases remain a major cause of disability worldwide and contribute approximately 1.79\% of the global burden of disease measured in disability-adjusted life years. In the United Kingdom alone, 60\% of the population suffer from skin diseases during their lifetime. In this paper, we propose an intelligent digital diagnosis scheme to improve the classification accuracy of multiple diseases. A Multi-Class Multi-Level ({MCML}) classification algorithm inspired by the “divide and conquer” rule is explored to address the research challenges. The {MCML} classification algorithm is implemented using traditional machine learning and advanced deep learning approaches. Improved techniques are proposed for noise removal in the traditional machine learning approach. The proposed algorithm is evaluated on 3672 classified images, collected from different sources and the diagnostic accuracy of 96.47\% is achieved. To verify the performance of the proposed algorithm, its metrics are compared with the Multi-Class Single-Level classification algorithm which is the main algorithm used in most of the existing literature. The results also indicate that the {MCML} classification algorithm is capable of enhancing the classification performance of multiple skin lesions.},
	pages = {112961},
	journaltitle = {Expert Systems with Applications},
	author = {Hameed, Nazia and Shabut, Antesar M. and Ghosh, Miltu K. and Hossain, M. A.},
	urldate = {2025-03-10},
	date = {2020-03-01},
	keywords = {Machine learning, Deep learning, Computer-aided diagnosise, Eczema classification, Melanoma classification, Skin lesion classification, Texture \& colour features},
	file = {Eingereichte Version:files/685/Hameed et al. - 2020 - Multi-class multi-level classification algorithm f.pdf:application/pdf;ScienceDirect Snapshot:files/686/S0957417419306797.html:text/html},
}


@online{Wang_2021,
	title = {Practice {AI} Responsibly with Proxy Variable Detection},
	url = {https://medium.com/bcggamma/practice-ai-responsibly-with-proxy-variable-detection-42c2156ad986},
	titleaddon = {{GAMMA} — Part of {BCG} X},
	author = {Wang, Sibo and Santinelli, Max and Hua, Guangying},
	commentator = {Ittner, Jan and Mills, Steve},
	urldate = {2025-06-05},
	date = {2021-03-11},
	langid = {english},
	file = {Snapshot:files/1042/practice-ai-responsibly-with-proxy-variable-detection-42c2156ad986.html:text/html},
}

@inreference{Farlex_nodate,
	title = {pediatric},
	url = {https://medical-dictionary.thefreedictionary.com/pediatric},
	booktitle = {The Free Dictionary},
	author = {Farlex},
	urldate = {2025-06-05},
}
