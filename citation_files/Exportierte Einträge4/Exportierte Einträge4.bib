
@misc{shankar_no_2017,
	title = {No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World},
	url = {http://arxiv.org/abs/1711.08536},
	doi = {10.48550/arXiv.1711.08536},
	shorttitle = {No Classification without Representation},
	abstract = {Modern machine learning systems such as image classifiers rely heavily on large scale data sets for training. Such data sets are costly to create, thus in practice a small number of freely available, open source data sets are widely used. We suggest that examining the geo-diversity of open data sets is critical before adopting a data set for use cases in the developing world. We analyze two large, publicly available image data sets to assess geo-diversity and find that these data sets appear to exhibit an observable amerocentric and eurocentric representation bias. Further, we analyze classifiers trained on these data sets to assess the impact of these training distributions and find strong differences in the relative performance on images from different locales. These results emphasize the need to ensure geo-representation when constructing data sets for use in the developing world.},
	number = {{arXiv}:1711.08536},
	publisher = {{arXiv}},
	author = {Shankar, Shreya and Halpern, Yoni and Breck, Eric and Atwood, James and Wilson, Jimbo and Sculley, D.},
	urldate = {2025-04-03},
	date = {2017-11-22},
	eprinttype = {arxiv},
	eprint = {1711.08536 [stat]},
	keywords = {Statistics - Machine Learning},
	annotation = {Mehrabi 142Comment: Presented at {NIPS} 2017 Workshop on Machine Learning for the Developing World
},
	file = {Preprint PDF:files/955/Shankar et al. - 2017 - No Classification without Representation Assessin.pdf:application/pdf;Snapshot:files/956/1711.html:text/html},
}

@article{fry_comparison_2017,
	title = {Comparison of Sociodemographic and Health-Related Characteristics of {UK} Biobank Participants With Those of the General Population},
	volume = {186},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/aje/kwx246},
	doi = {10.1093/aje/kwx246},
	abstract = {The {UK} Biobank cohort is a population-based cohort of 500,000 participants recruited in the United Kingdom ({UK}) between 2006 and 2010. Approximately 9.2 million individuals aged 40–69 years who lived within 25 miles (40 km) of one of 22 assessment centers in England, Wales, and Scotland were invited to enter the cohort, and 5.5\% participated in the baseline assessment. The representativeness of the {UK} Biobank cohort was investigated by comparing demographic characteristics between nonresponders and responders. Sociodemographic, physical, lifestyle, and health-related characteristics of the cohort were compared with nationally representative data sources. {UK} Biobank participants were more likely to be older, to be female, and to live in less socioeconomically deprived areas than nonparticipants. Compared with the general population, participants were less likely to be obese, to smoke, and to drink alcohol on a daily basis and had fewer self-reported health conditions. At age 70–74 years, rates of all-cause mortality and total cancer incidence were 46.2\% and 11.8\% lower, respectively, in men and 55.5\% and 18.1\% lower, respectively, in women than in the general population of the same age. {UK} Biobank is not representative of the sampling population; there is evidence of a “healthy volunteer” selection bias. Nonetheless, valid assessment of exposure-disease relationships may be widely generalizable and does not require participants to be representative of the population at large.},
	pages = {1026--1034},
	number = {9},
	journaltitle = {American Journal of Epidemiology},
	author = {Fry, Anna and Littlejohns, Thomas J and Sudlow, Cathie and Doherty, Nicola and Adamska, Ligia and Sprosen, Tim and Collins, Rory and Allen, Naomi E},
	urldate = {2025-04-03},
	date = {2017-11-01},
	annotation = {Mehrabi 54
},
	file = {Full Text PDF:files/958/Fry et al. - 2017 - Comparison of Sociodemographic and Health-Related .pdf:application/pdf;Snapshot:files/959/3883629.html:text/html},
}

@inproceedings{chen_fairness_2019,
	location = {New York, {NY}, {USA}},
	title = {Fairness Under Unawareness: Assessing Disparity When Protected Class Is Unobserved},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287594},
	doi = {10.1145/3287560.3287594},
	series = {{FAT}* '19},
	shorttitle = {Fairness Under Unawareness},
	abstract = {Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.},
	pages = {339--348},
	booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	publisher = {Association for Computing Machinery},
	author = {Chen, Jiahao and Kallus, Nathan and Mao, Xiaojie and Svacha, Geoffry and Udell, Madeleine},
	urldate = {2025-04-03},
	date = {2019-01-29},
	annotation = {Mehrabi 30
},
	file = {Full Text PDF:files/962/Chen et al. - 2019 - Fairness Under Unawareness Assessing Disparity Wh.pdf:application/pdf},
}

@misc{zhao_men_2017,
	title = {Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints},
	url = {http://arxiv.org/abs/1707.09457},
	doi = {10.48550/arXiv.1707.09457},
	shorttitle = {Men Also Like Shopping},
	abstract = {Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33\% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68\% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5\% and 40.5\% for multilabel classification and visual semantic role labeling, respectively.},
	number = {{arXiv}:1707.09457},
	publisher = {{arXiv}},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	urldate = {2025-04-03},
	date = {2017-07-29},
	eprinttype = {arxiv},
	eprint = {1707.09457 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	annotation = {Mehrabi 167 Comment: 11 pages, published in {EMNLP} 2017
},
	file = {Preprint PDF:files/966/Zhao et al. - 2017 - Men Also Like Shopping Reducing Gender Bias Ampli.pdf:application/pdf;Snapshot:files/967/1707.html:text/html},
}

@inproceedings{bolukbasi_man_2016,
	title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
	shorttitle = {Man is to Computer Programmer as Woman is to Homemaker?},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female.  Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
	urldate = {2025-04-03},
	date = {2016},
	annotation = {Mehrabi 20
},
	file = {Full Text PDF:files/969/Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homem.pdf:application/pdf},
}

@misc{zhao_gender_2018,
	title = {Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods},
	url = {http://arxiv.org/abs/1804.06876},
	doi = {10.48550/arXiv.1804.06876},
	shorttitle = {Gender Bias in Coreference Resolution},
	abstract = {We introduce a new benchmark, {WinoBias}, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in {WinoBias} without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at http://winobias.org.},
	number = {{arXiv}:1804.06876},
	publisher = {{arXiv}},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	urldate = {2025-04-03},
	date = {2018-04-18},
	eprinttype = {arxiv},
	eprint = {1804.06876 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annotation = {Mehrabi 168, Comment: {NAACL} '18 Camera Ready
},
	file = {Preprint PDF:files/973/Zhao et al. - 2018 - Gender Bias in Coreference Resolution Evaluation .pdf:application/pdf;Snapshot:files/974/1804.html:text/html},
}

@article{hajian_methodology_2013,
	title = {A Methodology for Direct and Indirect Discrimination Prevention in Data Mining},
	volume = {25},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/6175897},
	doi = {10.1109/TKDE.2012.72},
	abstract = {Data mining is an increasingly important technology for extracting useful knowledge hidden in large collections of data. There are, however, negative social perceptions about data mining, among which potential privacy invasion and potential discrimination. The latter consists of unfairly treating people on the basis of their belonging to a specific group. Automated data collection and data mining techniques such as classification rule mining have paved the way to making automated decisions, like loan granting/denial, insurance premium computation, etc. If the training data sets are biased in what regards discriminatory (sensitive) attributes like gender, race, religion, etc., discriminatory decisions may ensue. For this reason, antidiscrimination techniques including discrimination discovery and prevention have been introduced in data mining. Discrimination can be either direct or indirect. Direct discrimination occurs when decisions are made based on sensitive attributes. Indirect discrimination occurs when decisions are made based on nonsensitive attributes which are strongly correlated with biased sensitive ones. In this paper, we tackle discrimination prevention in data mining and propose new techniques applicable for direct or indirect discrimination prevention individually or both at the same time. We discuss how to clean training data sets and outsourced data sets in such a way that direct and/or indirect discriminatory decision rules are converted to legitimate (nondiscriminatory) classification rules. We also propose new metrics to evaluate the utility of the proposed approaches and we compare these approaches. The experimental evaluations demonstrate that the proposed techniques are effective at removing direct and/or indirect discrimination biases in the original data set while preserving data quality.},
	pages = {1445--1459},
	number = {7},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Hajian, Sara and Domingo-Ferrer, Josep},
	urldate = {2025-04-03},
	date = {2013-07},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Antidiscrimination, Data engineering, data mining, Data mining, direct and indirect discrimination prevention, Itemsets, Knowledge engineering, privacy, rule generalization, rule protection, Training, Training data},
	annotation = {Mehrabi 62
},
	file = {Full Text PDF:files/976/Hajian und Domingo-Ferrer - 2013 - A Methodology for Direct and Indirect Discriminati.pdf:application/pdf;IEEE Xplore Abstract Record:files/978/6175897.html:text/html},
}
