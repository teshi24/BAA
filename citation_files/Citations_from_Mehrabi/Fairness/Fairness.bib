
@article{berk_fairness_2017,
	title = {Fairness in Criminal Justice Risk Assessments: The State of the Art},
	volume = {50},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124118782533},
	doi = {10.1177/0049124118782533},
	shorttitle = {Fairness in Criminal Justice Risk Assessments},
	abstract = {Objectives:Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this article, we seek to clarify the trade-offs between different kinds of fairness and between fairness and accuracy.Methods:We draw on the existing literatures in criminology, computer science, and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments.Results:We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy.Conclusions:Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging trade-offs. These lessons apply to applications well beyond criminology where assessments of risk can be used by decision makers. Examples include mortgage lending, employment, college admissions, child welfare, and medical diagnoses.},
	pages = {3--44},
	number = {1},
	journaltitle = {Sociological Methods \& Research},
	author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Kearns, Michael and Roth, Aaron},
	urldate = {2025-03-16},
	date = {2017},
	note = {Publisher: {SAGE} Publications Inc},
	annotation = {Mehrabi 15
},
	file = {Eingereichte Version:files/745/Berk et al. - 2021 - Fairness in Criminal Justice Risk Assessments The.pdf:application/pdf},
}

@article{chouldechova_fair_2017,
	title = {Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments},
	volume = {5},
	issn = {2167-6461},
	url = {https://www.liebertpub.com/doi/abs/10.1089/big.2016.0047},
	doi = {10.1089/big.2016.0047},
	shorttitle = {Fair Prediction with Disparate Impact},
	abstract = {Recidivism prediction instruments ({RPIs}) provide decision-makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. Although such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This article discusses several fairness criteria that have recently been applied to assess the fairness of {RPIs}. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when an {RPI} fails to satisfy the criterion of error rate balance.},
	pages = {153--163},
	number = {2},
	journaltitle = {Big Data},
	author = {Chouldechova, Alexandra},
	urldate = {2025-03-16},
	date = {2017-06},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	annotation = {Mehrabi 34
},
	file = {Eingereichte Version:files/748/Chouldechova - 2017 - Fair Prediction with Disparate Impact A Study of .pdf:application/pdf},
}

@inproceedings{corbett-davies_algorithmic_2017,
	location = {New York, {NY}, {USA}},
	title = {Algorithmic Decision Making and the Cost of Fairness},
	isbn = {978-1-4503-4887-4},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098095},
	doi = {10.1145/3097983.3098095},
	series = {{KDD} '17},
	abstract = {Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally differ, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-off can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.},
	pages = {797--806},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
	urldate = {2025-03-16},
	date = {2017-08-04},
	annotation = {Mehrabi 41
},
	file = {Full Text PDF:files/751/Corbett-Davies et al. - 2017 - Algorithmic Decision Making and the Cost of Fairne.pdf:application/pdf},
}

@inproceedings{dwork_fairness_2012,
	location = {New York, {NY}, {USA}},
	title = {Fairness through awareness},
	isbn = {978-1-4503-1115-1},
	url = {https://dl.acm.org/doi/10.1145/2090236.2090255},
	doi = {10.1145/2090236.2090255},
	series = {{ITCS} '12},
	abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	pages = {214--226},
	booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
	publisher = {Association for Computing Machinery},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	urldate = {2025-03-16},
	date = {2012-01-08},
	annotation = {Mehrabi 48
},
	file = {Full Text PDF:files/754/Dwork et al. - 2012 - Fairness through awareness.pdf:application/pdf},
}

@inproceedings{farnadi_fairness_2018,
	location = {New York, {NY}, {USA}},
	title = {Fairness in Relational Domains},
	isbn = {978-1-4503-6012-8},
	url = {https://dl.acm.org/doi/10.1145/3278721.3278733},
	doi = {10.1145/3278721.3278733},
	series = {{AIES} '18},
	abstract = {{AI} and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic ({PSL}), to incorporate our definition of relational fairness. We refer to this fairness-aware framework {FairPSL}. {FairPSL} makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori({MAP}) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.},
	pages = {108--114},
	booktitle = {Proceedings of the 2018 {AAAI}/{ACM} Conference on {AI}, Ethics, and Society},
	publisher = {Association for Computing Machinery},
	author = {Farnadi, Golnoosh and Babaki, Behrouz and Getoor, Lise},
	urldate = {2025-03-16},
	date = {2018-12-27},
	annotation = {Mehrabi 50
},
	file = {Full Text PDF:files/757/Farnadi et al. - 2018 - Fairness in Relational Domains.pdf:application/pdf},
}

@article{grgic-hlaca_case_2016,
	title = {The Case for Process Fairness in Learning: Feature Selection for Fair Decision Making},
	abstract = {Machine learning methods are increasingly being used to inform, or sometimes even directly to make, important decisions about humans. A number of recent works have focussed on the fairness of the outcomes of such decisions, particularly on avoiding decisions that affect users of different sensitive groups (e.g., race, gender) disparately. In this paper, we propose to consider the fairness of the process of decision making. Process fairness can be measured by estimating the degree to which people consider various features to be fair to use when making an important legal decision. We examine the task of predicting whether or not a prisoner is likely to commit a crime again once released by analyzing the dataset considered by {ProPublica} relating to the {COMPAS} system. We introduce new measures of people’s discomfort with using various features, show how these measures can be estimated, and consider the effect of removing the uncomfortable features on prediction accuracy and on outcome fairness. Our empirical analysis suggests that process fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.},
	author = {Grgic-Hlacˇa, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
	date = {2016},
	langid = {english},
	annotation = {Mehrabi 61
},
	file = {Grgic-Hlacˇa et al. - The Case for Process Fairness in Learning Feature.pdf:files/759/Grgic-Hlacˇa et al. - The Case for Process Fairness in Learning Feature.pdf:application/pdf},
}

@inproceedings{hardt_equality_2016,
	title = {Equality of Opportunity in Supervised Learning},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html},
	abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
	urldate = {2025-03-16},
	date = {2016},
	annotation = {Mehrabi 63
},
	file = {Full Text PDF:files/763/Hardt et al. - 2016 - Equality of Opportunity in Supervised Learning.pdf:application/pdf},
}

@inproceedings{kearns_preventing_2018,
	title = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
	url = {https://proceedings.mlr.press/v80/kearns18a.html},
	shorttitle = {Preventing Fairness Gerrymandering},
	abstract = {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.},
	eventtitle = {International Conference on Machine Learning},
	pages = {2564--2572},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	urldate = {2025-03-16},
	date = {2018-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	annotation = {Mehrabi 79
},
	file = {Full Text PDF:files/766/Kearns et al. - 2018 - Preventing Fairness Gerrymandering Auditing and L.pdf:application/pdf},
}

@inproceedings{kearns_empirical_2019,
	location = {New York, {NY}, {USA}},
	title = {An Empirical Study of Rich Subgroup Fairness for Machine Learning},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287592},
	doi = {10.1145/3287560.3287592},
	series = {{FAT}* '19},
	abstract = {Kearns, Neel, Roth, and Wu [{ICML} 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded {VC} dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [{ICML} 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.},
	pages = {100--109},
	booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	publisher = {Association for Computing Machinery},
	author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	urldate = {2025-03-16},
	date = {2019-01-29},
	annotation = {Mehrabi 80
},
	file = {Full Text PDF:files/769/Kearns et al. - 2019 - An Empirical Study of Rich Subgroup Fairness for M.pdf:application/pdf},
}

@inproceedings{kusner_counterfactual_2017,
	title = {Counterfactual Fairness},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
	abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.  In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.  Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it  the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
	urldate = {2025-03-16},
	date = {2017},
	annotation = {Mehrabi 87
},
	file = {Full Text PDF:files/772/Kusner et al. - 2017 - Counterfactual Fairness.pdf:application/pdf},
}

@inproceedings{verma_fairness_2018,
	location = {New York, {NY}, {USA}},
	title = {Fairness definitions explained},
	isbn = {978-1-4503-5746-3},
	url = {https://dl.acm.org/doi/10.1145/3194770.3194776},
	doi = {10.1145/3194770.3194776},
	series = {{FairWare} '18},
	abstract = {Algorithm fairness has started to attract the attention of researchers in {AI}, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
	pages = {1--7},
	booktitle = {Proceedings of the International Workshop on Software Fairness},
	publisher = {Association for Computing Machinery},
	author = {Verma, Sahil and Rubin, Julia},
	urldate = {2025-03-16},
	date = {2018-05-29},
	annotation = {Mehrabi 149
},
	file = {Full Text PDF:files/775/Verma und Rubin - 2018 - Fairness definitions explained.pdf:application/pdf},
}
