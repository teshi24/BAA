
@article{baeza-yates_bias_2018,
	title = {Bias on the web},
	volume = {61},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3209581},
	doi = {10.1145/3209581},
	abstract = {Bias in Web data and use taints the algorithms behind Web-based applications, delivering equally biased results.},
	pages = {54--61},
	number = {6},
	journaltitle = {Commun. {ACM}},
	author = {Baeza-Yates, Ricardo},
	urldate = {2025-03-16},
	date = {2018-05-23},
	annotation = {Mehrabi 9
},
	file = {Full Text PDF:files/705/Baeza-Yates - 2018 - Bias on the web.pdf:application/pdf},
}

@inproceedings{buolamwini_gender_2018,
	title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	shorttitle = {Gender Shades},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, {IJB}-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for {IJB}-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	eventtitle = {Conference on Fairness, Accountability and Transparency},
	pages = {77--91},
	booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	publisher = {{PMLR}},
	author = {Buolamwini, Joy and Gebru, Timnit},
	urldate = {2025-03-16},
	date = {2018-01-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	annotation = {Mehrabi 24, demographic (skin type and gender)
},
	file = {Full Text PDF:files/708/Buolamwini und Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities.pdf:application/pdf;Supplementary PDF:files/709/Buolamwini und Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities.pdf:application/pdf},
}

@article{clarke_phantom_2005,
	title = {The Phantom Menace: Omitted Variable Bias in Econometric Research},
	volume = {22},
	issn = {0738-8942},
	url = {https://doi.org/10.1080/07388940500339183},
	doi = {10.1080/07388940500339183},
	shorttitle = {The Phantom Menace},
	abstract = {Quantitative political science is awash in control variables. The justification for these bloated specifications is usually the fear of omitted variable bias. A key underlying assumption is that the danger posed by omitted variable bias can be ameliorated by the inclusion of relevant control variables. Unfortunately, as this article demonstrates, there is nothing in the mathematics of regression analysis that supports this conclusion. The inclusion of additional control variables may increase or decrease the bias, and we cannot know for sure which is the case in any particular situation. A brief discussion of alternative strategies for achieving experimental control follows the main result.},
	pages = {341--352},
	number = {4},
	journaltitle = {Conflict Management and Peace Science},
	author = {Clarke, Kevin A.},
	urldate = {2025-03-16},
	date = {2005-09-01},
	note = {Publisher: {SAGE} Publications Ltd},
	annotation = {Mehrabi 38, difficultis regarding ommitted variable and overcoming methods
},
	file = {SAGE PDF Full Text:files/712/Clarke - 2005 - The Phantom Menace Omitted Variable Bias in Econo.pdf:application/pdf},
}

@inproceedings{danks_algorithmic_2017,
	location = {Melbourne, Australia},
	title = {Algorithmic Bias in Autonomous Systems},
	isbn = {978-0-9992411-0-3},
	url = {https://www.ijcai.org/proceedings/2017/654},
	doi = {10.24963/ijcai.2017/654},
	abstract = {Algorithms play a key role in the functioning of autonomous systems, and so concerns have periodically been raised about the possibility of algorithmic bias. However, debates in this area have been hampered by different meanings and uses of the term, “bias.” It is sometimes used as a purely descriptive term, sometimes as a pejorative term, and such variations can promote confusion and hamper discussions about when and how to respond to algorithmic bias. In this paper, we first provide a taxonomy of different types and sources of algorithmic bias, with a focus on their different impacts on the proper functioning of autonomous systems. We then use this taxonomy to distinguish between algorithmic biases that are neutral or unobjectionable, and those that are problematic in some way and require a response. In some cases, there are technological or algorithmic adjustments that developers can use to compensate for problematic bias. In other cases, however, responses require adjustments by the agent, whether human or autonomous system, who uses the results of the algorithm. There is no “one size fits all” solution to algorithmic bias.},
	eventtitle = {Twenty-Sixth International Joint Conference on Artificial Intelligence},
	pages = {4691--4697},
	booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Danks, David and London, Alex John},
	urldate = {2025-03-16},
	date = {2017-08},
	langid = {english},
	annotation = {Mehrabi 44
},
	file = {Danks und London - 2017 - Algorithmic Bias in Autonomous Systems.pdf:files/714/Danks und London - 2017 - Algorithmic Bias in Autonomous Systems.pdf:application/pdf},
}

@article{friedman_bias_1996,
	title = {Bias in computer systems},
	volume = {14},
	issn = {1046-8188},
	url = {https://dl.acm.org/doi/10.1145/230538.230561},
	doi = {10.1145/230538.230561},
	abstract = {From an analysis of actual cases, three categories of bias in computer systems have been developed: preexisting, technical, and emergent. Preexisting bias has its roots in social institutions, practices, and attitudes. Technical bias arises from technical constraints of considerations. Emergent bias arises in a context of use. Although others have pointed to bias inparticular computer systems and have noted the general problem, we know of no comparable work that examines this phenomenon comprehensively and which offers a framework for understanding and remedying it. We conclude by suggesting that freedom from bias should by counted amoung the select set of criteria—including reliability, accuracy, and efficiency—according to which the quality of systems in use in society should be judged.},
	pages = {330--347},
	number = {3},
	journaltitle = {{ACM} Trans. Inf. Syst.},
	author = {Friedman, Batya and Nissenbaum, Helen},
	urldate = {2025-03-16},
	date = {1996-07-01},
	annotation = {Mehrabi 53
},
	file = {Full Text PDF:files/718/Friedman und Nissenbaum - 1996 - Bias in computer systems.pdf:application/pdf},
}

@article{hargittai_whose_2007,
	title = {Whose Space? Differences among Users and Non-Users of Social Network Sites},
	volume = {13},
	issn = {1083-6101},
	url = {https://doi.org/10.1111/j.1083-6101.2007.00396.x},
	doi = {10.1111/j.1083-6101.2007.00396.x},
	shorttitle = {Whose Space?},
	abstract = {Are there systematic differences between people who use social network sites and those who stay away, despite a familiarity with them? Based on data from a survey administered to a diverse group of young adults, this article looks at the predictors of {SNS} usage, with particular focus on Facebook, {MySpace}, Xanga, and Friendster. Findings suggest that use of such sites is not randomly distributed across a group of highly wired users. A person’s gender, race and ethnicity, and parental educational background are all associated with use, but in most cases only when the aggregate concept of social network sites is disaggregated by service. Additionally, people with more experience and autonomy of use are more likely to be users of such sites. Unequal participation based on user background suggests that differential adoption of such services may be contributing to digital inequality.},
	pages = {276--297},
	number = {1},
	journaltitle = {Journal of Computer-Mediated Communication},
	author = {Hargittai, Eszter},
	urldate = {2025-03-16},
	date = {2007-10-01},
	annotation = {Mehrabi 64
},
	file = {Full Text PDF:files/721/Hargittai - 2007 - Whose Space Differences among Users and Non-Users.pdf:application/pdf;Snapshot:files/722/4583068.html:text/html},
}

@article{lerman_leveraging_2014,
	title = {Leveraging Position Bias to Improve Peer Recommendation},
	volume = {9},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0098914},
	doi = {10.1371/journal.pone.0098914},
	abstract = {With the advent of social media and peer production, the amount of new online content has grown dramatically. To identify interesting items in the vast stream of new content, providers must rely on peer recommendation to aggregate opinions of their many users. Due to human cognitive biases, the presentation order strongly affects how people allocate attention to the available content. Moreover, we can manipulate attention through the presentation order of items to change the way peer recommendation works. We experimentally evaluate this effect using Amazon Mechanical Turk. We find that different policies for ordering content can steer user attention so as to improve the outcomes of peer recommendation.},
	pages = {e98914},
	number = {6},
	journaltitle = {{PLOS} {ONE}},
	author = {Lerman, Kristina and Hogg, Tad},
	urldate = {2025-03-16},
	date = {2014-06-11},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Arithmetic, Attention, Intelligence, Permutation, Psychological attitudes, Social influence, Social media, Social systems},
	annotation = {Mehrabi 93
},
	file = {Full Text PDF:files/725/Lerman und Hogg - 2014 - Leveraging Position Bias to Improve Peer Recommend.pdf:application/pdf},
}

@article{mustard_reexamining_2003,
	title = {Reexamining Criminal Behavior: The Importance of Omitted Variable Bias},
	volume = {85},
	issn = {0034-6535},
	url = {https://doi.org/10.1162/rest.2003.85.1.205},
	doi = {10.1162/rest.2003.85.1.205},
	shorttitle = {Reexamining Criminal Behavior},
	abstract = {Recently many papers have used the arrest rate to measure punishments in crime-rate regressions. However, arrest rates account for only a portion of the criminal sanction. Conviction rates and time served are theoretically important, but rarely used, and excluding them generates omitted variable bias if they are correlated with the arrest rate. This paper uses the most complete set of conviction and sentencing data to show that arrest rates are negatively correlated with these normally excluded variables. Consequently, previous estimates of arrest-rate effects are understated by as much as 50\%. Also, conviction rates, but not sentence lengths, have significant explanatory power in standard crime-rate regressions.},
	pages = {205--211},
	number = {1},
	journaltitle = {The Review of Economics and Statistics},
	author = {Mustard, David B.},
	urldate = {2025-03-16},
	date = {2003-02-01},
	annotation = {Mehrabi 114
},
	file = {Snapshot:files/728/Reexamining-Criminal-Behavior-The-Importance-of.html:text/html},
}

@article{ciampaglia_how_2018,
	title = {How algorithmic popularity bias hinders or promotes quality},
	volume = {8},
	rights = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-34203-2},
	doi = {10.1038/s41598-018-34203-2},
	abstract = {Algorithms that favor popular items are used to help us select among many choices, from top-ranked search engine results to highly-cited scientific papers. The goal of these algorithms is to identify high-quality items such as reliable news, credible information sources, and important discoveries–in short, high-quality content should rank at the top. Prior work has shown that choosing what is popular may amplify random fluctuations and lead to sub-optimal rankings. Nonetheless, it is often assumed that recommending what is popular will help high-quality content “bubble up” in practice. Here we identify the conditions in which popularity may be a viable proxy for quality content by studying a simple model of a cultural market endowed with an intrinsic notion of quality. A parameter representing the cognitive cost of exploration controls the trade-off between quality and popularity. Below and above a critical exploration cost, popularity bias is more likely to hinder quality. But we find a narrow intermediate regime of user attention where an optimal balance exists: choosing what is popular can help promote high-quality items to the top. These findings clarify the effects of algorithmic popularity bias on quality outcomes, and may inform the design of more principled mechanisms for techno-social cultural markets.},
	pages = {15951},
	number = {1},
	journaltitle = {Sci Rep},
	author = {Ciampaglia, Giovanni Luca and Nematzadeh, Azadeh and Menczer, Filippo and Flammini, Alessandro},
	urldate = {2025-03-16},
	date = {2018-10-29},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Complex networks, Statistics},
	annotation = {Mehrabi 117
},
	file = {Full Text PDF:files/731/Ciampaglia et al. - 2018 - How algorithmic popularity bias hinders or promote.pdf:application/pdf},
}

@article{olteanu_social_2019,
	title = {Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries},
	volume = {2},
	issn = {2624-909X},
	url = {https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2019.00013/full},
	doi = {10.3389/fdata.2019.00013},
	shorttitle = {Social Data},
	abstract = {{\textless}p{\textgreater}Social data in digital form—including user-generated content, expressed or implicit relations between people, and behavioral traces—are at the core of popular applications and platforms, driving the research agenda of many researchers. The promises of social data are many, including understanding “what the world thinks” about a social issue, brand, celebrity, or other entity, as well as enabling better decision-making in a variety of fields including public policy, healthcare, and economics. Many academics and practitioners have warned against the naïve usage of social data. There are biases and inaccuracies occurring at the source of the data, but also introduced during processing. There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked. This paper recognizes the rigor with which these issues are addressed by different researchers varies across a wide range. We identify a variety of menaces in the practices around social data use, and organize them in a framework that helps to identify them.{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}disp-quote{\textgreater}{\textless}p{\textgreater}“{\textless}italic{\textgreater}For your own sanity, you have to remember that not all problems can be solved. Not all problems can be solved, but all problems can be illuminated.” –Ursula Franklin{\textless}/italic{\textgreater}{\textless}xref ref-type="fn" rid="fn0001"{\textgreater}$^{\textrm{1}}${\textless}/xref{\textgreater}{\textless}/p{\textgreater}{\textless}/disp-quote{\textgreater}{\textless}/p{\textgreater}},
	journaltitle = {Front. Big Data},
	author = {Olteanu, Alexandra and Castillo, Carlos and Diaz, Fernando and Kıcıman, Emre},
	urldate = {2025-03-16},
	date = {2019-07-11},
	note = {Publisher: Frontiers},
	keywords = {biases, Ethics, Evaluation, Social Media, User data},
	annotation = {Mehrabi 120
},
	file = {Full Text PDF:files/734/Olteanu et al. - 2019 - Social Data Biases, Methodological Pitfalls, and .pdf:application/pdf},
}

@article{riegg_causal_2008,
	title = {Causal Inference and Omitted Variable Bias in Financial Aid Research: Assessing Solutions},
	volume = {31},
	issn = {1090-7009},
	url = {https://muse.jhu.edu/pub/1/article/232773},
	shorttitle = {Causal Inference and Omitted Variable Bias in Financial Aid Research},
	abstract = {, This article highlights the problem of omitted variable bias in research on the causal effect of financial aid on college‑going. I first describe the problem of self‑selection and the resulting bias from omitted variables. I then assess and explore the strengths and weaknesses of random assignment, multivariate regression, proxy variables, fixed effects, difference‑in‑differences, regression discontinuity, and instrumental variables techniques in addressing the problem. I focus on the intuition, assumptions, and applications of each method in the context of the same research question, providing practical guidance for researchers interested in implementing these approaches.},
	pages = {329--354},
	number = {3},
	journaltitle = {The Review of Higher Education},
	author = {Riegg, Stephanie K.},
	urldate = {2025-03-16},
	date = {2008},
	note = {Publisher: Johns Hopkins University Press},
	annotation = {Mehrabi 131
},
}

@inproceedings{suresh_framework_2021,
	location = {New York, {NY}, {USA}},
	title = {A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle},
	isbn = {978-1-4503-8553-4},
	url = {https://dl.acm.org/doi/10.1145/3465416.3483305},
	doi = {10.1145/3465416.3483305},
	series = {{EAAMO} '21},
	abstract = {As machine learning ({ML}) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the {ML} life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.},
	pages = {1--9},
	booktitle = {Proceedings of the 1st {ACM} Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
	publisher = {Association for Computing Machinery},
	author = {Suresh, Harini and Guttag, John},
	urldate = {2025-03-16},
	date = {2021-11-04},
	annotation = {Mehrabi 144
},
	file = {Full Text PDF:files/739/Suresh und Guttag - 2021 - A Framework for Understanding Sources of Harm thro.pdf:application/pdf},
}

@article{wang_why_2014,
	title = {Why Amazon's Ratings Might Mislead You: The Story of Herding Effects},
	volume = {2},
	issn = {2167-6461},
	url = {https://www.liebertpub.com/doi/full/10.1089/big.2014.0063},
	doi = {10.1089/big.2014.0063},
	shorttitle = {Why Amazon's Ratings Might Mislead You},
	abstract = {Our society is increasingly relying on digitalized, aggregated opinions of individuals to make decisions (e.g., product recommendation based on collective ratings). One key requirement of harnessing this “wisdom of crowd” is the independency of individuals' opinions; yet, in real settings, collective opinions are rarely simple aggregations of independent minds. Recent experimental studies document that disclosing prior collective ratings distorts individuals' decision making as well as their perceptions of quality and value, highlighting a fundamental discrepancy between our perceived values from collective ratings and products' intrinsic values. Here we present a mechanistic framework to describe herding effects of prior collective ratings on subsequent individual decision making. Using large-scale longitudinal customer rating datasets, we find that our method successfully captures the dynamics of ratings growth, helping us separate social influence bias from inherent values. Leveraging the proposed framework, we quantitatively characterize the herding effects existing in product rating systems and promote strategies to untangle manipulations and social biases.},
	pages = {196--204},
	number = {4},
	journaltitle = {Big Data},
	author = {Wang, Ting and Wang, Dashun},
	urldate = {2025-03-16},
	date = {2014-12},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	annotation = {Mehrabi 151
},
	file = {Full Text PDF:files/742/Wang und Wang - 2014 - Why Amazon's Ratings Might Mislead You The Story .pdf:application/pdf},
}
