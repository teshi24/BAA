<ul>
<li><p>Prof. Dr. Name Surname from Lucerne University of Applied
Sciences and Arts, Switzerland (President of the Jury);</p></li>
<li><p>Prof. Dr. Name Surname from Lucerne University of Applied
Sciences and Arts, Switzerland (Thesis Supervisor);</p></li>
<li><p>Prof. Dr. Name Surname from Lucerne University of Applied
Sciences and Arts, Switzerland (External Expert).</p></li>
</ul>
<p>Lucerne, October 27th, 2024</p>
<table>
<tbody>
<tr>
<td style="text-align: left;"><strong>Dr. Ludovic
Amruthalingam</strong><br />
@supervisor</td>
<td style="text-align: left;"><strong>Prof. Dr. René
Hüsler</strong><br />
</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<div class="center">
<p><strong>@hmain</strong></p>
</div>
<p><span>@hi</span></p>
<table>
<tbody>
<tr>
<td style="text-align: left;">@title:</td>
<td style="text-align: left;">title</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">@student:</td>
<td style="text-align: left;">author</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">@study@topic:</td>
<td style="text-align: left;">Bachelor in Computer Science</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">@def@year:</td>
<td style="text-align: left;">date</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">@supervis:</td>
<td style="text-align: left;">Dr. Ludovic Amruthalingam</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">@expert:</td>
<td style="text-align: left;">Dr. Jürg Schelldorfer</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">@ind@partner:</td>
<td style="text-align: left;">Applied AI Research Lab</td>
</tr>
</tbody>
</table>
<p><span>@hii</span></p>
<div class="todolist">
<p>@public</p>
<p>@confid</p>
</div>
<p><span>@hiii </span></p>
<p>@self</p>
<p>@sign <u></u></p>
<p><span>@hiv</span></p>
<p>@vis</p>
<p>@save</p>
<p>@sign <u></u></p>
<p><span>@thanks</span></p>
<p>Thanks to my family, relatives and friends for all the support given
to finish this thesis. Ludovic Amruthalingam Simone Lionetti - deputy
Ludovic Pascal Baumann - LaTeX Philippe Gottfrois - information and work
on PASSION project</p>
<p>author, date</p>
<p><em></em></p>
<p>cceptedforaempty ssistaempty uthoraempty atedempty maileempty
eminarnamesempty upervissempty hankstempty itletempty</p>
<div class="center">
<p><strong></strong></p>
</div>
<p>The content of your thesis in brief.</p>
<p><span style="color: blue">Alle Fakten (fundiertes Wissen Dritter)
sind korrekt zitiert. Es werden verschiedene Zitierweisen verwendet und
teilweise mehrere Interpretationen gegenübergestellt. Der gemeinsam
definierte Zitierstil im Text, in Abbildungen und Tabellen sowie im
Literaturverzeichnis wird korrekt und durchgängig angewendet. Eigene
Leistungen (sowie Bewertungen) und Fremdquellen sowie Recherchen sind
klar unterscheidbar.</span></p>
<p><span style="color: blue">Die erstellten Artefakte sind von sehr
hoher Qualität. Das trifft u.a. auf Diagramme, Skizzen sowie Notationen
(z.B. BPMN/UML) zu. Darstellungen sind einwandfrei, alle statistisch
notwendigen Qualitätskriterien sind erfüllt. Beschriftungen etc. sind
vorhanden, keine Einwände, Text und Bild stimmen beschreibend gut
überein. Es wurden angemessene Dokumentationsmethoden und -arten korrekt
verwendet. Vereinbarte Interview Transkripte, Beobachtungsprotokolle
bzw. Zusammen-fassungen sind vorhanden. Daten, Ort, Kontext,
Beschreibung, Zeilennummer, Verweise, Strukturen sind erkennbar, gut
formatiert und korrekt mit dem Text/ der Analyse verknüpft. Alle
Elemente und Themen sind im methodischen Teil/Text erklärt und
verständlich, keine technischen oder strukturellen Einwände. Auch
Zwischenanalysen, Zwischenschritte oder Gesamtauswertungen wurden
durchgeführt, die Herkunft der Daten ist erkennbar und professionell
aufbereitet.</span></p>
<p><span style="color: blue">Der Schreibstil aller Dokumente entspricht
hohen Standards und enthält keine Übertreibungen oder unbegründete
Beurteilungen. Die Sprache ist aussagekräftig, prägnant und präzise. Die
Fachterminologie ist konsistent, d.h. für gleiche Gegenstände und Themen
werden immer die gleichen Begriffe verwendet. Der Sprachgebrauch ist
durchgängig geschlechtergerecht, einheitlich und sachlich.</span></p>
<h1 data-number="1" id="problem-statement"><span
class="header-section-number">1</span> Problem Statement</h1>
<p><span style="color: blue">Welche Ziele, Fragestellungen werden mit
dem Projekt verfolgt? Die Bedeutung, Auswirkung und Relevanz dieses
Projektes für die unterschiedlichen Beteiligten soll aufgeführt werden.
Typischerweise wird hier ein Verweis auf die Aufgabenstellung im Anhang
gemacht.</span></p>
<p>In Sub-Saharan Africa, dermatology treatment is inaccessible. There
are fewer than one dermatologist available per one million people. On
the other hand, up to 80% of the children and adolescents in the area
are affected by skin conditions. AI-based <span
data-acronym-label="teledermatology"
data-acronym-form="singular+short">teledermatology</span> promises to
close this gap of specialists per case, for example by serving as a
triage option. Potential patients could upload pictures to diagnostic
dermatology AIs which can indicate whether the person should indeed
visit a dermatologist or promote other treatment options. However, the
current dermatology AIs have a tendency to fail to deliver accurate
results for patients with highly pigmented skin tones. This is mainly
due to demographic biases in existing AI models. The models are trained
on established datasets which mainly feature low-pigmented skin.
Therefore, the datasets lack representation of highly-pigmented skin,
leading to AI models which do not generalize to the population in
Sub-Saharan Africa <span class="citation"
data-cites="Gottfrois2024"></span>.</p>
<p>These biases result in unequal access to treatment and especially
affect underrepresented groups. Such biased results must be avoided,
especially in AI models which impact life-changing decisions <span
class="citation" data-cites="Mehrabi_2021"></span>.</p>
<p>Demographic biases are especially important in dermatology.
Demographic differences in patients influence the appearance of
dermatological conditions. The differences in appearance can be
developed depending on genetic factors, such as skin tone, age, sex
<span class="citation" data-cites="Diaz2022"></span>. Research showed,
that in patients with lower socio-economic status the disease
progression is more advanced at time of diagnosis, which in turn can
lead to different appearances for the same disease <span
class="citation" data-cites="BAD2021"></span>. Since the AI models use
pictures as the inputs and can only learn to diagnose diseases according
to their appearances in the data, the factors which affects the disease
appearances must be considered when creating an inclusive dataset.</p>
<p>In order to overcome these issues, the PASSION research team founded
the PASSION project. The projects vision is to make dermatology
treatment accessible in Africa by enabling the AI-supported <span
data-acronym-label="teledermatology"
data-acronym-form="singular+short">teledermatology</span> for triage by
reducing the demographic biases in the dermatology AI models. For this
bias mitigation, the researcher collected a dataset in Sub-Saharan
Africa, focusing on patients with highly pigmented skin and the most
common regional <span data-acronym-label="pediatric"
data-acronym-form="singular+short">pediatric</span> skin conditions. The
PASSION dataset is complementary to existing datasets and improves their
diversity. Further, the PASSION team trained an ResNet-50 model with the
dataset which is refered to as PASSION model in this thesis. This model
should serve as a benchmark model to assess other dermatology models in
regards of fairness <span class="citation"
data-cites="Gottfrois2024"></span>.</p>
<p>So that the PASSION model can become an unbiased benchmark model,
potential demographic biases in it must be reduced as far as possible.
To reach this goal, demographic biases in the model as well as the
limitation of the gathered dataset must be identified and mitigated.
This thesis supports the PASSION team in this process. The main
objective of the thesis is to assess the effectiveness of mitigation
strategies to reduce demographic biases in context of PASSION.</p>
<p>First, I need to gain an overview over what biases, fairness metrics
and mitigation strategies are known in general. Then, I must scope the
found information, to find what is relevant for PASSION and what is
feasible to achieve within the timeconstraints of this thesis. Before
starting an assessment, a baseline needs to established by computing
chosen fairness metrics. Afterwards, the mitigation methods can be
applied and the performance can be compared to the baseline, to find out
whether the methods are indeed mitigating the biases.</p>
<ul>
<li><p>With the advent of telemedicine, developing countries are
learning newer ways of leveraging their information and communication
technologies (ICTs) to play an increasingly vital role in the health
care industry. Telemedicine is defined as a health care delivery
mechanism where physicians and other medical personnel can examine
patients remotely using information and telecommunication technologies
(ICTs; Bashshur, Sanders, and Shannon, 1997). <span class="citation"
data-cites="Kifle_2024"></span></p></li>
<li><p>AI systems can be used in many sensitive environments to make
important and life-changing decisions; thus, it is crucial to ensure
that these decisions do not reflect discriminatory behavior toward
certain groups or populations <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p>There are clear benefits to algorithmic decision-making; unlike
people, machines do not become tired or bored [45, 119], and can take
into account orders of magnitude more factors than people can. However,
like people, algorithms are vulnerable to biases that render their
decisions “unfair” [6, 121]. In the context of decision-making, fairness
is <em>the absence of any prejudice or favoritism toward an individual
or group based on their inherent or acquired characteristics</em>. Thus,
an unfair algorithm is one whose decisions are skewed toward a
particular group of people. <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p>it is important for researchers and engineers to be concerned
about the downstream applications and their potential harmful effects
when modeling an algorithm or a system <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p>We should think responsibly, and recognize that the application
of these tools, and their subsequent decisions affect peoples’ lives;
therefore, considering fairness constraints is a crucial task while
designing and engineering these types of sensitive tools <span
class="citation" data-cites="Mehrabi_2021"></span>.</p></li>
</ul>
<h1 data-number="2" id="state-of-research"><span
class="header-section-number">2</span> State of Research</h1>
<p><span style="color: blue">Bezogen auf die eigenen Zielsetzungen und
Fragestellungen soll aufgezeigt werden, wie andere dieses oder ähnliche
Probleme gelöst haben. Worauf können Sie aufbauen, was müssen Sie neu
angehen? Wodurch unterscheidet sich Ihre Lösung von anderen Lösungen?
Für wissenschaftlich orientierte Arbeiten sei hier explizit auf
(Balzert, S. 66 ff) verwiesen.</span> <span
style="color: blue">Relevante, aktuelle und fundierte Fachliteratur
wurde identifiziert, kritisch geprüft und verwendet. Die Begriffe der
Fragestellung sind definiert und referenziert. Der gesamte Kontext ist
verknüpft und eine Abgrenzung wurde vorgenommen. All dies ist in einer
leicht verständlichen Struktur formuliert und überprüft.</span></p>
<p>Bias mitigation in AI has already been investigated by different
researchers, who crafted fiting mitigation methods . This thesis aims to
assess those existing methods in the context of PASSION. Therefore, this
chapter focuses first on getting an overview over the PASSION project
based on the PASSION paper and dataset. Then, the general knowledge
about existing biases, fairness metrics and mitigation methods is
gathered via a literature review. The review process was split between
the general ML context and specific dermatology ML context, so that the
technical and dermatological perspective can be taken into account, when
the knowledge is applied to PASSION. The tables in this chapter will
indicate, which points where found in which context. This is important,
because what may be an issue in general might not be relevant for a
specific usecase or vise versa. For example, in theory, all ages should
be represented in datasets to cover demographic insights. However, for
car insurance, the age representation is not imporant, because the age
does not affect how well a driver can drive .</p>
<p>During the literature review, lots of biases and mitigation methods
where found, which might have a relation to the PASSION project. Since
it is not feasible to assess all of them during the duration of this
thesis, the thesis focus on those which are related to skin type, age
and gender. The other items are passed as a list to the PASSION research
team for further investigation. The list can be found in the appendix
.</p>
<h2 data-number="2.1" id="passion-for-dermatology"><span
class="header-section-number">2.1</span> PASSION for Dermatology</h2>
<p>With their paper, the PASSION research team provides the PASSION
dataset including data analysis scripts as well as the PASSION model.
For this thesis, it is important to understand which metadata the
dataset provides, what architecture the model has and which mitigation
methods were already applied. The available data can influence which
biases could arise in the model or rather which ones can be measured.
Further, the labels which should be predicted and the model architecture
give insight into the ML task. All this information affect which
mitigation methods are feasible to be used for the project.</p>
<h3 data-number="2.1.1" id="passion-dataset"><span
class="header-section-number">2.1.1</span> PASSION Dataset</h3>
<p>The PASSION dataset contains data from patients from four African
countries in dermatology clinics. It contains 4901 images of 1653
dermatology cases with the corresponding demographic and clinical
information, see <a href="#tab:PASSION_metadata"
data-reference-type="ref+label"
data-reference="tab:PASSION_metadata">2.1</a>. There is one record per
patient and one or more corresponding images which are linked to the
record through the filename. The images are taken with mobile phones, in
order to train the models on a <span
data-acronym-label="teledermatology"
data-acronym-form="singular+short">teledermatology</span>-like setup
regarding image quality <span class="citation"
data-cites="Gottfrois2024"></span>.</p>
<p>The datasplit is a predefined 80/20 stratified development-test split
at patient level. This ensures reproducibility, fair comparison, while
preventing information leaking <span class="citation"
data-cites="Gottfrois2024"></span>. Stratified splitting is a method to
split imbalanced datasets without changing the original class
distribution within the subset. This is important to maintaining the
representativeness of the data, especially for minority classes <span
class="citation" data-cites="Balde_2023"></span>. .</p>
<p>Access to the dataset can be requested via <a
href="https://passionderm.github.io/">https://passionderm.github.io/</a>
<span class="citation" data-cites="Gottfrois2024"></span>.</p>
<div id="tab:PASSION_metadata">
<table>
<caption>PASSION dataset - metadata attributes and descriptions <span
class="citation" data-cites="Gottfrois2024"></span></caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Metadata Attribute</strong></th>
<th style="text-align: left;"><strong>Data Type</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">subject_id</td>
<td style="text-align: left;">string</td>
<td style="text-align: left;">Participant’s unique identifier</td>
</tr>
<tr>
<td style="text-align: left;">country</td>
<td style="text-align: left;">string</td>
<td style="text-align: left;">Country of data origin</td>
</tr>
<tr>
<td style="text-align: left;">age</td>
<td style="text-align: left;">integer</td>
<td style="text-align: left;">Age of the participant in years</td>
</tr>
<tr>
<td style="text-align: left;">sex</td>
<td style="text-align: left;">m/f/o</td>
<td style="text-align: left;">Gender of the participant</td>
</tr>
<tr>
<td style="text-align: left;">fitzpatrick</td>
<td style="text-align: left;">integer</td>
<td style="text-align: left;"><span data-acronym-label="FST"
data-acronym-form="singular+short">FST</span></td>
</tr>
<tr>
<td style="text-align: left;">body_loc</td>
<td style="text-align: left;">string (list; null-able,
semicolon-separated)</td>
<td style="text-align: left;">Specific affected body locations</td>
</tr>
<tr>
<td style="text-align: left;">impetig</td>
<td style="text-align: left;">0/1</td>
<td style="text-align: left;">Presence of impetigo (1=present), may
occur alone or with other conditions, affects the treatment options for
coexisting conditions</td>
</tr>
<tr>
<td style="text-align: left;">conditions_PASSION</td>
<td style="text-align: left;">Eczema, Scabies, Fungal, Others</td>
<td style="text-align: left;">Primary diagnosed skin condition</td>
</tr>
</tbody>
</table>
</div>
<p>For this thesis, the attributes <em>fitzpatrick</em>, <em>sex</em>
and <em>age</em> are relevant, as they cover relevant demographic
information from the patients. The skin type is directly relevant
because it can affect disease presentation and therefore the model’s
learning process to detect the condition . According to Philippe
Gottfrois, the main author of the PASSION paper, the same condition
looks similar regardless of a patient’s age and sex. However, the
prevalence of conditions varies based on these factors . Therefore, the
data distribution of age and sex is relevant for an inclusive AI model,
because these demographic factors influence the likelihood of a
condition being represented in the dataset.</p>
<p>The attribute <em>country</em> seems to be another demographic
factor. Since it is the country where the data collection took place ,
the only demographic information it could indicate is the geographic
location where the patient was at diagnosis. This could potentially be
used as <span data-acronym-label="proxyVar"
data-acronym-form="singular+short">proxyVar</span> for where the patient
lives, which could be relevant for disease prevalance . However, this
could lead to false conclusions which is why this label will not be used
in this thesis. The PASSION team should investigate the need for this
label and possible enhancement. Also, they should state the meaning of
the label clearer in the dataset description.</p>
<p>The labels <em>impetig</em> and <em>conditions_PASSION</em> are the
ones which the PASSION model must predict. They can appear independently
of each other. Therefore, the ML task for PASSION it is a
multiclassification task.</p>
<h3 data-number="2.1.2" id="passion-data-analysis-scripts"><span
class="header-section-number">2.1.2</span> PASSION Data Analysis
Scripts</h3>
<p>With the dataset, the PASSION research team provides a <span
data-acronym-label="JupyterNotebook"
data-acronym-form="singular+short">JupyterNotebook</span> with code
examples and data analysis scripts. The scripts are listed in <a
href="#tab:PASSION_scripts" data-reference-type="ref+label"
data-reference="tab:PASSION_scripts">2.2</a> with an indicator, how
relevant the scripts are for this thesis. Relevant are the scripts
related to demographic distributions of the chosen labels. When the
model output is biased, these scripts can help determine whether the
bias is potentially caused by imbalance in the data distribution.
Somewhat relevant are the scripts which lay foundations for further data
analysis scripts. Not relevant for this thesis are all other
scripts.</p>
<div class="threeparttable">
<div id="tab:PASSION_scripts">
<table>
<caption>PASSION dataset - existing analysis scripts <span
class="citation" data-cites="Gottfrois2024"></span> </caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Script Title</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Relevance -
Reasoning</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Linking CSV Data with Image Files</td>
<td style="text-align: left;">Mapping between data records and
images.</td>
<td style="text-align: left;"><strong>Medium</strong> - Basis for other
analyses</td>
</tr>
<tr>
<td style="text-align: left;">Extracting and Comparing Subject IDs</td>
<td style="text-align: left;">Dataset verification regarding
completeness</td>
<td style="text-align: left;"><strong>Low</strong> - No insight in
regards of demographic distribution</td>
</tr>
<tr>
<td style="text-align: left;">Regrouping Malawi and Tanzania to EAS</td>
<td style="text-align: left;">Data aggregation due to dataset size and
geographical proximity</td>
<td style="text-align: left;"><strong>Medium</strong> - Might impact
interpretation of the results of the following scripts</td>
</tr>
<tr>
<td style="text-align: left;">Conditions by Country</td>
<td style="text-align: left;">Correlation between clinical conditions
and country</td>
<td style="text-align: left;"><strong>Low</strong> - The label
<em>country</em> is out of scope of this thesis</td>
</tr>
<tr>
<td style="text-align: left;">Body Localizations by Conditions</td>
<td style="text-align: left;">Correlation between the condition and
primarily affected body parts</td>
<td style="text-align: left;"><strong>Low</strong> - No insight in
regards of demographic distribution</td>
</tr>
<tr>
<td style="text-align: left;">Impetigo Cases</td>
<td style="text-align: left;">Total count of impetigo cases and
proportion to all cases</td>
<td style="text-align: left;"><strong>Low</strong> - No insight in
regards of demographic distribution</td>
</tr>
<tr>
<td style="text-align: left;">Distribution of <span
data-acronym-label="FST"
data-acronym-form="plural+short">FSTs</span></td>
<td style="text-align: left;">Counts and visualizes the skin type
distribution</td>
<td style="text-align: left;"><strong>High</strong> - Insight into
demographic distributions</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p>Research is divided which demographic factors play into the
prevalence of impetigo <span class="citation"
data-cites="Romani_2017 Aleid_2024"></span>.</p>
</div>
</div>
<h3 data-number="2.1.3" id="passion-model"><span
class="header-section-number">2.1.3</span> PASSION Model</h3>
<p>The model architecture is a ResNet-50 model which is pretrained on
ImageNet. The model was finetuned by replacing the last fully connected
classification layer by a dropout layer with a 0.3 dropout rate followed
by batch normalization. The class activation is done by a single linear
layer. To minimize the weighted cross-entropy loss, Adam optimization is
used. For improved generalization and to avoid overfitting, data
augmentations were applied. The used methods were random resizing,
cropping, flipping and rotating <span class="citation"
data-cites="Gottfrois2024"></span>. .</p>
<h3 data-number="2.1.4" id="passion-experiments"><span
class="header-section-number">2.1.4</span> PASSION Experiments</h3>
<p>The PASSION team conducted various experiments to evaluate the
classifiers on the test set with the following schemes <span
class="citation" data-cites="Gottfrois2024"></span>:</p>
<ul>
<li><p>Performance for skin condition prediction</p></li>
<li><p>Performance for impetigo detection</p></li>
<li><p>Generalization from two centers to a wider population (test set
contains data from the known centers and one unknown center)</p></li>
<li><p>Generalization from different age groups (test set contains data
from the known age groups and one unknown)</p></li>
<li><p>Subject level analysis over the predictions of multiple pictures,
using majority voting</p></li>
</ul>
<p>These experiments can serve as a starting point, since reproducing
the results help to setup works the same on my side. Also, they can be
used as examples for further experiments.</p>
<p>The paper indicates a lower performance when evaluating on a subject
level versus on a sample level. The authors highlight the importance to
evaluate a classifier’s performance on both levels for completeness
<span class="citation" data-cites="Gottfrois2024"></span>. Therefore,
the subject level performance should also be considered during this
thesis.</p>
<h2 data-number="2.2" id="bias"><span
class="header-section-number">2.2</span> Bias</h2>
<p>The algorithmic decisions of AI affect peoples’ lives. Due to that,
healthcare AI applications, like PASSION, are sensitive tools. However,
decisions from AI applications proved to be biased, which can harm
underrepresented groups. Therefore, AI engineers should aim to address
and mitigate those biases. To do so, it is crucial to know what bias is,
what types of biases exist and where they are coming from <span
class="citation" data-cites="Mehrabi_2021"></span>. This chapter
provides therefore an overview over biases and related features which
were mentioned in ML- and dermatology-related research.</p>
<ul>
<li><p>Bias in facial recognition systems [128] and recommender systems
[140] have also been largely studied and evaluated and in many cases
shown to be discriminative towards certain populations and subgroups. In
order to be able to address the bias issue in these applications, it is
important for us to know where these biases are coming from and what we
can do to prevent them.<span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p>We should think responsibly, and recognize that the application
of these tools, and their subsequent decisions affect peoples’ lives;
therefore, considering fairness constraints is a crucial task while
designing and engineering these types of sensitive tools <span
class="citation" data-cites="Mehrabi_2021"></span>.</p></li>
</ul>
<h3 data-number="2.2.1" id="introduction-to-bias-in-ai"><span
class="header-section-number">2.2.1</span> Introduction to Bias in
AI</h3>
<p>In ML, bias can be defined as <em>a systematic error that causes a
model or estimator to consistently deviate from the true value or
relationship</em> <span class="citation"
data-cites="Delgado-Rodriguez_2004 Taylor_2023"></span>.</p>
<p>According to the Cambridge English dictionary, bias can be defined as
"the fact of preferring a particular subject or thing" or even as "the
action of supporting or opposing a particular person or thing in an
unfair way, because of allowing personal opinions to influence your
judgment" <span class="citation" data-cites="Cambridge_2025"></span>.
While an AI does not hold personal opinions, the judgment of an AI
algorithm is influenced by the underlying data and how the algorithm
uses this data. The data and even the algorithm itself can hold biases,
which affects the final outcome and potential lead to unfair decisions.
In decision-making, fairness means "absence of any prejudice or
favoritism toward an individual or group based on their inherent or
acquired characteristics" - or in other terms, a fair algorithms
decisions are not scewed toward a group of people <span class="citation"
data-cites="Mehrabi_2021"></span>.</p>
<ul>
<li><p>The Cambridge English dictionary defines bias as “the action of
supporting or opposing a particular person or thing in an unfair way as
a result of allowing personal opinions to influence your judgement.”1
However, statistical bias is defined as any systematic error in the
determination of the association between exposure and disease.2 <span
class="citation" data-cites="Chakraborty_2024"></span></p></li>
<li><p>These biased predictions stem from the hidden or neglected biases
in data or algorithms <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p>There are clear benefits to algorithmic decision-making; unlike
people, machines do not become tired or bored [45, 119], and can take
into account orders of magnitude more factors than people can. However,
like people, algorithms are vulnerable to biases that render their
decisions “unfair” [6, 121]. In the context of decision-making, fairness
is <em>the absence of any prejudice or favoritism toward an individual
or group based on their inherent or acquired characteristics</em>. Thus,
an unfair algorithm is one whose decisions are skewed toward a
particular group of people. <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
</ul>
<h3 data-number="2.2.2" id="bias-sources"><span
class="header-section-number">2.2.2</span> Bias Sources</h3>
<p>The general ML lifecycle consists of data gathering, training the
algorithm and the user interaction with the trained model. Now, while
data gathering, biases can arise either through the collection process
or it is already inherited in the available data. Further, depending on
the algorithm design, during training, the existing bias in the data can
be amplified and new bias can be introduced. Lastly, the result of the
algorithm can affect the user experience on inference which can lead to
further bias amplification. This generates a feedback loop between the
biases in each step of the ML lifecycle which can make it hard to
identify the original bias source. The feedback loop is illustrated in
<a href="#fig:bias_definitions_ML_lifecycle"
data-reference-type="ref+label"
data-reference="fig:bias_definitions_ML_lifecycle">2.1</a>, which also
shows first bias definitons, which were categorized according to this
feedback loop <span class="citation"
data-cites="Mehrabi_2021"></span>.</p>
<figure id="fig:bias_definitions_ML_lifecycle">
<img src="figures/BiasCategoriesInMLLifecycle.png"
style="width:80.0%" />
<figcaption>Bias definitions in a ML lifecycle <span class="citation"
data-cites="Mehrabi_2021"></span>.</figcaption>
</figure>
<ul>
<li><p>two potential sources of unfairness in machine learning outcomes
- those that arise from biases in the data and those that arise from the
algorithms ... we observe that biased algorithmic outcomes might impact
user experience, thus generating a feedback loop between data,
algorithms and users that can perpetuate and even amplify existing
sources of bias <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p>The loop capturing this feedback between biases in data,
algorithms, and user interaction is illustrated in Figure 1. We use this
loop to categorize definitions of bias in the section below <span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
</ul>
<h3 data-number="2.2.3" id="bias-types"><span
class="header-section-number">2.2.3</span> Bias Types</h3>
<p>The <a href="#tab:biases_types" data-reference-type="ref+label"
data-reference="tab:biases_types">2.3</a> aims to provide an overview
over what kind of biases exist according to research. The more detailed
categories listed in the table try to capture similar kind of biases.
This thesis follows roughly the categorization of <span class="citation"
data-cites="Mehrabi_2021"></span>. Some biases might acctually fit in
multiple categories. The definition of the categories including examples
of specific biases follows.</p>
<div class="threeparttable">
<div id="tab:biases_types">
<table>
<caption>Bias categories - grouped according the ML lifecycle of <span
class="citation" data-cites="Mehrabi_2021"></span></caption>
<tbody>
<tr>
<td style="text-align: left;"><strong>Bias</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>ML</strong></td>
<td style="text-align: left;"><strong>Dermatology</strong></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Sampling Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Representation Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Measurement Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Research Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Feature Representation Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Imaging Biases</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Medical Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Temporal Data Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">User-Algorithm Interaction Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">External Influence Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Cognitive Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Behavioral Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Publication Biases</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Medical Biases</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<div class="minipage">
<p><span class="citation" data-cites="Mehrabi_2021"></span></p>
<p><span class="citation" data-cites="HP_2022"></span></p>
<p><span class="citation" data-cites="Mester_2022"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="Chakraborty_2024"></span></p>
<p><span class="citation" data-cites="Young_2020"></span></p>
<p><span class="citation" data-cites="Montoya_2025"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="Mester_2017"></span></p>
<p><span class="citation"
data-cites="Delgado-Rodriguez_2004"></span></p>
</div>
</div>
</div>
<h4 data-number="2.2.3.1" id="data-biases"><span
class="header-section-number">2.2.3.1</span> Data Biases</h4>
<h4 class="unnumbered" id="sampling-biases">Sampling Biases</h4>
<p>When gathering data, it’s usually not possible to gather the data of
a whole population. Instead, the data is gathered by sampling. A sample
is a subgroup of individuals from the population. To get unbiased
results, this sampling process should represent the true population,
with a low sampling error <span class="citation"
data-cites="HP_2022"></span>. This is often achieved with randomized
samples. With non-random sampling processes, sampling bias arises. The
consequence is, that the insights of one sampled population may not
generalize with insights on another sampled popluation <span
class="citation" data-cites="Mehrabi_2021"></span>.</p>
<p>Those biases can be introduced with a flawed sampling process:</p>
<ul>
<li><p><strong>Sampling bias</strong>, due to nonrandom sampling of
subgroups, leading to poor generalization <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p><strong>Selection bias</strong>, working only on specific subset
of the population which is not representative <span class="citation"
data-cites="Mester_2022 Chakraborty_2024"></span></p></li>
<li><p><strong>Systematic selection bias</strong>, chosen samples differ
dramatically from the representative populations; e.g. in dermatology,
when only the most severe patient data gets included <span
class="citation"
data-cites="Chakraborty_2024 c5 c6 c33"></span></p></li>
<li><p><strong>Ascertainment bias</strong>, tendency to exclude segments
from the population due to e.g. cultural differences, such as which
patient segment goes to government clinics vs. private clinics (usually
influenced by socioeconomic status) <span class="citation"
data-cites="Chakraborty_2024 c5"></span></p></li>
<li><p><strong>Availability bias</strong>, focus on widely available
data instead of most representative data <span class="citation"
data-cites="Chakraborty_2024 c9 c10"></span></p></li>
<li><p><strong>Survivorship bias</strong>, focus only on pre-selected
data, ignoring the initial data-points which got filtered out <span
class="citation" data-cites="Mester_2022"></span>.</p></li>
</ul>
<h6 data-number="2.2.3.1.0.1" id="potential-biases-in-passion"><span
class="header-section-number">2.2.3.1.0.1</span> Potential Biases in
PASSION</h6>
<p>PASSION tries to reduce sampling bias in dermatology against high
pigmented skin. PASSION might introduce (systematic) selection bias or
Ascertainment bias, if in the dermatology centers only sickest / more
severe patients are seen as indicated by <span class="citation"
data-cites="Chakraborty_2024"></span> PASSION inherits availability bias
as it is using <span data-acronym-label="FST"
data-acronym-form="singular+short">FST</span> scale. Survivorship bias
could be relevant for PASSION, if dermatology diseases could be lethal.
Further, all patients which are not able to go to one of the dermatology
centers which were used in PASSION could be considered to left out by
survivorship bias.</p>
<p>used</p>
<ul>
<li><p>Sampling Bias. Sampling bias is similar to representation bias,
and it arises due to nonrandom sampling of subgroups. As a consequence
of sampling bias, the trends estimated for one population may not
generalize to data collected from a new population. <span
class="citation" data-cites="Mehrabi_2021"></span>. This is what the
PASSION dataset tries to improve</p></li>
<li><p>Selection bias - wrong sampling method, working on a specific
subset of audience; usually by working only with data that is easy to
access <span class="citation"
data-cites="Mester_2022 Mester_2017"></span> - statistical bias</p></li>
<li><p>Selection bias: Since it is not possible to work with large
populations, for most dermatological studies, samples are chosen that
are said to be representative of the original population. In selection
bias, the selected subgroups are not representative of their original
population. A variation of this is systematic selection bias, where
samples chosen differ dramatically from their representative
populations. Our experience suggests, such selection bias occurs more
commonly in studies conducted in regional referral centers where only
the sickest or more severe patients are usually seen. For example, a
study compared the efficacy of thalidomide vs. prednisolone in
hospitalised patients of erythema nodosum leprosum. It derived that
thalidomide was more efficacious than steroids in erythema nodosum
leprosum. Such findings cannot be generalised to all erythema nodosum
leprosum since patients admitted to a regional referral center will
likely have more severe disease.5,6,33 <span class="citation"
data-cites="Chakraborty_2024"></span></p></li>
<li><p>Availability bias: More emphasis is placed on widely available
data than scantily available data. A classic example is the use of
antihistamines in pregnancy dermatoses, where nearly all standard books
recommend first-generation antihistamine chlorpheniramine because more
data is available.9 10. <span class="citation"
data-cites="Chakraborty_2024"></span> - dermatology</p></li>
<li><p>Survivorship bias <span class="citation"
data-cites="Mester_2022 Mester_2017"></span> - statistical bias</p></li>
<li><p>Ascertainment Bias: This bias is commonly encountered in
venereology practice. It is defined as a bias due to the tendency of
some segments of the target population to get excluded due to cultural
and other differences. For example, in most venereology clinics in
government setups, studies show that venereal diseases are commoner in
lower socioeconomic status. One reason might be that the higher
socioeconomic status people tend to go to private practitioners and
thereby get excluded from government-run clinics.9,10 Allocation
concealment and blinding are good ways to avoid this. 5. <span
class="citation" data-cites="Chakraborty_2024"></span> -
healthcare</p></li>
</ul>
<p>even more extensive</p>
<ul>
<li><p>Selection bias is again divided into two types endogenous
selection bias and exogenous selection bias. The best example of
endogenous selection bias in dermatology is the inclusion of
non-response. If a trial tests the efficacy of a particular biologic in
psoriasis, the response is usually collected from trial participants via
postal services. Certain participants will not respond, although they
might have substantially improved. Their exclusion will result in
significant differences in efficacy evaluation.33 Exogenous selection
bias results when both treatment and outcome result from dependency on
an external variable that is not controlled. For example, if sunlight
exposure is not controlled, it will influence both the intervention and
control groups since psoriasis is a photosensitive (and photoexcerbated)
dermatosis. <span class="citation" data-cites="Chakraborty_2024"></span>
- dermatology</p></li>
<li><p>survivorship bias - World War II planes <span class="citation"
data-cites="Silfwer_2017"></span> -
https://doctorspin.org/media-psychology/psychology/survivorship-bias/</p></li>
</ul>
<h4 class="unnumbered" id="representation-biases">Representation
Biases</h4>
<p>Those biases can be introduced :</p>
<ul>
<li><p><strong>Representation bias</strong>, non-representative sample
lead to missing subgroups or other representation anomalies, which can
be harmful to downstream applications. Popular ML datasets suffer from
representation bias <span class="citation"
data-cites="Mehrabi_2021 M142_Shankar_2017"></span></p></li>
<li><p><strong>Population Bias</strong>. Population bias arise when
statistics, demographics and characteristics in the sample differ from
the target population <span class="citation"
data-cites="M120_Olteanu_2019"></span>. The data it creates is
non-representative for the target population <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p><strong>Aggregation bias</strong> occurs, when "false conclusions
are drawn about individuals from observing the entire population". It
doesn’t matter, whether the subgroups are represented equally in the
training set, any generalized assumptions can result in aggregation bias
<span class="citation" data-cites="Mehrabi_2021"></span>. In medicine,
diseases can present themselves differently across genders and
ethnicities <span class="citation"
data-cites="M144_Suresh_2021"></span>. Therefore, diagnostic models need
to incorporate those differences to mitigate aggregation bias <span
class="citation" data-cites="Mehrabi_2021"></span>.</p></li>
<li><p><strong>Simpson’s Paradox</strong> is a type of aggregation bias,
which arises in heterogeneous data analysis. Observed associations
disappear or reverses in the subgroup data <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
</ul>
<h6 data-number="2.2.3.1.0.2" id="potential-biases-in-passion-1"><span
class="header-section-number">2.2.3.1.0.2</span> Potential Biases in
PASSION</h6>
<p>PASSION tries to mitigate representation bias, by including more FST
skin types - however, it could introuce other representation biases
Aggregation bias and Simpson’s Paradox could potentially be an issue
when the analyzed skin diseases present themselves differently in
patients based on their genetics</p>
<p>used</p>
<ul>
<li><p>Representation Bias. Representation bias arises from how we
sample from a population during data collection process <span
class="citation" data-cites="M144_Suresh_2021"></span>.
Non-representative samples lack the diversity of the population, with
missing subgroups and other anomalies <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p>Popular machine-learning datasets that serve as a base for most
of the developed algorithms and tools can also be biased—which can be
harmful to the downstream applications that are based on these datasets.
... In <span class="citation" data-cites="M142_Shankar_2017"></span>,
researchers showed that these datasets suffer from representation bias
and advocate for the need to incorporate geographic diversity and
inclusion while creating such datasets. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Population Bias. Population bias arises when statistics,
demographics, representatives, and user characteristics are different in
the user population of the platform from the original target population
<span class="citation" data-cites="M120_Olteanu_2019"></span>.
Population bias creates non-representative data. ... More such examples
and statistics related to social media use among young adults according
to gender, race, ethnicity, and parental educational background can be
found in <span class="citation" data-cites="M64_Hargittai_2007"></span>.
<span class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>Aggregation Bias. Aggregation bias (or ecological fallacy) arises
when false conclusions are drawn about individuals from observing the
entire population. An example of this type of bias can be seen in
clinical aid tools. Consider diabetes patients who have apparent
morbidity differences across ethnicities and genders. Specifically,
HbA1c levels, that are widely used to diagnose and monitor diabetes,
differ in complex ways across genders and ethnicities. Therefore, a
model that ignores individual differences will likely not be well-suited
for all ethnic and gender groups in the population <span
class="citation" data-cites="M144_Suresh_2021"></span>. This is true
even when they are represented equally in the training data. Any general
assumptions about subgroups within the population can result in
aggregation bias. <span class="citation"
data-cites="Mehrabi_2021"></span>. –&gt; could also be important for
dermatology issues!!!</p>
<ul>
<li><p>Simpson’s Paradox. Simpson’s paradox is a type of aggregation
bias that arises in the analysis of heterogeneous data [18]. The paradox
arises when an association observed in aggregated data disappears or
reverses when the same data is disaggregated into its underlying
subgroups (Fig. 2(a)). ... After analyzing graduate school admissions
data, it seemed like there was bias toward women, a smaller fraction of
whom were being admitted to graduate programs compared to their male
counterparts. However, when admissions data was separated and analyzed
over the departments, women applicants had equality and in some cases
even a small advantage over men. The paradox happened as women tended to
apply to departments with lower admission rates for both genders.
Simpson’s paradox has been observed in a variety of domains, including
biology [37], psychology [81], astronomy [109], and computational social
science [91].<span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
</ul></li>
</ul>
<h4 class="unnumbered" id="measurement-biases">Measurement Biases</h4>
<p>How features are chosen, used and measured can lead to biases <span
class="citation" data-cites="Mehrabi_2021 M144_Suresh_2021"></span>.</p>
<p>Examples for such biases are:</p>
<ul>
<li><p><strong>Measurement bias</strong> in general, e.g. using
mismeasured <span data-acronym-label="proxyVar"
data-acronym-form="plural+short">proxyVars</span> lead to
misinterpretations of the outcome <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p><strong>Observer bias</strong> is a subconscious bias which can
occur in different forms. Either, researchers projects their own
expectations on the research and influence the testers accordingly <span
class="citation" data-cites="Mester_2022"></span>. In other cases,
different observes report the same observation differently <span
class="citation" data-cites="Chakraborty_2024 c29 c26"></span></p></li>
<li><p><strong>Annotator bias</strong> is a special form of observer
bias. The labeling process of human annotators can be influenced by lots
of factors (e.g. personal background, social context) and even minor
design choices (e.g. scale order, image context). This can introduce
inconsistencies when labeling the data <span class="citation"
data-cites="Montoya_2025"></span></p></li>
<li><p><strong>Recall bias</strong>. This bias occurs when queried
individuals do not remember things correctly, due to humans selective
memory. This can cause misinterpretation, for example when analyzing
causes and effects of behaviour on certain diseases in medicine <span
class="citation"
data-cites="Mester_2022 Chakraborty_2024 c3-6 c2"></span>.</p></li>
</ul>
<h6 data-number="2.2.3.1.0.3" id="potential-biases-in-passion-2"><span
class="header-section-number">2.2.3.1.0.3</span> Potential Biases in
PASSION</h6>
<p>Measurement Bias (proxy var) - Country of Origin in PASSION depending
on the interpretation - should not be used for ethnicity, as this is not
linked directly to the genes, see example
https://medium.com/bcggamma/practice-ai-responsibly-with-proxy-variable-detection-42c2156ad986</p>
<p>Annotator bias regarding skin tone labeling has been investigated in
<span class="citation" data-cites="Montoya_2025"></span>. PASSION should
evaluate its process.</p>
<p>used</p>
<ul>
<li><p>Measurement Bias. Measurement, or reporting, bias arises from how
we choose, utilize, and measure particular features <span
class="citation" data-cites="M144_Suresh_2021"></span> (e.g. mismeasured
proxy variables) <span class="citation"
data-cites="Mehrabi_2021"></span>. (= e.g. someone who lives at that
postal code probably has this ethnicity ); –&gt; could that be an issue
with the country of origin feature?</p></li>
<li><p>This study found that while using skin tone instead of race for
fairness evaluations in computer vision seems objective, the annotation
process remains biased by human annotators. Untested scales, unclear
procedures, and a lack of awareness about annotator backgrounds and
social context significantly influence skin tone labeling. This study
exposes how even minor design choices in the annotation process, like
scale order (dark to light instead of light to dark) or image context
(face or no face, skin lesion presence), can sway agreement and
introduce uncertainty in skin tone assessments. ... The researchers
emphasize the need for greater transparency, standardized procedures,
and careful consideration of annotator biases to mitigate these
challenges and ensure fairer and more robust evaluations in computer
vision. <span class="citation" data-cites="Montoya_2025"></span> -
demographic dermatology bias</p></li>
<li><p>Observer bias - projecting expectations onto the research <span
class="citation" data-cites="Mester_2022 Mester_2017"></span> -
statistical bias</p></li>
<li><p>Observer bias: When different observers view the same
observation, they report it differently e.g., different observers may
give differing descriptions about subtle features in the histopathology
report of a skin biopsy.29 26. <span class="citation"
data-cites="Chakraborty_2024"></span> - dermatology</p></li>
<li><p>Recall bias - respondent doesn’t remember things correctly;
Recall bias is another common error of interview/survey situations. It
happens when the respondent doesn’t remember things correctly. It’s not
about bad or good memory – humans have selective memory by default.
After a few years (or even a few days), certain things stay and others
fade. It’s normal, but it makes research much more difficult. <span
class="citation" data-cites="Mester_2022 Mester_2017"></span> -
statistical bias</p></li>
<li><p>Memory or recall bias: This is a type of bias where sufferers of
a disease, often termed cases, have a greater tendency to recall a
particular habit than non-sufferers, viz controls. This results in an
uneven distribution of risk factors between the cases and controls. An
example of this would be a case-control study to evaluate the
association between dental amalgam use and the development of oral
lichen planus. Those with lichen planus are more likely to recall a
history of dental amalgam use than those who do not have the disease.
This difference in recall between a diseased cohort and control has
resulted in difficulties in assessing the association between diet and
many dermatological diseases – like milk and chocolate consumption and
acne, fatty meals and psoriasis, sugary meals and psoriasis,
agricultural exposure to insecticides and pemphigus and so on.3–6 2.
<span class="citation" data-cites="Chakraborty_2024"></span> -
dermatology</p></li>
</ul>
<h4 class="unnumbered" id="research-biases">Research Biases</h4>
<p>Researchers and their processes can also be biased in multiple
ways:</p>
<ul>
<li><p><strong>Funding / Sponsorship bias</strong>, when a study is
deliberately supporting those findings, which the sponsor expects <span
class="citation"
data-cites="Chakraborty_2024 c22 Mester_2017"></span></p></li>
<li><p><strong>Data dredging bias</strong>. The statistical methods and
model are chosen to provide a certain p-value, to improve the
probability of the research hypothesis being true. <span
class="citation" data-cites="Chakraborty_2024"></span></p></li>
<li><p><strong>Hypothetical bias</strong>. Hypothetical questions lead
to responses that do not reflect, what interviewees would do in real
life. <span class="citation"
data-cites="Chakraborty_2024 c31 c28"></span></p></li>
</ul>
<h6 data-number="2.2.3.1.0.4" id="potential-biases-in-passion-3"><span
class="header-section-number">2.2.3.1.0.4</span> Potential Biases in
PASSION</h6>
<p>Since the PASSION dataset is already published, the research biases
might already be introduced. It is not feasible during the duration of
this thesis to make an evaluation on those biases. Instead, I would
recommend the PASSION team and researcher in general, to check the list
above carefully and take measures against them. Maybe, an external
evaluation could help to detect and prevent those biases even
better.</p>
<p>used</p>
<ul>
<li><p>Funding bias <span class="citation"
data-cites="Mester_2022 Mester_2017"></span> - statistical bias</p></li>
<li><p>Industry sponsorship bias: This has now been reclassified as
conflict-of-interest bias. In short, the study deliberately supports the
findings expected from it by its sponsors. 22.<span class="citation"
data-cites="Chakraborty_2024"></span> - dermatology</p>
<p>Reporting biases</p></li>
<li><p>Data dredging bias: It is an entirely avoidable bias. This is
subdivided into two types – Fishing type and “P-value hacking” type. It
involves using multiple statistical methods to get the desired p-value
and selecting the statistical model that gives the p-value the author
wants. This is “lamentably common” in dermatological research.16 To
detect data dredging bias, always perform a “p-curve analysis” while
performing a meta-analysis.17,18 Much emphasis is nowadays given to the
confidence interval instead of the p-value, which gives an approximate
idea of the range in which one can be 95% (or 90%, depending on the
confidence interval chosen) sure that the result is correct. The
confidence interval remains unaffected by p-value dredging. This subject
has been reviewed in depth in recent works.18,19 15.<span
class="citation" data-cites="Chakraborty_2024"></span></p></li>
<li><p>Hypothetical bias: Many dermatological researches (and some life
quality questionnaires like vitiQoL) use hypothetical questions – like
“What would you do when some stranger asks you about your lesion?”. The
responses to these questions by the study participants often do not
tally with what they would do in real life. This is called hypothetical
bias and is avoided by adopting the ex-ante approach.31 28. <span
class="citation" data-cites="Chakraborty_2024"></span> -
dermatology</p></li>
</ul>
<h4 class="unnumbered" id="feature-representation-biases">Feature
Representation Biases</h4>
<p>Some of those biases are:</p>
<ul>
<li><p><strong>Omitted Variable Bias</strong> arises when variables are
not included in the model, which leads to situations for which the model
is not ready for <span class="citation"
data-cites="Mehrabi_2021 Mester_2022"></span><span class="citation"
data-cites="M38_Clarke_2005 M131_Riegg_2008"></span><span
class="citation" data-cites="M114_Mustard_2003"></span>.</p></li>
<li><p><strong>Collider Bias</strong> Two variables can influence a
common third variable, the collider variable. When sampling is
restricted by this collider variable, it could lead to a distortion
<span class="citation"
data-cites="Chakraborty_2024 c4 c8 c9"></span>.</p></li>
</ul>
<h6 data-number="2.2.3.1.0.5" id="potential-biases-in-passion-4"><span
class="header-section-number">2.2.3.1.0.5</span> Potential Biases in
PASSION</h6>
<p>The ethnicity is omitted in the PASSION dataset which could lead to
issues See the medical section for more specific collider bias, maybe
there could be others</p>
<p>used</p>
<ul>
<li><p>Omitted Variable Bias. Omitted variable bias4 occurs when one or
more important variables are left out of the model <span
class="citation"
data-cites="M38_Clarke_2005 M131_Riegg_2008"></span><span
class="citation" data-cites="M114_Mustard_2003"></span>. Something that
the model was not ready for<span class="citation"
data-cites="Mehrabi_2021"></span>. did not take into account <span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>Omitted variable bias <span class="citation"
data-cites="Mester_2022 Mester_2017"></span> - statistical bias</p></li>
<li><p>Collider Bias: This is an under-appreciated bias, and often
confused with a confounder. This is especially seen in observational
studies where it is defined as a distortion produced by the restriction
of sampling by a collider variable. A collider variable is defined as
one that has an independent effect on the outcome studied apart from the
studied variable. In simpler terms, collider bias occurs when exposure
and development influence a common third variable. That variable or
collider is controlled by study design or in the analysis. An example is
the observation that psoriasis patients tend to have more depression and
anxiety disorders. Since severe psoriasis patients tend to get
hospitalised and also get screened for mental health issues, a spurious
association between them could have been obtained due to collider bias.
The two variables viz psoriasis and depression converged, i.e.,
collided, into a single outcome – hospitalization.8,9 4. <span
class="citation" data-cites="Chakraborty_2024"></span> -
dermatology</p></li>
</ul>
<h4 class="unnumbered" id="imaging-biases">Imaging Biases</h4>
<p>Dealing with images can lead to a whole other set of challenges,
which can lead to biases. The challenges are for example technical
variations in hardware and software but also differences in how images
are gathered or what is in it <span class="citation"
data-cites="Young_2020"></span>.</p>
<p>Those biases can be introduced :</p>
<ul>
<li><p><strong>Image Quality Bias</strong>. The quality of an image
(zoom level, focus, lightning) could be associated with the
classification <span class="citation"
data-cites="Young_2020"></span></p></li>
<li><p><strong>Visual Artifact Bias</strong>. Other artifacts, such as
presence of hair or surgical ink markings on dermatology images, can
decrease classification performance <span class="citation"
data-cites="Winkler et al. 2019 &amp; Bisla et al. 2019 (from Young_2020)"></span></p></li>
<li><p><strong>Field of View Bias</strong>. What view is captured in the
image can interfere with prediction quality what is it, consequence
<span class="citation"
data-cites="Mishra et al. 2019 from Young_2020"></span></p></li>
</ul>
<h6 data-number="2.2.3.1.0.6" id="potential-biases-in-passion-5"><span
class="header-section-number">2.2.3.1.0.6</span> Potential Biases in
PASSION</h6>
<p>The PASSION model could learn to associate unrelated visual effects,
hair, body parts or image quality with a disease.</p>
<p>used</p>
<ul>
<li><p>Image quality. Several barriers to AI implementation in the
clinic need to be overcome with regards to imaging (Figure 1). These
include technical variations (e.g., camera hardware and software) and
differences in image acquisition and quality (e.g., zoom level, focus,
lighting, and presence of hair). For example, the presence of surgical
ink markings is associated with decreased specificity (Winkler et al.,
2019), field of view can significantly affect prediction quality (Mishra
et al., 2019), and classification performance improves when hair and
rulers are removed (Bisla et al., 2019). We have developed a method to
measure how model predictions might be biased by the presence of a
visual artifact (e.g., ink) and proposed methods to reduce such biases
(Pfau et al., 2019). Poor quality images are often excluded from
studies, but the problem of what makes an image adequate is not well
studied. Ideally, models need to be able to express a level of
confidence in a prediction as a function of image quality and
appropriately direct a user to retake photos if needed. <span
class="citation" data-cites="Young_2020"></span> - dermatology</p></li>
</ul>
<h4 class="unnumbered" id="medical-biases">Medical Biases</h4>
<p>In ML for health care, there are special medical versions of the
mentioned biases as well as completely new biases. They require special
attention, since they directly influence the diagnosis or treatment of a
disease.</p>
<p>Those biases can be introduced:</p>
<ul>
<li><p><strong>Berkesonian bias</strong> occurs in hospital-based
studies when two variables influence hospital or clinical attendance
independently. This can lead to a distorted estimation of the
relationship between those variables because the study population of
hospitalized patients is not representative of the whole population
<span class="citation"
data-cites="Chakraborty_2024 c3 c7"></span></p></li>
<li><p><strong>Informed presence bias</strong>, the probability to get
screened for other diseases is higher for people who seek medical care.
Like Berkesonian bias, this can lead to misleading interpretations of
relationships between two diseases <span class="citation"
data-cites="Chakraborty_2024 c27 c23"></span></p></li>
<li><p><strong>Diagnostic access bias</strong>, depending on the
geographical location, individuals have better access to medical care.
Therefore, their disease prevalence could appear to be higher and
diseases could be diagnosed earlier. <span class="citation"
data-cites="Chakraborty_2024 c19-c21"></span></p></li>
<li><p><strong>Diagnostic reference test bias</strong> is a
<strong>verification bias</strong>, where not all individuals receive
the same reference test for the diagnostic process, potentially leading
to different diagnoses. <span class="citation"
data-cites="Chakraborty_2024 c21"></span></p></li>
<li><p><strong>Mimicry bias</strong>, exposures to treatment options can
cause a disease which presents itself similar to the study disease,
which potentially creates misleading data <span class="citation"
data-cites="Chakraborty_2024 c28 c25"></span></p></li>
<li><p><strong>Unacceptable Disease bias</strong>. When a disease is
socially unacceptable, it can result in under-reporting of the same
disease <span class="citation"
data-cites="Chakraborty_2024 c30 c27"></span></p></li>
<li><p><strong>Healthy volunteer selection bias</strong>, is a type of
self-selection bias where the volunteers are in general healthier than
the population due to more interest in health <span class="citation"
data-cites="Delgado-Rodriguez_2004"></span></p></li>
</ul>
<h6 data-number="2.2.3.1.0.7" id="potential-biases-in-passion-6"><span
class="header-section-number">2.2.3.1.0.7</span> Potential Biases in
PASSION</h6>
<p>Berkesonian bias depending on the chosen hospitals Informed presence
bias regarding correlation between impedigo and the other diseases
Diagnostic access bias can somewhat be addressed by PASSION, since its
dataset includes samples of later states of diseases. However, in the
PASSION context itself, this bias could still be relevant. Diagnostic
reference test bias could be inherited in the PASSION dataset, depending
on how the dermatologists work. Mimicry bias is not relevant regarding
the exposures since PASSION does not hold any exposure data. However,
diseases which mimicry others could lead to issues if they are not
detected.</p>
<p>used</p>
<ul>
<li><p>Berkesonian Bias: Named after Dr. Joseph Berkeson, this bias
reflects the variation in rates of hospital admission or clinic
attendance for different diseases. For example, if a study is conducted
to know the effect of pregnancy on syphilis in an antenatal clinic, we
are likely to get biased data since the two conditions, viz pregnancy
and syphilis, are both likely to affect clinic attendance and all
observations related to the relationship between pregnancy and
syphilis.7 3. <span class="citation"
data-cites="Chakraborty_2024"></span> - dermatology</p></li>
<li><p>Informed presence bias: Simply, a person attending a health
center is more likely to get screened for other unrelated comorbidities
than those not attending a health center e.g., the finding psoriasis is
associated with depression has now been criticised because those having
psoriasis also have a greater chance to be screened for depression since
they are already attending a health center.27 23. <span class="citation"
data-cites="Chakraborty_2024"></span> - dermatology</p></li>
<li><p>Diagnostic Access Bias: Individuals in certain geographical
localities have better access to medical care and, hence, may appear to
have higher disease prevalence. For example, atopic dermatitis is
believed to be commoner in the West – this could be due to better and
earlier diagnostic facilities available than in India.19,20 17.<span
class="citation" data-cites="Chakraborty_2024"></span></p></li>
<li><p>Diagnostic reference test bias: These bias results when all
individuals do not receive the same reference test. e.g., direct
immunofluorescence studies may not be done for all patients with
pemphigus vulgaris some patients may receive only a skin biopsy-based
diagnosis. It is a subtype of verification bias. Another variation of
this type of bias is partial reference bias, where only some of the
study participants receive the index and the reference tests.21<span
class="citation" data-cites="Chakraborty_2024"></span></p></li>
<li><p>Mimicry bias: When an exposure causes a disease that resembles
the study disease, mimicry bias can result. For example, certain drugs
are known to cause a pityriasis rosea-like reaction, which, although
looks like pityriasis rosea, differs from it.28 25.<span
class="citation" data-cites="Chakraborty_2024"></span> -
dermatology</p></li>
<li><p>Unacceptable disease bias: This occurs in socially unacceptable
diseases like leprosy and STDs, which result in under-reporting.30 27.
<span class="citation" data-cites="Chakraborty_2024"></span> -
dermatology</p></li>
<li><p>Other such studies were conducted in [<span class="citation"
data-cites="M54_Fry_2017"></span>] which states that UK Biobank, a large
and widely used genetic dataset, may not represent the sampling
population. Researchers found evidence of a “healthy volunteer”
selection bias. [150] has other examples of studies on existing biases
in the data used in the medical domain. [157] also looks at
machine-learning algorithms and data utilized in medical fields, and
writes about how artificial intelligence in health care has not impacted
all patients equally.<span class="citation"
data-cites="Mehrabi_2021"></span> –&gt; [150] also provides an ovverview
over the impact of social determinants on health, such as Economic
stability, neighborhood and physical environment, education, food,
community and scial context, access to healthcare and quality</p></li>
<li><p>The healthy volunteer effect is a particular case: when the
participants are healthier than the general population. <span
class="citation" data-cites="Delgado-Rodriguez_2004"></span></p></li>
</ul>
<h4 class="unnumbered" id="temporal-biases">Temporal Biases</h4>
<p>Differences in populations and their behaviour over time can lead to
temporal biases <span class="citation"
data-cites="M120_Olteanu_2019"></span>. Certain studies require to track
temporal data, to learn about their behaviour over time. Disease
progression is also a factor measured over time <span class="citation"
data-cites="Mehrabi_2021"></span>. For PASSION, temporal biases are
currently irrelevant, since PASSION contains images independently of
time and is not tracking the disease progression. Therefore, the listed
biases in this chapter are not explained in detail, refer to the sources
for further information.</p>
<p>Examples for temporal data biases are:</p>
<ul>
<li><p><strong>Longitudinal Data Fallacy</strong> <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p><strong>Chronological bias</strong> <span class="citation"
data-cites="Chakraborty_2024 c9 c13"></span></p></li>
<li><p><strong>Immortal time bias</strong> <span class="citation"
data-cites="Chakraborty_2024 c24 c20"></span></p></li>
</ul>
<h3 data-number="2.2.4" id="algorithmic-biases"><span
class="header-section-number">2.2.4</span> Algorithmic Biases</h3>
<p>When an algorithm adds biases to unbiased input data one speaks of
<strong>Algorithmic Bias</strong> <span class="citation"
data-cites="M9_Baeza-Yates_2018"></span>. This could occur due to
algorithmic design choices like optimization functions, regularizations
and statistically biased estimators <span class="citation"
data-cites="M44_Danks_2017"></span>.</p>
<p>used</p>
<ul>
<li><p>Algorithmic Bias. Algorithmic bias is when the bias is not
present in the input data and is added purely by the algorithm <span
class="citation" data-cites="M9_Baeza-Yates_2018"></span>. The
algorithmic design choices, such as use of certain optimization
functions, regularizations, choices in applying regression models on the
data as a whole or considering subgroups, and the general use of
statistically biased estimators in algorithms <span class="citation"
data-cites="M44_Danks_2017"></span>, can all contribute to biased
algorithmic decisions that can bias the outcome of the algorithms.<span
class="citation" data-cites="Mehrabi_2021"></span>.</p></li>
</ul>
<h4 class="unnumbered" id="user-algorithm-interaction-biases">User
Algorithm Interaction Biases</h4>
<ul>
<li><p><strong>User Interaction Bias</strong>. This biases can be
triggered by the user interface or the user themselves. The user
interface influences the user to behave in a certain way, which could
introduce bias in the user behaviour. Users impose this (or their own)
biased behavior through interaction on the algorithm <span
class="citation" data-cites="M9_Baeza-Yates_2018"></span>.
<strong>Presentation bias</strong> and <strong>Ranking bias</strong> are
further subtypes mentioned by <span class="citation"
data-cites="M93_Lerman_2014 Mehrabi_2021"></span>.</p></li>
<li><p><strong>Emergent Bias</strong>. When real users interact with an
algorithm, this bias arises some time after the design was completed due
to changes in population. It appears more likely in user interfaces
<span class="citation" data-cites="M53_Friedman_1996"></span>.</p></li>
</ul>
<h6 data-number="2.2.4.0.0.1" id="potential-biases-in-passion-7"><span
class="header-section-number">2.2.4.0.0.1</span> Potential Biases in
PASSION</h6>
<p>The user interaction biases, especially the emergent bias could
potentially become an issue for PASSION, when the project starts to
become publicly available <span data-acronym-label="teledermatology"
data-acronym-form="singular+short">teledermatology</span>. Also, the
interface design should be evaluated, so that no presentation or ranking
bias gets introduced.</p>
<p>used</p>
<ul>
<li><p>Emergent Bias. Emergent bias occurs as a result of use and
interaction with real users. This bias arises as a result of change in
population, cultural values, or societal knowledge usually some time
after the completion of design <span class="citation"
data-cites="M53_Friedman_1996"></span>. This type of bias is more likely
to be observed in user interfaces, ... This type of bias can itself be
divided into more subtypes, as discussed in detail in <span
class="citation" data-cites="M53_Friedman_1996"></span>. <span
class="citation" data-cites="Mehrabi_2021"></span>. probably less
relevant at the first stage</p></li>
<li><p>User Interaction Bias. User Interaction bias is a type of bias
that can not only be observant on the Web but also get triggered from
two sources—the user interface and through the user itself by imposing
his/her self-selected biased behavior and interaction <span
class="citation" data-cites="M9_Baeza-Yates_2018"></span>. This type of
bias can be influenced by other types and subtypes, such as presentation
and ranking biases. <span class="citation"
data-cites="Mehrabi_2021"></span>. – more relevant for later, when the
application would become bigger</p>
<ul>
<li><p>Presentation Bias. Presentation bias is a result of how
information is presented <span class="citation"
data-cites="M9_Baeza-Yates_2018"></span> (can only click on content they
see, could be the case that user does not see all info on web) <span
class="citation" data-cites="Mehrabi_2021"></span>.</p></li>
<li><p>Ranking Bias. The idea that top-ranked results are the most
relevant and important will result in attraction of more clicks than
others. This bias affects search engines <span class="citation"
data-cites="M9_Baeza-Yates_2018"></span> and crowdsourcing applications
<span class="citation" data-cites="M93_Lerman_2014"></span>.<span
class="citation" data-cites="Mehrabi_2021"></span>.</p></li>
</ul></li>
</ul>
<h4 class="unnumbered" id="external-influence-biases">External Influence
Biases</h4>
<p>Those biases can be introduced :</p>
<ul>
<li><p><strong>Evaluation Bias</strong>. When inappropriate or
disproprtionate benchmarks are used in model evaluation, they can
introduce the benchmarks biases into the model. <span class="citation"
data-cites="M144_Suresh_2021 M24_Buolamwini_2018"></span></p></li>
<li><p><strong>Incorporation bias</strong>. When index tests in
diagnostic accuracy studies are part of the reference tests, this
results in elevated sensitivity for the index tests <span
class="citation"
data-cites="Chakraborty_2024 c21 c25 c26 Young_2020"></span>.</p></li>
<li><p><strong>Popularity Bias</strong>. More popular items tend to be
exposed more. Popularity metrics can be manipulated though or not
reflecting good quality, this can lead to bias <span class="citation"
data-cites="M117_Ciampaglia_2018 Mehrabi_2021"></span>.</p></li>
<li><p><strong>Generalization Issues</strong>. <span class="citation"
data-cites=""></span></p></li>
</ul>
<h6 data-number="2.2.4.0.0.2" id="potential-biases-in-passion-8"><span
class="header-section-number">2.2.4.0.0.2</span> Potential Biases in
PASSION</h6>
<p>used</p>
<ul>
<li><p>Evaluation Bias. Evaluation bias happens during model evaluation
<span class="citation" data-cites="M144_Suresh_2021"></span>. This
includes the use of inappropriate and disproportionate benchmarks for
evaluation of applications such as Adience and IJB-A benchmarks. These
benchmarks are used in the evaluation of facial recognition systems that
were biased toward skin color and gender <span class="citation"
data-cites="M24_Buolamwini_2018"></span>, and can serve as examples for
this type of bias <span class="citation"
data-cites="M144_Suresh_2021"></span>. <span class="citation"
data-cites="Mehrabi_2021"></span>. – important for this thesis</p></li>
<li><p>Incorporation bias: This is principally relevant for diagnostic
accuracy studies when the index test forms a part of the reference test,
resulting in elevated sensitivity e.g., if one wants to compare the
grattage test vs. dermoscopy in psoriasis and does dermoscopy only from
areas of grattage positivity, one would get a very high sensitivity for
the grattage test because it was incorporated into the reference test,
i.e., dermoscopy.25,26 21.<span class="citation"
data-cites="Chakraborty_2024"></span></p></li>
<li><p>Popularity Bias. Items that are more popular tend to be exposed
more. However, popularity metrics are subject to manipulation—for
example, by fake reviews or social bots <span class="citation"
data-cites="M117_Ciampaglia_2018"></span>. ... this presentation may not
be a result of good quality; instead, it may be due to other biased
factors. <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
</ul>
<h3 data-number="2.2.5" id="user-biases"><span
class="header-section-number">2.2.5</span> User Biases</h3>
<h4 class="unnumbered" id="cognitive-biases">Cognitive Biases</h4>
<p>Biases which are related to human perception belong to the category
of cognitive biases. They are affecting how data should be presented and
interpreted <span class="citation" data-cites="Mester_2017"></span></p>
<p>Those biases can be introduced :</p>
<ul>
<li><p><strong>Confirmation Bias</strong>. When people have
pre-conceptions, they will only listen to the part of presented
information which reinforce those "facts", regardless whether the facts
are true or not <span class="citation" data-cites="Mester_2017"></span>.
In health-care, this can be observed when patients report increases in
diseases due to potentially nonfactual information they found on the
internet <span class="citation"
data-cites="Chakraborty_2024 c15 c14"></span>.</p></li>
<li><p><strong>Belief Bias</strong>. A stronger version of the
confirmation bias: Someone who is affected by this bias is so sure about
their own gut feelings that they will ignore results of a data research
project <span class="citation"
data-cites="Mester_2017"></span>.</p></li>
<li><p><strong>Previous Opinion Bias</strong>. When performing multiple
tests, the knowledge about the outcome of the previous tests probably
influences the results <span class="citation"
data-cites="Chakraborty_2024"></span></p></li>
<li><p><strong>Cause-Effect Bias</strong>. The famous senctence
"correlation does not imply causation" can be used here - when
correlation between two variables is misinterpreted as a cause-effect in
the wrong direction, this bias applies <span class="citation"
data-cites="Mester_2017"></span></p></li>
<li><p><strong>Historical Bias</strong>. Preexisting biases in the world
can affect the data generation process <span class="citation"
data-cites="M144_Suresh_2021"></span>. Even if they reflect the current
reality, it is worth to consider whether those biases should affect the
algorithms in question <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p><strong>Content Production Bias</strong>. User generated contents
can introduce biases by systematical differences in the production
process, stucture and appearance, which might stem from the users
background <span class="citation"
data-cites="M120_Olteanu_2019"></span>.</p></li>
</ul>
<h6 data-number="2.2.5.0.0.1" id="potential-biases-in-passion-9"><span
class="header-section-number">2.2.5.0.0.1</span> Potential Biases in
PASSION</h6>
<p>For PASSION, confirmation bias could lead to issues in the initial
diagnosis and could therefore lead to biased data labeling. Same with
the previous opinion bias. The later can be reduced when it is ensured,
that the labeling experts are diagnosing the diseases independently of
each other, so that they do not know the previous opinions. Cause-Effect
bias is lesser an issue for PASSION, since the causes of the diseases
are not analyzed. It could more be an inherit problem, that the
algorithm learns wrong causes for diseases, such as appearing hair
Historical bias can affect PASSIONs process in various ways. In PASSION
context, Content Production Bias could have an impact on how the images
are taken.</p>
<p>used</p>
<ul>
<li></li>
<li><p>Cognitive bias <span class="citation"
data-cites="Mester_2017"></span> - statistical bias</p></li>
<li><p>Previous opinion bias: In performing a second diagnostic test, if
the result of a previous test is known, it is likely to influence the
result. An extension of this is the Greenwald’s law of lupus: the
Sontheimer amendment – anything and everything that happens to a lupus
erythematosus patient is correctly or incorrectly attributed to lupus.32
29. <span class="citation" data-cites="Chakraborty_2024"></span> -
dermatology</p></li>
<li><p>Confirmation bias: This bias occurs when study participants have
a preconceived notion of their disease that may not be based on facts.
For example, we have observed that in North India many tinea patients
report an increase in their disease due to taking meat, fish, and other
so-called “hot foods”. They may also present information they have
collected from the internet which reinforces their beliefs.15 14.<span
class="citation" data-cites="Chakraborty_2024"></span> -
dermatology</p></li>
<li><p>Cause-effect bias <span class="citation"
data-cites="Mester_2022 Mester_2017"></span> - statistical bias</p></li>
<li><p>Historical Bias. Historical bias is the already existing bias and
socio-technical issues in the world and can seep into from the data
generation process even given a perfect sampling and feature selection
<span class="citation" data-cites="M144_Suresh_2021"></span>. ... search
results were of course reflecting the reality, but whether or not the
search algorithms should reflect this reality is an issue worth
considering <span class="citation" data-cites="Mehrabi_2021"></span> -
maybe relevant</p></li>
<li><p>Content Production Bias. Content Production bias arises from
structural, lexical, semantic, and syntactic differences in the contents
generated by users <span class="citation"
data-cites="M120_Olteanu_2019"></span>. <span class="citation"
data-cites="Mehrabi_2021"></span> – could the quality of the pictures
been related to this as well?</p></li>
</ul>
<h4 class="unnumbered" id="behavioral-biases">Behavioral Biases</h4>
<p>Those biases can be introduced :</p>
<ul>
<li><p><strong>Behavioral Bias</strong>. User behaviour can differ
depending on the platforms, contexts, cultures, or datasets <span
class="citation" data-cites="M120_Olteanu_2019"></span>.</p></li>
<li><p><strong>Self-Selection Bias</strong>. This subtype of selection
bias occurs when study participants can select themselves. Less
proactive people, people with less time or interest will be excluded or
underrepresented <span class="citation"
data-cites="Mester_2022 Mehrabi_2021"></span>. <strong>Non-Responder
bias</strong> is a subtype, where part of the population is not
responding e.g. to fill out a survey or post-study responses queried by
postal services <span class="citation"
data-cites="Chakraborty_2024"></span>.</p></li>
<li><p><strong>Social Bias</strong>. When the actions of others affect
our judgment, it is called social bias. For example ratings in juries
can be affected by this <span class="citation"
data-cites="M9_Baeza-Yates_2018"></span>.</p></li>
</ul>
<h6 data-number="2.2.5.0.0.2" id="potential-biases-in-passion-10"><span
class="header-section-number">2.2.5.0.0.2</span> Potential Biases in
PASSION</h6>
<p>For PASSION the behavioral biases can affect who is going to the
dermatologists for what reasons. Therefore, the approach to use data
from different countries may be benefitial, since potentially the
cultural differences could differ. Self-selection is an issue, since
only those patients can be included in the database which go to the
hospitals.</p>
<p>used</p>
<ul>
<li><p>Self-Selection Bias. Self-selection bias4 is a subtype of the
selection or sampling bias in which subjects of the research select
themselves. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Self-selection bias - when you let the subjects of the analyses
select themselves, less proactive people will be excluded <span
class="citation" data-cites="Mester_2022 Mester_2017"></span>-
statistical bias A variation of this is non-responder bias, where
non-responders to a questionnaire differ significantly from responders.9
9. <span class="citation" data-cites="Chakraborty_2024"></span> -
dermatology</p></li>
<li><p>Social Bias. Social bias happens when others’ actions affect our
judgment <span class="citation"
data-cites="M9_Baeza-Yates_2018"></span>. (case where we want to rate or
review an item with a low score, but when influenced by other high
ratings, we change our scoring thinking that perhaps we are being too
harsh [<span class="citation" data-cites="M9_Baeza-Yates_2018"></span>,
<span class="citation" data-cites="M151_Wang_2014"></span>.) <span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>Behavioral Bias. Behavioral bias arises from different user
behavior across platforms, contexts, or different datasets <span
class="citation" data-cites="M120_Olteanu_2019"></span>. <span
class="citation" data-cites="Mehrabi_2021"></span> maybe, people from
different countries go to the dermatologist for different diseases,
based on cultural differences?</p></li>
</ul>
<h4 class="unnumbered" id="publication-biases">Publication Biases</h4>
<p>Those biases can be introduced :</p>
<ul>
<li><p><strong>Publication Bias</strong></p></li>
<li><p><strong>Hot stuff bias</strong> is a subtype of publication bias,
where Journals are less critical about trending topics, which lead to
more frequent publishing of those topics. This in turn can lead to
flawed meta-analyses regarding those topics <span class="citation"
data-cites="Chakraborty_2024 c22 c23 c19"></span>.</p></li>
<li><p><strong>All is Well Bias</strong>. This bias is a different view
on the hot stuff bias. Theories which align with the view of the
majority are more likely to be pubhlished than an opposing view <span
class="citation"
data-cites="Chakraborty_2024 c7 c10-12"></span>.</p></li>
<li><p><strong>Rethoric Bias</strong>. Charismatic writing or when the
press is more vocal about findings can lead to greater influence over
individuals than other available facts <span class="citation"
data-cites="Chakraborty_2024"></span>.</p></li>
<li><p><strong>Novelty Bias</strong>. Newer interventions appear to be
better. Over time, this effect decreases <span class="citation"
data-cites="Chakraborty_2024"></span>.</p></li>
</ul>
<h6 data-number="2.2.5.0.0.3" id="potential-biases-in-passion-11"><span
class="header-section-number">2.2.5.0.0.3</span> Potential Biases in
PASSION</h6>
<p>These biases are relevant for all researchers. They should kept in
mind when interpreting, publishing and peer-reviewing papers.</p>
<p>used</p>
<ul>
<li><p>Hot stuff bias: Editors of journals may be less critical about
topics that are “fashionable” or currently in vogue and consequently end
up publishing them more frequently, resulting in publication bias as
well as hot stuff bias. It can result in flawed meta-analyses based on
these studies. An example is how cutaneous manifestations of COVID-19
were published. Indian Journal of Dermatology Venereology and Leprosy
stood out by choosing not to publish anything and everything related to
COVID-19, thus reducing hot stuff bias.22,23 19. <span class="citation"
data-cites="Chakraborty_2024"></span></p></li>
<li><p>All is well bias: It is a subjective bias where theories
supported by the majority tend to get more easily published than the
opposing view supported by the minority. For example, ideas on the
origin of endemic pemphigus supporting autoimmunity are more likely to
be published than theories exploring an infectious trigger. According to
some authors, this bias is very difficult to eliminate and is a variant
of publication bias.10-12 7.<span class="citation"
data-cites="Chakraborty_2024"></span> - dermatology</p></li>
<li><p>Rhetoric bias: A more charismatic piece of writing has a greater
influence on the study participants than other available literature. An
example is the wider use of sunscreen for polymorphous light eruption
over photoprotective strategies like umbrellas, broadbrimmed hats, etc,
because the lay press is more vocal about sunscreens.14 11. <span
class="citation" data-cites="Chakraborty_2024"></span> -
dermatology</p></li>
<li><p>Novelty bias: The newer an intervention, the better it appears,
and with time, its efficacy seems to decrease. When ligelizumab, an IgE
antagonist was first discovered, ligelizumab was believed to be better
than omalizumab; however, evidence soon pointed to the contrary.
16.<span class="citation" data-cites="Chakraborty_2024"></span> -
dermatology</p></li>
</ul>
<h4 class="unnumbered" id="medical-biases-1">Medical Biases</h4>
<p>Those biases can be introduced :</p>
<ul>
<li><p><strong>Popularity Bias</strong>. In medicine, when more popular
diseases (usually well-known or stigmatized ones) get compared with less
popular diseases, clinic rates can show a distorted view. The more
popular diseases appear to be over-represented over more commoner ones
<span class="citation"
data-cites="Chakraborty_2024 c9 c6"></span>.</p></li>
<li><p><strong>Apprehension Bias</strong>. Fear related to an upcoming
procedure can lead to false evaluations, e.g. when measuring blood
pressure <span class="citation"
data-cites="Chakraborty_2024 c13"></span>.</p></li>
<li><p><strong>Hawthrone bias</strong>. Subjects might modify their
behaviour when they know they are being watched. This bias can be
practically utilized by introducing regular follow-ups <span
class="citation" data-cites="Chakraborty_2024 c8"></span>.</p></li>
<li><p><strong>Centripetal Bias</strong>. Better reputations affect to
which physicians or hospitals patients tend to go to. Famous specialists
probably see more cases in regards of their specialty than others <span
class="citation" data-cites="Chakraborty_2024 c12"></span>.</p></li>
</ul>
<h6 data-number="2.2.5.0.0.4" id="potential-biases-in-passion-12"><span
class="header-section-number">2.2.5.0.0.4</span> Potential Biases in
PASSION</h6>
<p>PASSION must be careful in interpreting the metadata. Since the data
is from hospitals, they could be biased towards more popular diseases.
PASSION can potentially use Hawthrone bias to improve the work of the
annotators. Centripetal bias can also be used when selecting the
partners to work with.</p>
<p>used</p>
<ul>
<li><p>Popularity Bias: This bias arises when a particular disease is
more popular (i.e. either more well-known or more stigmatised) among the
participants than the disease with which it is compared. For example, if
a study compares clinic attendance rates among various dermatological
disorders, one would see vitiligo patients are over-represented over
melasma. While melasma is commoner in the normal population, vitiligo,
due to its popularity because of media publicity and other factors,
tends to present earlier.9 6. <span class="citation"
data-cites="Chakraborty_2024"></span> - dermatology</p></li>
<li><p>Apprehension bias: This results from fear and apprehensions
related to an impending procedure. The classic example is the false
elevation of blood pressure because the person is apprehensive of his or
her blood pressure being measured.13 A variant of this is the Hawthorne
bias, where subjects modify their behavior, such as regularly taking a
prescribed drug or exercising, simply because they know they are being
watched, but not due to any apprehensions. Hawthorne bias is practically
utilised in many leprosy clinics since regular follow-up has been shown
to improve adherence to therapy based on Hawthorne bias. 8. <span
class="citation" data-cites="Chakraborty_2024"></span> -
dermatology</p></li>
<li><p>Centripetal bias: Patients tend to go to more reputed physicians
and hospitals than others. For example, a famous or better-known
cosmetologist with a good reputation tends to see more cases than other
cosmetologists. 12.<span class="citation"
data-cites="Chakraborty_2024"></span> - dermatology</p></li>
</ul>
<h3 data-number="2.2.6" id="sensitive-features"><span
class="header-section-number">2.2.6</span> Sensitive Features</h3>
<p>Research showed sensitive features which are prone to bias. Those led
to issues in existing AI applications and their usage should therefore
be investigated carefully.</p>
<p><a href="#tab:biases_features" data-reference-type="ref+label"
data-reference="tab:biases_features">2.4</a> summarizes mentioned
features. Since PASSION tries to improve the classification of skin
diseases with photographs alone, those features which directly influence
the appearance of a disease on the skin are most important. Skin type
including undertone are affecting the diseases appearance, but also
genetic factors, gender and the age of a patient can influence how the
diseases manifest themselves. Therefore, they are directly relevant for
skin detection and must be considered in the dataset.</p>
<p>Other demographic factors can have an impact on the progression of
skin diseases or increase the prevalence of skin conditions (e.g.
tropical vs dry climates) . While this information could support the
diagnostic process, it is not visible in the images. Therefore, they
could potentially be relevant and enhance the diagnostic process when
included into the PASSION dataset. However, the linkage is weak and
could introduce further biases.</p>
<p>For completeness, <a href="#tab:biases_features"
data-reference-type="ref+label"
data-reference="tab:biases_features">2.4</a> shows further sensitive
demographic features which seem not to be linked to dermatology
according to research.</p>
<p>Other important features according to (<span class="citation"
data-cites="Montoya_2025"></span> 13): lesion type, anatomical location
of lesion, img characteristics such as source, imaging techniques,
resolution, real vs. artificially generated</p>
<div class="threeparttable">
<div id="tab:biases_features">
<table>
<caption>Features which often hold biases</caption>
<tbody>
<tr>
<td style="text-align: left;"><strong>Bias-Sensitive
Features</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>ML</strong></td>
<td style="text-align: left;"><strong>Dermatology</strong></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Skin Type</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Skin Undertones</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Age</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Gender/Sex</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Gender and Skin Type Subgroups</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Ethnicity/Race</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Geographic Location</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Socio-Economic Status</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Disabilities</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Familial status</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Marital status</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Nationality/National origin</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Recipient of public assistance</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Religion</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<div class="minipage">
<p><span class="citation" data-cites="Mehrabi_2021"></span></p>
<p><span class="citation" data-cites="M24_Buolamwini_2018"></span></p>
<p><span class="citation" data-cites="M142_Shankar_2017"></span></p>
<p><span class="citation" data-cites="M98_Manrai_2016"></span></p>
<p><span class="citation" data-cites="M54_Fry_2017"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M150_Vickers_2014"></span></p>
<p><span class="citation" data-cites="M30_Chen_2019"></span></p>
<p><span class="citation" data-cites="M167_Zhao_2017"></span></p>
<p><span class="citation" data-cites="M20_Bolukbasi_2016"></span></p>
<p><span class="citation" data-cites="M168_Zhao_2018"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M62_Hajian_2013"></span></p>
<p><span class="citation" data-cites="Young_2020"></span></p>
<p><span class="citation" data-cites="Montoya_2025"></span></p>
</div>
</div>
</div>
<div class="threeparttable">
<div id="tab:biases_sensitive_features">
<table>
<caption>Features which often hold biases</caption>
<tbody>
<tr>
<td style="text-align: left;"><strong>Bias-Sensitive
Features</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>ML</strong></td>
<td style="text-align: left;"><strong>Dermatology</strong></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Skin Type</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Skin Undertones</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Age</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Gender/Sex</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Gender and Skin Type Subgroups</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Ethnicity/Race</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Geographic Location</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Socio-Economic Status</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Disabilities</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Familial status</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Marital status</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Nationality/National origin</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Recipient of public assistance</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Religion</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<div class="minipage">
<p><span class="citation" data-cites="Mehrabi_2021"></span></p>
<p><span class="citation" data-cites="M150_Vickers_2014"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M30_Chen_2019"></span></p>
<p><span class="citation" data-cites="M62_Hajian_2013"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="Young_2020"></span></p>
<p><span class="citation" data-cites="Montoya_2025"></span></p>
</div>
</div>
</div>
<h2 data-number="2.3" id="fairness"><span
class="header-section-number">2.3</span> Fairness</h2>
<p>As <span class="citation" data-cites="Mehrabi_2021"></span> states,
"in order to be able to fight against discrimination and achieve
fairness, one should first define fairness". However, research is no
clear agreement for a single definition of fairness. Further, the
fairness definitions are also prone to biases since different cultures
and preferences influence the perception of fairness. In spite of those
challenges, fairness can broadly be defined as absence of bias in
decision making. While fairness is very desirable, it "can be
surprisingly difficult to achieve in practice" <span class="citation"
data-cites="Mehrabi_2021"></span>. This chapter summarizes the existing
fairness methods, which aim to achieve fairness. The fairness methods
were listed by <span class="citation"
data-cites="Mehrabi_2021"></span>.</p>
<h3 data-number="2.3.1" id="fairness-definitions"><span
class="header-section-number">2.3.1</span> Fairness Definitions</h3>
<div class="threeparttable">
<div id="tab:fairness_definitions_extensive sources">
<table>
<caption>Fairness definitions as stated in <span class="citation"
data-cites="Mehrabi_2021"></span></caption>
<tbody>
<tr>
<td style="text-align: left;"><strong>Fairness Definitions</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>ML</strong></td>
<td style="text-align: left;"><strong>Dermatology</strong></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conditional Statistical Parity</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Demographic/Statistical Parity</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Equalized Odds</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Equal Opportunity</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Treatment Equality</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Test Fairness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Subgroup Fairness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Counterfactual Fairness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fairness Through Awareness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fairness Through Unawareness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fairness in Relational Domains</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<div class="minipage">
<p><span class="citation" data-cites="Mehrabi_2021"></span></p>
<p><span class="citation" data-cites="M63_Hardt_2016"></span></p>
<p><span class="citation" data-cites="M149_Verma_2018"></span></p>
<p><span class="citation" data-cites="M48_Dwork_2012"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M87_Kusner_2017"></span></p>
<p><span class="citation" data-cites="M61_Grgic-Hlaca_2016"></span></p>
<p><span class="citation" data-cites="M15_Berk_2017"></span></p>
<p><span class="citation" data-cites="M34_Chouldechova_2017"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M50_Farnadi_2018"></span></p>
<p><span class="citation"
data-cites="M41_Corbett-Davies_2017"></span></p>
<p><span class="citation" data-cites="M79_Kearns_2018"></span></p>
<p><span class="citation" data-cites="M80_Kearns_2019"></span></p>
</div>
</div>
</div>
<div class="threeparttable">
<div id="tab:fairness_definitions">
<table>
<caption>Fairness definitions as summarized in <span class="citation"
data-cites="Mehrabi_2021"></span></caption>
<tbody>
<tr>
<td style="text-align: left;"><strong>Fairness Definitions</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>ML</strong></td>
<td style="text-align: left;"><strong>Dermatology</strong></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conditional Statistical Parity</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Demographic/Statistical Parity</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Equal Opportunity</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Treatment Equality</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Test Fairness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Equalized Odds</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Subgroup Fairness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Counterfactual Fairness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fairness Through Awareness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fairness Through Unawareness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fairness in Relational Domains</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
</div>
<p>As shown in the <a href="#tab:fairness_definitions"
data-reference-type="ref+label"
data-reference="tab:fairness_definitions">2.7</a>, there are mainly
three categories for fairness definitions. According to <span
class="citation" data-cites="Mehrabi_2021"></span>, fairness can be
achieved on a group level, subgroup level or even for a individual.
Group fairness is about treating different groups as equal. Individual
fairness tries to achieve similar predictions for similar individuals.
Subgroup fairness tries to incorporate the best properties of the other
two levels to improve the outcome in larger collections of subgroups
<span class="citation" data-cites="Mehrabi_2021"></span>.</p>
<p>The specific fairness definitions can be found in <span
class="citation" data-cites="Mehrabi_2021"></span>. In general, they try
to get similar probability outcomes for ’unprotected’ or ’protected’
groups. This list summarizes how they work:</p>
<ul>
<li><p><strong>Demographic/Statistical Parity</strong> and
<strong>Conditional Statistical Parity</strong>: The parity checks that
likelihood of a positive outcome is the same for both protected groups
<span class="citation" data-cites="Mehrabi_2021"></span>. The
conditional version adds legitimate factors before calculating the
statistical parity <span class="citation"
data-cites="M41_Corbett-Davies_2017"></span>.</p></li>
<li><p><strong>Equalized Odds</strong>, <strong>Test Fairness</strong>.
and <strong>Equal Opportunity</strong>: All those methods protected and
unprotected groups should have equal rates for a positive outcome when
belonging to a positive class, essentially comparing the true positive
rates. <strong>Equalized Odds</strong> is a more restrictive since it
also checks for similar false positive rates <span class="citation"
data-cites="M149_Verma_2018 Mehrabi_2021"></span>.</p></li>
<li><p><strong>Treatment Equality</strong>: It compares the false
negative and false positive rates <span class="citation"
data-cites="M151_Wang_2014"></span></p></li>
<li><p><strong>Counterfactual Fairness</strong>: This approach is
different from the others as it is testing the same individual in both
different demographic groups with the intention that the outcome is the
same <span class="citation"
data-cites="M87_Kusner_2017 Mehrabi_2021"></span>. It differs from to
the first group of fairness metrics since it does not compare the
likelihoods of the outcomes for any person in a group, but checking how
the exact same individual would be treated if it was in the other
group.</p></li>
<li><p><strong>Fairness Through Awareness</strong>: This method compares
similar individuals based on similarity metrics to get a similar outcome
<span class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p><strong>Fairness Through Unawareness</strong>: This measure is
ensuring that protected attributes are not explicitly used in
desicion-making <span class="citation"
data-cites="M61_Grgic-Hlaca_2016 M87_Kusner_2017"></span>.</p></li>
<li><p><strong>Fairness in Relational Domains</strong>: This notion also
takes into consideration relational structures between individuals <span
class="citation" data-cites="M50_Farnadi_2018"></span>.</p></li>
</ul>
<h3 data-number="2.3.2" id="challenges-of-fairness-definitions"><span
class="header-section-number">2.3.2</span> Challenges of Fairness
Definitions</h3>
<h2 data-number="2.4" id="mitigation-methods"><span
class="header-section-number">2.4</span> Mitigation Methods</h2>
<p>see text from bias chapter - Further, AI engineers need to know what
prevention methods are available to reduce the biases <span
class="citation" data-cites="Mehrabi_2021"></span>.</p>
<h2 data-number="2.5" id="extensive-sources"><span
class="header-section-number">2.5</span> Extensive Sources</h2>
<h3 data-number="2.5.1" id="discrimination-vs.-biases"><span
class="header-section-number">2.5.1</span> Discrimination vs.
Biases</h3>
<ul>
<li><p>bias and discrimination = source of unfairness. Discrimination
can be considered as a source for unfairness that is due to human
prejudice and stereotyping based on the sensitive attributes, which may
happen intentionally or unintentionally, while bias can be considered as
a source for unfairness that is due to the data collection, sampling,
and measurement. Although bias can also be seen as a source of
unfairness that is due to human prejudice and stereotyping, in the
algorithmic fairness literature it is more intuitive to categorize them
as such according to the existing research in these areas. In this
survey, we mainly focus on concepts that are relevant to algorithmic
fairness issues. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Explainable Discrimination. Differences in treatment and outcomes
amongst different groups can be justified and explained via some
attributes in some cases. In situations where these differences are
justified and explained, it is not considered to be illegal
discrimination and hence called explainable [77]. In [77], authors
present a methodology to quantify the explainable and illegal
discrimination in data. They argue that methods that do not take the
explainable part of the discrimination into account may result in
non-desirable outcomes, so they introduce a reverse discrimination which
is equally harmful and undesirable. They explain how to quantify and
measure discrimination in data or a classifier’s decisions which
directly considers illegal and explainable discrimination.<span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>Unexplainable Discrimination. In contrast to explainable
discrimination, there is unexplainable discrimination in which the
discrimination toward a group is unjustified and therefore considered
illegal. Authors in [77] also present local techniques for removing only
the illegal or unexplainable discrimination, allowing only for
explainable differences in decisions. These are preprocessing techniques
that change the training data such that it contains no unexplainable
discrimination. We expect classifiers trained on this preprocessed data
to not capture illegal or unexplainable discrimination. Unexplainable
discrimination consists of direct and indirect discrimination.<span
class="citation" data-cites="Mehrabi_2021"></span></p>
<ul>
<li><p>Direct Discrimination. Direct discrimination happens when
protected attributes of individuals explicitly result in non-favorable
outcomes toward them [164]. ... these traits that are considered to be
“protected” or “sensitive” attributes in computer science literature
<span class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>Indirect Discrimination. In indirect discrimination, individuals
appear to be treated based on seemingly neutral and non-protected
attributes; however, protected groups, or individuals still get to be
treated unjustly as a result of implicit effects from their protected
attributes <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
</ul></li>
<li><p>Discrimination can be either direct or indirect. Direct
discrimination occurs when decisions are made based on sensitive
attributes. Indirect discrimination occurs when decisions are made based
on nonsensitive attributes which are strongly correlated with biased
sensitive ones <span class="citation"
data-cites="M62_Hajian_2013"></span></p></li>
<li><p>ML algorithms reflect cognitive biases in humans, which can
result in predictions and decisions that are “unfair” [5]. Unfairness in
ML decision-making equates to prejudice or favoritism based on inherent
or acquired characteristics of an individual or group [6]. Within
medical research, the term bias refers to “a feature of the design of a
study, or the execution of a study, or the analysis of the data from a
study, that makes evidence misleading” [7,8]. <span class="citation"
data-cites="Montonaya_2025"></span></p></li>
</ul>
<h3 data-number="2.5.2" id="bias-introduction"><span
class="header-section-number">2.5.2</span> Bias Introduction</h3>
<ul>
<li><p>compared SAVRY, a tool used in risk assessment frameworks that
includes human intervention in its process, with automatic machine
learning methods in order to see which one is more accurate and more
fair. Conducting these types of studies should be done more frequently,
but prior to releasing the tools in order to avoid doing harm <span
class="citation" data-cites="Mehrabi_2021"></span>.</p></li>
<li><p><strong>Assessment Tools</strong> An interesting direction that
researchers have taken is introducing tools that can assess the amount
of fairness in a tool or system. For example, Aequitas [136] is a
toolkit that lets users to test models with regards to several bias and
fairness metrics for different population subgroups. Aequitas produces
reports from the obtained data that helps data scientists, machine
learning researchers, and policymakers to make conscious decisions and
avoid harm and damage toward certain populations. AI Fairness 360
(AIF360) is another toolkit developed by IBM in order to help moving
fairness research algorithms into an industrial setting and to create a
benchmark for fairness algorithms to get evaluated and an environment
for fairness researchers to share their ideas [11]. These types of
toolkits can be helpful for learners, researchers, and people working in
the industry to move towards developing fair machine learning
application away from discriminatory behavior <span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p>At first sight, automating decisions may give a sense of
fairness: classification rules do not guide themselves by personal
preferences. However, at a closer look, one realizes that classification
rules are actually learned by the system (e.g., loan granting) from the
training data. If the training data are inherently biased for or against
a particular community (e.g., foreigners), the learned model may show a
discriminatory prejudiced behavior. In other words, the system may infer
that just being foreign is a legitimate reason for loan denial. <span
class="citation" data-cites="M62_Hajian_2013"></span></p></li>
<li><p>One might think of a straightforward preprocessing approach
consisting of just removing the discriminatory attributes from the data
set. Although this would solve the direct discrimination problem, it
would cause much information loss and in general it would not solve
indirect discrimination. <span class="citation"
data-cites="M62_Hajian_2013"></span></p></li>
<li><p>Hence, there are two important challenges regarding
discrimination prevention: one challenge is to consider both direct and
indirect discrimination instead of only direct discrimination; the other
challenge is to find a good tradeoff between discrimination removal and
the quality of the resulting training data sets and data mining models.
<span class="citation" data-cites="M62_Hajian_2013"></span></p></li>
</ul>
<ul>
<li><p>Most AI systems and algorithms are data driven and require data
upon which to be trained. Thus, data is tightly coupled to the
functionality of these algorithms and systems. In the cases where the
underlying training data contains biases, the algorithms trained on them
will learn these biases and reflect them into their predictions. As a
result, existing biases in data can affect the algorithms using the
data, producing biased outcomes. Algorithms can even amplify and
perpetuate existing biases in the data.<span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p>In addition, algorithms themselves can display biased behavior
due to certain design choices, even if the data itself is not biased.
The outcomes of these biased algorithms can then be fed into real-world
systems and affect users’ decisions, which will result in more biased
data for training future algorithms.<span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
<li><p>Bias can exist in many shapes and forms, some of which can lead
to unfairness in different downstream learning tasks. In <span
class="citation" data-cites="M144_Suresh_2021"></span>, authors talk
about sources of bias in machine learning with their categorizations and
descriptions in order to motivate future solutions to each of the
sources of bias introduced in the paper. In <span class="citation"
data-cites="M120_Olteanu_2019"></span>, the authors prepare a complete
list of different types of biases with their corresponding definitions
that exist in different cycles from data origins to its collection and
its processing.<span class="citation"
data-cites="Mehrabi_2021"></span>.</p></li>
</ul>
<ul>
<li><p>The list of biases that can occur in any research is considerably
long, and certainly not all of them can be avoided. However,
dermatologists should be well aware of them because: 1. While conducting
systematic reviews and metaanalysis, PRISMA guidelines need to be
followed, and the PRISMA checklist requires a very exhaustive list of
declarations to be made by the authors as to how biases in the
individual studies were detected, their types and whether they were
included in the systematic review or not. This usually requires more
than two authors working independently. 2. Biases can result in
dramatically opposite inferences, which may not be biologically
plausible; the knowledge of the biases can help detect them and thereby
negate such findings. 3. The knowledge of biases is a vital part of the
postgraduate dermatology curriculum and is a mustknow area for
thesis/dissertation purposes. 4. Since there is no fool-proof way to
avoid all biases, the help of a well-qualified biostatistician might
help detect and prevent many of these biases in research. <span
class="citation" data-cites="Chakraborty_2024"></span></p></li>
</ul>
<p>Already rewritten: The following categorization was modeled with the
intent to show that the different biases are intertwined and one should
consider the effects between each other in the cycle to address them
correctly <span class="citation" data-cites="Mehrabi_2021"></span></p>
<h3 data-number="2.5.3" id="biases-extensive-sources"><span
class="header-section-number">2.5.3</span> Biases Extensive Sources</h3>
<h4 class="unnumbered" id="data-biases-1">Data Biases</h4>
<p>Data biases (data to algorithm (biases in data which might have an
impact on biased algorithmic outcomes <span class="citation"
data-cites="Mehrabi_2021"></span>))</p>
<ul>
<li></li>
</ul>
<h4 class="unnumbered" id="algorithmic-biases-1">Algorithmic Biases</h4>
<p>Algorithmic biases (Algorithm to user (A modulates U behaviour,
biases in algorithm might lead to introduce biases in user behaviour and
affect it as a consequence)) <span class="citation"
data-cites="Mehrabi_2021"></span></p>
<ul>
<li></li>
</ul>
<h4 class="unnumbered" id="user-biases-1">User Biases</h4>
<p>User to Data (user-generated data, inherent biases in users could be
reflected in the data they generate; biases in last section might
introduce further bias in this process) <span class="citation"
data-cites="Mehrabi_2021"></span></p>
<ul>
<li></li>
</ul>
<h4 class="unnumbered" id="dermatology-biases">Dermatology Biases</h4>
<ul>
<li><p>Equity. AI has the potential to worsen health-care disparities,
as recognized by the popular media (Khullar, 2019), particularly in
dermatology (Adamson and Smith, 2018). The first concern is adequate
representation of underserved populations in training data. Existing DL
models have been trained on mainly European or East Asian populations,
and the relative lack of training on darker skin pigmentation may limit
overall diagnostic accuracy. This possibility is demonstrated by the
increased error rates in commercial systems, trained on predominantly
white datasets, for facial analysis in identifying black individuals
(Buolamwini and Gebru, 2018). Second, AI may entrench existing social
and economic biases and perpetuate inadvertent discriminatory practices,
for example, in recommending less follow-up for black patients than for
whites, when health costs are used as a proxy for health needs
(Obermeyer et al., 2019). Third, disproportionate adoption by different
groups may exacerbate existing inequities. Access to and use of
technology differs based on sociodemographics (Tsetsi and Rains, 2017),
and more techsavvy users may be more likely to embrace AI for skin
screening (Tong and Sopory, 2019). The issue of equity in AI diagnosis
needs to be carefully addressed to avoid inadvertent exacerbation of
health-care disparities. <span class="citation"
data-cites="Young_2020"></span> - dermatology</p></li>
<li><p>Model generalizability. Generalizability is a major concern for
AI models; studies of computer-assisted diagnosis of melanoma report
lower sensitivity for melanoma on independent test sets than on
nonindependent test sets (Dick et al., 2019). It is difficult to study
generalizability because published DL models are not publicly available,
making it impossible to compare performance, unless each study uses a
standardized benchmark database, such as the Melanoma Classification
Benchmark (Brinker et al., 2019d). Han et al. (2018a) reported excellent
metrics of performance and made their model available for image
submission; however, the model prediction was not robust when images
from an outside clinic were submitted, image magnification or contrast
was altered, or images were rotated (NavarreteDechent et al., 2018). On
ImageNet, a nonmedical dataset of 1,000 object categories, training on a
dataset of 300 ... <span class="citation"
data-cites="Young_2020"></span></p></li>
</ul>
<h4 class="unnumbered" id="papers-i-would-need-access-to">Papers I would
need access to</h4>
<ul>
<li><p>https://jamanetwork.com/journals/jamadermatology/article-abstract/2688587
(linked to <span class="citation"
data-cites="Young_2020"></span>)</p></li>
<li><p>https://academic.oup.com/bjd/article-abstract/190/6/789/7603706?redirectedFrom=fulltext
Ethical considerations for artificial intelligence in dermatology: a
scoping review</p></li>
</ul>
<h3 data-number="2.5.4" id="fairness-extensive-sources"><span
class="header-section-number">2.5.4</span> Fairness Extensive
Sources</h3>
<h4 class="unnumbered" id="algorithmic-fairness">Algorithmic
Fairness</h4>
<ul>
<li><p>in order to be able to fight against discrimination and achieve
fairness, one should first define fairness. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>The fact that no universal definition of fairness exists shows
the difficulty of solving this problem [138]. Different preferences and
outlooks in different cultures lend a preference to different ways of
looking at fairness, which makes it harder to come up with just a single
definition that is acceptable to everyone in a situation. there is still
no clear agreement on which constraints are the most appropriate for
those problems. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Broadly, fairness is the absence of any prejudice or favoritism
towards an individual or a group based on their intrinsic or acquired
traits in the context of decision-making [139]. Even though fairness is
an incredibly desirable quality in society, it can be surprisingly
difficult to achieve in practice. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Here we will reiterate and provide some of the most widely used
definitions, along with their explanations inspired from <span
class="citation" data-cites="M149_Verma_2018"></span>.<span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>Definitions on page 12, 13, 14 <span class="citation"
data-cites="Mehrabi_2021"></span></p>
<ul>
<li><p>Equalized Odds (TP and FP rate should be the same for individuals
in different sub groups) <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Equal Opportunity (TP rate should be the same) <span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>Demographic Parity / Statistical Parity (likelihood of positive
outcome the same regardless of protected group) <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Fairness Through Awareness (similar predictions to similar
individuals (similarity = inverse distance)) <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Fairness Through Unawareness (no protected attributes explicitly
used in decision-making process) <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Treatment Equality (Ration FN and FP same for both protected
group categories) <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Test Fairness (for predicted probability scores, people in both
groups must have equal probability of TP) <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Counterfactual Fairness (same outcome in actual world and
counterfactual world where the individual belonged to a different
demographic group) <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Fairness in Relational Domains (“A notion of fairness that is
able to capture the relational structure in a domain—not only by taking
attributes of individuals into consideration but by taking into account
the social, organizational, and other connections between individuals”
<span class="citation" data-cites="M50_Farnadi_2018"></span>) <span
class="citation" data-cites="Mehrabi_2021"></span> probably not relevant
since not relational</p></li>
<li><p>Conditional Statistical Parity (people in both groups have equal
possibilities of being assigned to a positive outcome given a set of
legitimate factors) <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Subgroup fairness: Subgroup fairness intends to obtain the best
properties of the group and individual notions of fairness. It is
different than these notions but uses them in order to obtain better
outcomes. It picks a group fairness constraint like equalizing false
positive and asks whether this constraint holds over a large collection
of subgroups</p>
<p><span class="citation" data-cites="M79_Kearns_2018"></span><span
class="citation" data-cites="M80_Kearns_2019"></span><span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>it is impossible to satisfy some of the fairness constraints at
once except in highly constrained special cases. In [83], the authors
show the inherent incompatibility of two conditions: calibration and
balancing the positive and negative classes. These cannot be satisfied
simultaneously with each other unless under certain constraints;
therefore, it is important to take the context and application in which
fairness definitions need to be used into consideration and use them
accordingly [141]<span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Another important aspect to consider is time and temporal
analysis of the impacts that these definitions may have on individuals
or groups. In [95] authors show that current fairness definitions are
not always helpful and do not promote improvement for sensitive
groups—and can actually be harmful when analyzed over time in some
cases. They also show that measurement errors can also act in favor of
these fairness definitions; therefore, they show how temporal modeling
and measurement are important in evaluation of fairness criteria and
introduce a new range of trade-offs and challenges toward this
direction. It is also important to pay attention to the sources of bias
and their types when trying to solve fairness-related questions. <span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
</ul></li>
</ul>
<h3 data-number="2.5.5" id="mitigation-methods-overview"><span
class="header-section-number">2.5.5</span> Mitigation Methods
Overview</h3>
<div class="threeparttable">
<div id="tab:mitigation_methods_unbiasing_data">
<table>
<caption>Mitigation Methods - Unbiasing Data - Mentioned in Contextual
Research, grouped like in <span class="citation"
data-cites="Mehrabi_2021"></span>, the author cannot guarantee for
completeness</caption>
<tbody>
<tr>
<td style="text-align: left;"><strong>Mitigation Methods - Unbiasing
Data (Pre-Processing)</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>ML</strong></td>
<td style="text-align: left;"><strong>Dermatology</strong></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Good Practices while using Data</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Datasheets as supporting document for
dataset creation method, characteristics, motivations and skews</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Datasheets as supporting document for
model method, characteristics, motivations and skews</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Dataset (Nutrition) Labels</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Messaging</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Test for Simpson’s Paradox</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Detect Direct Discrimination with Causal
Models and Graphs</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Out-of-Distribution Detection in
Dermatology Using Input Perturbation and Subset Scanning</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Check confidence interval and p-curve
analysis instead of p-value</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Allocation concealment and blinding</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Preventing Direct and Indirect
Discrimination</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Data Collection from diverse sources
(incl. primary care clinics)</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Robust standards for external
validation</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Preferential Sampling</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Geographical Diversity and Inclusion for
Dataset creation</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Balanced Representation accross skin tones
and genders</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Disparate Impact Removal</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Multidimensional Scale for Skin Tones</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Publish Datasets accessible for the
public</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<div class="minipage">
<p><span class="citation" data-cites="Mehrabi_2021"></span></p>
<p><span class="citation" data-cites="M13_"></span></p>
<p><span class="citation" data-cites="M55_"></span></p>
<p><span class="citation" data-cites="M110_"></span></p>
<p><span class="citation" data-cites="M66_"></span></p>
<p><span class="citation" data-cites="M66Successor_"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M81_"></span></p>
<p><span class="citation" data-cites="M3_"></span></p>
<p><span class="citation" data-cites="M4_"></span></p>
<p><span class="citation" data-cites="M163_"></span></p>
<p><span class="citation" data-cites="M62_Hajian_2013"></span></p>
<p><span class="citation" data-cites="M74_"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M75_"></span></p>
<p><span class="citation" data-cites="M76_"></span></p>
<p><span class="citation" data-cites="M51_"></span></p>
<p><span class="citation" data-cites="M142_Shankar_2017"></span></p>
<p><span class="citation" data-cites="Chakraborty_2024"></span></p>
<p><span class="citation" data-cites="Young_2020"></span></p>
<p><span class="citation" data-cites="Montoya_2025"></span></p>
</div>
</div>
</div>
<div class="threeparttable">
<div id="tab:mitigation_methods_fair_classification">
<table>
<caption>Mitigation Methods - Fair Classification - Mentioned in
Contextual Research, grouped like in <span class="citation"
data-cites="Mehrabi_2021"></span>, the author cannot guarantee for
completeness</caption>
<tbody>
<tr>
<td style="text-align: left;"><strong>Mitigation Methods - Fair
Classification</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>ML</strong></td>
<td style="text-align: left;"><strong>Dermatology</strong></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Satisfy Subgroup Fairness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Satisfy Equality of Opportunity</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Satisfy Equalized Odds</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Disparate Treatment</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Disparate Impact</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Representation Learning by
Disentanglement</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Variational Fair Autoencoder</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">VAE without adversarial training</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Adversial Learning with FairGAN</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Removing correlation between protected and
unprotected features with a geometric solution</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Modified Discrimination-Free Naive Bayes
Classifier</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fairness-Aware Classification
Framework</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fairness Constraints in Multitask Learning
(MTL) Framework</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Decoupled Classification System with
Transfer Learning</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Wasserstein Distance Measure for
Dependence Mitigation</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Preferential Sampling (PS) for
Discrimination-Free Training Data</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Post-Processing with Attention
Mechanism</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Use Brier Score and Response Rate
Accuracy</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">some more methods</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<div class="minipage">
<p>possible to satisfy together</p>
<p>possible to satisfy together</p>
<p><span class="citation" data-cites="Mehrabi_2021"></span></p>
<p><span class="citation" data-cites="M147_"></span></p>
<p><span class="citation" data-cites="M63_Hardt_2016"></span></p>
<p><span class="citation" data-cites="M2_"></span></p>
<p><span class="citation" data-cites="M159_"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M154_"></span></p>
<p><span class="citation" data-cites="M57_"></span></p>
<p><span class="citation" data-cites="M78_"></span></p>
<p><span class="citation" data-cites="M85_"></span></p>
<p><span class="citation" data-cites="M106_"></span></p>
<p><span class="citation" data-cites="M69_"></span></p>
<p><span class="citation" data-cites="M25_"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M155_"></span></p>
<p><span class="citation" data-cites="M12_"></span></p>
<p><span class="citation" data-cites="M49_"></span></p>
<p><span class="citation" data-cites="M73_"></span></p>
<p><span class="citation" data-cites="M75_"></span></p>
<p><span class="citation" data-cites="M102_"></span></p>
<p><span class="citation" data-cites="Young_2020"></span></p>
</div>
</div>
</div>
<div class="threeparttable">
<div id="tab:mitigation_methods_others">
<table>
<caption>Mitigation Methods - Others - Mentioned in Contextual Research,
grouped like in <span class="citation"
data-cites="Mehrabi_2021"></span>, the author cannot guarantee for
completeness</caption>
<tbody>
<tr>
<td style="text-align: left;"><strong>Mitigation Methods - not so
relevant for us</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>ML</strong></td>
<td style="text-align: left;"><strong>Dermatology</strong></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fair Word-Embedding</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Train-Time Data Augmentation</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Test-Time Neutralization</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Price of Fairness (POF)</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">XY and bounded group loss</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Decision Tree for Disparate Impact and
Treatment</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Reducing Bias Amplification (RBA) as
calibration algorithm</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fair PCA</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Community Detection / Graph Embedding</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Disparate Learning Processes (DLP)</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Causal Approach to Fairness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Disregard path in causal graph which
result in sensitive attributes affecting decision outcome</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Disregard sensitive attributes in effect
on decision making</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<div class="minipage">
<p><span class="citation" data-cites="Mehrabi_2021"></span></p>
<p><span class="citation" data-cites="M42_"></span></p>
<p><span class="citation" data-cites="M97_"></span></p>
<p><span class="citation" data-cites="M112_"></span></p>
<p><span class="citation" data-cites="M20_Bolukbasi_2016"></span></p>
<p><span class="citation" data-cites="M58_"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M169_"></span></p>
<p><span class="citation" data-cites="M166_"></span></p>
<p><span class="citation" data-cites="M94_"></span></p>
<p><span class="citation" data-cites="M14_"></span></p>
<p><span class="citation" data-cites="M1_"></span></p>
<p><span class="citation" data-cites="M2_"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M167_Zhao_2017"></span></p>
<p><span class="citation" data-cites="M137_"></span></p>
<p><span class="citation" data-cites="M5_"></span></p>
<p><span class="citation" data-cites="M90_"></span></p>
<p><span class="citation" data-cites="M65_"></span></p>
</div>
</div>
</div>
<p>satisfy Equalized Odds / Subgroup fairness highlight allocation
concealment and blinding and data collection from diverse sources and
Preferential Sampling</p>
<h3 data-number="2.5.6" id="mitigation-methods-overview-1"><span
class="header-section-number">2.5.6</span> Mitigation Methods
Overview</h3>
<div class="threeparttable">
<div id="tab:mitigation_methods_unbiasing_data_praesi">
<table>
<caption>Mitigation Methods - Draft</caption>
<tbody>
<tr>
<td style="text-align: left;"><strong>Mitigation Methods</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>ML</strong></td>
<td style="text-align: left;"><strong>Dermatology</strong></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Documentation and Transparency</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Bias Detection and Evaluation</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Study Design</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Data Gathering</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Data Availability and Open Science</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;">Removing Sensitive Attributes</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Satisfy Fairness Definitions</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Satisfy Fairness and Stability Under
Distribution Shifts</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fair Representation Learning</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fairness-Aware ML Frameworks</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Preferential Data Selection and
Representation</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Model Interpretability</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">X</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fair NLP</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fair Regression</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Structured Prediction</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fair Principal Component Analysis</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Graph-Based Fairness Methods</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Causal Fairness and Disparate
Learning</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<div class="minipage">
<p><span class="citation" data-cites="Mehrabi_2021"></span></p>
<p><span class="citation" data-cites="Chakraborty_2024"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="Young_2020"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="Montoya_2025"></span></p>
</div>
</div>
</div>
<div class="threeparttable">
<div id="tab:mitigation_methods_others">
<table>
<caption>Mitigation Methods - Others - Mentioned in Contextual Research,
grouped like in <span class="citation"
data-cites="Mehrabi_2021"></span>, the author cannot guarantee for
completeness</caption>
<tbody>
<tr>
<td style="text-align: left;"><strong>Mitigation Methods - not so
relevant for us</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>ML</strong></td>
<td style="text-align: left;"><strong>Dermatology</strong></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fair Word-Embedding</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Train-Time Data Augmentation</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Test-Time Neutralization</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Price of Fairness (POF)</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">XY and bounded group loss</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Decision Tree for Disparate Impact and
Treatment</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Reducing Bias Amplification (RBA) as
calibration algorithm</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fair PCA</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Community Detection / Graph Embedding</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Disparate Learning Processes (DLP)</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Causal Approach to Fairness</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Disregard path in causal graph which
result in sensitive attributes affecting decision outcome</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Disregard sensitive attributes in effect
on decision making</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<div class="minipage">
<p><span class="citation" data-cites="Mehrabi_2021"></span></p>
<p><span class="citation" data-cites="M42_"></span></p>
<p><span class="citation" data-cites="M97_"></span></p>
<p><span class="citation" data-cites="M112_"></span></p>
<p><span class="citation" data-cites="M20_Bolukbasi_2016"></span></p>
<p><span class="citation" data-cites="M58_"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M169_"></span></p>
<p><span class="citation" data-cites="M166_"></span></p>
<p><span class="citation" data-cites="M94_"></span></p>
<p><span class="citation" data-cites="M14_"></span></p>
<p><span class="citation" data-cites="M1_"></span></p>
<p><span class="citation" data-cites="M2_"></span></p>
</div>
<div class="minipage">
<p><span class="citation" data-cites="M167_Zhao_2017"></span></p>
<p><span class="citation" data-cites="M137_"></span></p>
<p><span class="citation" data-cites="M5_"></span></p>
<p><span class="citation" data-cites="M90_"></span></p>
<p><span class="citation" data-cites="M65_"></span></p>
</div>
</div>
</div>
<h3 data-number="2.5.7" id="mitigation-methods-extensive-sources"><span
class="header-section-number">2.5.7</span> Mitigation Methods Extensive
Sources</h3>
<h4 class="unnumbered" id="bias-examples-and-mitigation-ideas">Bias
Examples and Mitigation Ideas</h4>
<p>Data bias examples and mitigation ideas</p>
<ul>
<li><p>Bias in ML Data - <span class="citation"
data-cites="M24_Buolamwini_2018"></span> IJB-A / Adience imbalanced
(mainly light-skinned subjects) - Bias towards dark-skinned groups
(underrepresented). Other instance - when we do not consider different
subgroups in the data. Considering only male-female groups not enough,
use race to further subdivide gender groups. Only then, clear biases in
sub groups can be found, since otherwise part of the groups would
compromise the other group and hide the underlaying bias towards that
subgroup <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Popular machine-learning datasets that serve as a base for most
of the developed algorithms and tools can also be biased—which can be
harmful to the downstream applications that are based on these datasets.
... In [<span class="citation" data-cites="M142_Shankar_2017"></span>,
researchers showed that these datasets suffer from representation bias
and advocate for the need to incorporate geographic diversity and
inclusion while creating such datasets. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Examples of Data Bias in Medical Applications. These data biases
can be more dangerous in other sensitive applications. For example, in
medical domains there are many instances in which the data studied and
used are skewed toward certain populations—which can have dangerous
consequences for the underrepresented communities. [98] showed how
exclusion of African-Americans resulted in their misclassification in
clinical studies, so they became advocates for sequencing the genomes of
diverse populations in the data to prevent harm to underrepresented
populations <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
</ul>
<h4 class="unnumbered" id="methods-for-fair-machine-learning">Methods
for Fair Machine Learning</h4>
<ul>
<li><p>While this section is largely domain-specific, it can be useful
to take a cross-domain view. Generally, methods that target biases in
the algorithms fall under three categories <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Pre-processing. Pre-processing techniques try to transform the
data so that the underlying discrimination is removed [43]. If the
algorithm is allowed to modify the training data, then pre-processing
can be used [11].<span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>In-processing. In-processing techniques try to modify and change
state-of-the-art learning algorithms in order to remove discrimination
during the model training process [43]. If it is allowed to change the
learning procedure for a machine learning model, then in-processing can
be used during the training of a model— either by incorporating changes
into the objective function or imposing a constraint [11, 14].<span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>Post-processing. Post-processing is performed after training by
accessing a holdout set which was not involved during the training of
the model [43]. If the algorithm can only treat the learned model as a
black box without any ability to modify the training data or learning
algorithm, then only post-processing can be used in which the labels
assigned by the black-box model initially get reassigned based on a
function during the post-processing phase [11, 14].<span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>we concentrate on discrimination prevention based on
preprocessing, because the preprocessing approach seems the most
flexible one: it does not require changing the standard data mining
algorithms, unlike the inprocessing approach, and it allows data
publishing (rather than just knowledge publishing), unlike the
postprocessing approach. <span class="citation"
data-cites="M62_Hajian_2013"></span> –&gt;</p></li>
<li><p>From learning fair representations [42, 97, 112] to learning fair
word embeddings [<span class="citation"
data-cites="M20_Bolukbasi_2016"></span>, 58, 169], debiasing methods
have been proposed in different AI applications and domains. <span
class="citation" data-cites="Mehrabi_2021"></span> –&gt; seems to refer
mostly to NLP domains</p></li>
<li><p>Most of these methods try to avoid unethical interference of
sensitive or protected attributes into the decision-making process,
while others target exclusion bias by trying to include users from
sensitive groups. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>However, a recent paper [58] argues against these debiasing
techniques and states that many recent works on debiasing word
embeddings have been superficial, that those techniques just hide the
bias and don’t actually remove it. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>some works try to satisfy one or more of the fairness notions in
their methods, such as disparate learning processes (DLPs) which try to
satisfy notions of treatment disparity and impact disparity by allowing
the protected attributes during the training phase but avoiding them
during prediction time [94].<span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Some of the existing work tries to treat sensitive attributes as
noise to disregard their effect on decision-making, while some causal
methods use causal graphs, and disregard some paths in the causal graph
that result in sensitive attributes affecting the outcome of the
decision.<span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Different bias-mitigating methods and techniques are discussed
below for different domains—each targeting a different problem in
different areas of machine learning in detail. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
</ul>
<h6 data-number="2.5.7.0.0.1" id="unbiasing-data"><span
class="header-section-number">2.5.7.0.0.1</span> Unbiasing Data</h6>
<ul>
<li><p>Every dataset is the result of several design decisions made by
the data curator. Those decisions have consequences for the fairness of
the resulting dataset, which in turn affects the resulting algorithms.
In order to mitigate the effects of bias in data, some general methods
have been proposed that advocate having good practices while using data,
such as having datasheets that would act like a supporting document for
the data reporting the dataset creation method, its characteristics,
motivations, and its skews [13, 55]. A similar suggestion has been
proposed for models in [110].<span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Authors in [66] also propose having labels, just like nutrition
labels on food, in order to better categorize each data for each task.
<span class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>some work has targeted more specific types of biases. For
example, [81] has proposed methods to test for cases of Simpson’s
paradox in the data, and [3, 4] proposed methods to discover Simpson’s
paradoxes in data automatically. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Causal models and graphs were also used in some work to detect
direct discrimination in the data along with its prevention technique
that modifies the data such that the predictions would be absent from
direct discrimination [163].<span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>in [<span class="citation" data-cites="M62_Hajian_2013"></span>]
also worked on preventing discrimination in data mining, targeting
direct, indirect, and simultaneous effects.<span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Other pre-processing approaches, such as messaging [74],
preferential sampling [75, 76], disparate impact removal [51], also aim
to remove biases from the data. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Image quality. Several barriers to AI implementation in the
clinic need to be overcome with regards to imaging (Figure 1). These
include technical variations (e.g., camera hardware and software) and
differences in image acquisition and quality (e.g., zoom level, focus,
lighting, and presence of hair). For example, the presence of surgical
ink markings is associated with decreased specificity (Winkler et al.,
2019), field of view can significantly affect prediction quality (Mishra
et al., 2019), and classification performance improves when hair and
rulers are removed (Bisla et al., 2019). We have developed a method to
measure how model predictions might be biased by the presence of a
visual artifact (e.g., ink) and proposed methods to reduce such biases
(Pfau et al., 2019). Poor quality images are often excluded from
studies, but the problem of what makes an image adequate is not well
studied. Ideally, models need to be able to express a level of
confidence in a prediction as a function of image quality and
appropriately direct a user to retake photos if needed. <span
class="citation" data-cites="Young_2020"></span> - dermatology</p></li>
</ul>
<h6 data-number="2.5.7.0.0.2" id="fair-classification"><span
class="header-section-number">2.5.7.0.0.2</span> Fair
Classification</h6>
<ul>
<li><p>certain methods have been proposed [57, 78, 85, 106] that satisfy
certain definitions of fairness in classification. For instance, in
[147] authors try to satisfy subgroup fairness in classification,
equality of opportunity and equalized odds in [63], both disparate
treatment and disparate impact in [2, 159], and equalized odds in [154].
<span class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>Other methods try to not only satisfy some fairness constraints
but to also be stable toward change in the test set [69] <span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>The authors in [155], propose a general framework for learning
fair classifiers. This framework can be used for formulating
fairness-aware classification with fairness guarantees. In another work
[25], authors propose three different modifications to the existing
Naive Bayes classifier for discrimination-free classification.<span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>paper [122] takes a new approach into fair classification by
imposing fairness constraints into a Multitask learning (MTL) framework.
In addition to imposing fairness during training, this approach can
benefit the minority groups by focusing on maximizing the average
accuracy of each group as opposed to maximizing the accuracy as a whole
without attention to accuracy across different groups. In a similar work
[49], authors propose a decoupled classification system where a separate
classifier is learned for each group. They use transfer learning to
reduce the issue of having less data for minority groups.<span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>In [73] authors propose to achieve fair classification by
mitigating the dependence of the classification outcome on the sensitive
attributes by utilizing the Wasserstein distance measure.<span
class="citation" data-cites="Mehrabi_2021"></span></p></li>
<li><p>In [75] authors propose the Preferential Sampling (PS) method to
create a discrimination free train data set. They then learn a
classifier on this discrimination free dataset to have a classifier with
no discrimination.<span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>In [102], authors propose a post-processing bias mitigation
strategy that utilizes attention mechanism for classification and that
can provide interpretability. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
</ul>
<h6 data-number="2.5.7.0.0.3" id="fair-regression"><span
class="header-section-number">2.5.7.0.0.3</span> Fair Regression</h6>
<ul>
<li><p>“price of fairness” (POF) to measure accuracy-fairness
trade-offs, 3 penalites: Individual fairness, group fairness and hybrid
fairness [14] <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>In addition to the previous work, [1] considers the fair
regression problem formulation with regards to two notions of fairness
statistical (demographic) parity and bounded group loss. [2] uses
decision trees to satisfy disparate impact and treatment in regression
tasks in addition to classification. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
</ul>
<h6 data-number="2.5.7.0.0.4" id="structured-prediction"><span
class="header-section-number">2.5.7.0.0.4</span> Structured
Prediction</h6>
<ul>
<li><p>RBA (reducing bias amplification) as calibration algorithm to
prevent risk of leveraging social bias, distributions in training data
are followed in the predictions. multi-label obeject and visual semantic
role labeling classification amplify existing bias in data [<span
class="citation" data-cites="M167_Zhao_2017"></span>] <span
class="citation" data-cites="Mehrabi_2021"></span> –&gt;</p></li>
</ul>
<h6 data-number="2.5.7.0.0.5" id="fair-pca"><span
class="header-section-number">2.5.7.0.0.5</span> Fair PCA</h6>
<ul>
<li><p>Pincipal Component Analysis (PCA)
https://www.geeksforgeeks.org/principal-component-analysis-pca/ –&gt;
dimensionality reduction, statistical technic, high-dimensional data
into lower-dimensional space while maximising variance in new space
-&gt; most important patterns and relationships is preserved</p></li>
<li><p>vanilla PCA exaggerate error in reconstruction for one group of
people [137] <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>And their proposed algorithm is a two-step process listed below:
(1) Relax the Fair PCA objective to a semidefinite program (SDP) and
solve it. (2) Solve a linear program that would reduce the rank of the
solution. [137] <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
</ul>
<h6 data-number="2.5.7.0.0.6" id="community-detection"><span
class="header-section-number">2.5.7.0.0.6</span> Community
Detection</h6>
<p>Community detection algorithms are specifically tailored to analyze
network data and find connections in such datasets. For example, they
can be used to detect groups of people with similar interest in social
networks <span class="citation" data-cites="Jayawickrama_2021"></span>.
This kind of data is not found in the context of PASSION, which is a
classification task. Please refer to <span class="citation"
data-cites="Mehrabi_2021"></span> for more information on bias
mitigation in community detection algorithms.</p>
<h6 data-number="2.5.7.0.0.7" id="causal-approach-to-fairness"><span
class="header-section-number">2.5.7.0.0.7</span> Causal Approach to
Fairness</h6>
<h6 data-number="2.5.7.0.0.8" id="fair-representation-learning"><span
class="header-section-number">2.5.7.0.0.8</span> Fair Representation
Learning</h6>
<p>https://medium.com/superlinear-eu-blog/representation-learning-breakthroughs-what-is-representation-learning-5dda2e2fed2e</p>
<ul>
<li><p>Variational Auto encoders –&gt; Variational Fair Autoencoder
introduced in [97]. Here,they treat the sensitive variable as the
nuisance variable, so that by removing the information about this
variable they will get a fair representation. They use a maximum mean
discrepancy regularizer to obtain invariance in the posterior
distribution over latent variables. Adding this maximum mean discrepancy
(MMD) penalty into the lower bound of their VAE architecture satisfies
their proposed model for having the Variational Fair Autoencoder.<br />
In [5] authors also propose a debiased VAE architecture called DB-VAE
which learns sensitive latent variables that can bias the model (e.g.,
skin tone, gender, etc.) and propose an algorithm on top of this DB-VAE
using these latent variables to debias systems like facial detection
systems.<br />
In [112] authors model their representation-learning task as an
optimization objective that would minimize the loss of the mutual
information between the encoding and the sensitive variable. The relaxed
version of this assumption is shown in Equation 1. They use this in
order to learn fair representation and show that adversarial training is
unnecessary and in some cases even counter-productive.<br />
In [42], authors introduce flexibly fair representation learning by
disentanglement that disentangles information from multiple sensitive
attributes. Their flexible and fair variational autoencoder is not only
flexible with respect to downstream task labels but also flexible with
respect to sensitive attributes. They address the demographic parity
notion of fairness, which can target multiple sensitive attributes or
any subset combination of them. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
<li><p>Adversarial Learning - In [90] authors present a framework to
mitigate bias in models learned from data with stereotypical
associations. using adversarial networks by introducing FairGAN which
generates synthetic data that is free from discrimination and is similar
to the real data. They use their newly generated synthetic data from
FairGAN, which is now debiased, instead of the real data for training
and testing. They do not try to remove discrimination from the dataset,
unlike many of the existing approaches, but instead generate new
datasets similar to the real one which is debiased and preserves good
data utility. <span class="citation"
data-cites="Mehrabi_2021"></span></p></li>
</ul>
<h6 data-number="2.5.7.0.0.9" id="fair-nlp"><span
class="header-section-number">2.5.7.0.0.9</span> Fair NLP</h6>
<ul>
<li><p>Word Embedding</p></li>
<li><p>Coreference Resolution "Coreference resolution involves
identifying when two or more expressions in a text refer to the same
entity, be it a person, place, or thing."
https://medium.com/@datailm/the-key-to-unlocking-true-language-understanding-coreference-resolution-c01d569e2e87</p></li>
</ul>
<h4 class="unnumbered"
id="comparison-of-different-mitigation-algorithms">comparison of
different mitigation algorithms</h4>
<ul>
<li><p>The field of algorithmic fairness is a relatively new area of
research and work still needs to be done for its improvement. With that
being said, there are already papers that propose fair AI algorithms and
bias mitigation techniques and compare different mitigation algorithms
using different benchmark datasets in the fairness domain. For instance,
authors in [65] propose a geometric solution to learn fair
representations that removes correlation between protected and
unprotected features. The proposed approach can control the trade-off
between fairness and accuracy via an adjustable parameter. In this work,
authors evaluate the performance of their approach on different
benchmark datasets, such as COMPAS, Adult and German, and compare them
against various different approaches for fair learning algorithms
considering fairness and accuracy measures [65, 72, 158, 159]. In
addition, IBM’s AI Fairness 360 (AIF360) toolkit [11] has implemented
many of the current fair learning algorithms and has demonstrated some
of the results as demos which can be utilized by interested users to
compare different methods with regards to different fairness measures.
<span class="citation" data-cites="Mehrabi_2021"></span></p></li>
</ul>
<h3 data-number="2.5.8" id="statistical-biases"><span
class="header-section-number">2.5.8</span> Statistical biases</h3>
<p>https://data36.com/statistical-bias-types-explained/</p>
<ul>
<li></li>
</ul>
<h3 data-number="2.5.9" id="dermatology-bias"><span
class="header-section-number">2.5.9</span> Dermatology Bias</h3>
<ul>
<li><p>https://ijdvl.com/biases-in-dermatology-a-primer/ 29 biases, 4
reasons to know about it, 7 mitigation methods <span class="citation"
data-cites="Chakraborty_2024"></span> - dermatology</p></li>
<li><p>A recent study reported mean top-1 and top-5 model accuracy of
44.8% and 78.1%, respectively, for the classification of 134 diseases
(Han et al., 2019b). Most datasets are proprietary, often with minimal
description, and datasets collected in dermatology clinics may be skewed
toward more complex cases, to those patients with better access to care,
or by the choice of camera used in one clinic versus another. Data
should be collected from as many diverse sources as possible, including
primary care clinics, and robust standards for external validation are
needed. <span class="citation" data-cites="Young_2020"></span></p></li>
<li><p>There have been successful efforts to support reproducibility and
open access. For example, the study by Han et al. (2018a) details the
number and characteristics of images from each data source and makes
thumbnails of the images publicly available. Additionally, several
studies classifying dermoscopic images use the publicly available
International Skin Imaging Collaboration archive (Gutman et al., 2016).
By making datasets public, it becomes possible to examine them for bias
(Bissoto et al., 2019). Alternatively, reporting a model training
database’s patient demographics and disease classes would be helpful in
predicting model performance on external populations. <span
class="citation" data-cites="Young_2020"></span></p></li>
<li><p>Metrics of model performance. Standard metrics are needed to
assess the performance of different models (Figure 1). Currently,
standard performance metrics such as accuracy and area under the
receiver operating characteristic and precision recall curves are
routinely reported. However, for use in the clinic, studies should
additionally describe how well their models deal with uncertainty by
reporting (i) the Brier Score, or mean-squared calibration error
(Rufibach, 2010), which measures how reliably a model can forecast its
accuracy, and (ii) area under the response rate accuracy curve, which
measures how capably a model can identify examples it is likely to
predict falsely and thus abstain from predicting (Hendrycks et al.,
2019) <span class="citation" data-cites="Young_2020"></span></p></li>
<li><p>Model interpretability. Acceptance of AI in clinical decision
making hinges on being able to understand the decisionmaking process
fundamental to its predictions. DL models are inherently difficult to
interpret because they are complex, routinely containing millions of
learned parameters; interpretation of DL models’ output is an active
field of research (Murdoch et al., 2019). One approach for interpreting
model diagnoses is contentbased image retrieval, a method for retrieving
training images that are visually similar to a test image (Tschandl et
al., 2019a). This method may reassure the physician if all the retrieved
training images have the same diagnosis as the predicted diagnosis but
is less helpful if the test image looks similar to two or more training
images with conflicting diagnoses. A second approach is to highlight
pixels in an image most relevant for a model’s prediction, using methods
such as saliency mapping (Figure 1). However, it is often the case that
highlighted pixels correspond to the entire lesion or visually
distinctive features that are already obvious to clinicians without
indication as to why these pixels are important to the diagnosis. A
third approach is to see through the eyes of a model by plotting an
activation atlas (Carter et al., 2019), which shows how subtle changes,
in particular visual features, may tip the model over into choosing one
diagnosis over another. These activation atlases are experimental and
have yet to be applied in dermatology. Understanding a model’s
predictions and how the prediction is applicable to the patient at hand
is necessary to build trust. As AI exceeds human performance in various
tasks, interpreting models may help to advance scientific knowledge by
understanding what the machine sees that is relevant to its predications
<span class="citation" data-cites="Young_2020"></span></p></li>
</ul>
<h4 data-number="2.5.9.1" id="demographic-bias-in-dermatology"><span
class="header-section-number">2.5.9.1</span> Demographic Bias in
Dermatology</h4>
<h4 class="unnumbered" id="fairness-melanoma-detection">fairness
melanoma detection</h4>
<ul>
<li><p>Some biases can be easily detected and countered, such as through
appropriate data curation; for example, having a balanced representation
across skin tones and genders in training sets. However, in other cases,
biases are hidden and untraceable [9]. <span class="citation"
data-cites="Montoya_2025"></span></p></li>
<li><p>whether information on demographic diversity (age, gender, race,
or ethnicity of patients), clinical diversity (skin type, lesion type,
anatomical location of lesion), or image characteristics (source,
imaging techniques, resolution, and whether the images were real or
artificially generated) <span class="citation"
data-cites="Montoya_2025"></span></p></li>
<li><p>The most popular skin color scale currently being used for data
annotation for image recognition techniques is the Fitzpatrick Skin Tone
Scale (FST) [10]which has six skin tones. Dating from the 1970s, it
originally featured just 4 light tones and was designed for detecting
photo sensitivity for white skin, with two darker tones added later
[11]. The Monk Skin Scale was recently developed and still needs
testing, but promisingly has 10 tones, 5 light and 5 dark [12].<span
class="citation" data-cites="Montoya_2025"></span></p></li>
<li><p>Fig. 4. Comparison of skin tone scales that can be used for skin
cancer detection utilizing AI. Recreation of fitzpatrick skin type
scale, monk skin tone scale, and sampling of L’Oreal color chart map for
reference. <span class="citation"
data-cites="Montoya_2025"></span></p></li>
<li><p>While this systemic review provides a comprehensive review of the
literature on fairness in AI for melanoma detection, it is primarily
based on existing research. To validate the proposed recommendations or
frameworks, continuing work is necessary to complete empirical analysis
and experiments. Additionally, the suggested adoption of new skin tone
scales, while beneficial, may face practical challenges in
implementation. Furthermore, while the paper strongly advocates for
specific skin tone scales, it’s important to note that other methods or
tools might also effectively address fairness issues in AI for melanoma
detection. Finally, while the study addresses fairness in AI, it could
benefit from further exploration of the practical implementation of
these recommendations in real-world clinical settings. Potential
obstacles and the feasibility of widespread adoption should be
considered to ensure that the proposed solutions are not only
theoretically sound but also practically viable. <span class="citation"
data-cites="Montoya_2025"></span></p></li>
<li><p>Recent research [13] adds another axis, skin hue, which is
described as ranging from red to yellow. This offers a more complete
representation of variations of skin color by providing a
multidimensional scale [13]. <span class="citation"
data-cites="Montoya_2025"></span></p></li>
<li><p>The effect of hue (blue, red, yellow, green) on skin tones adds
depth to each face producing a range of undertones (cold, neutral, warm,
and olive). In the realm of color theory, the concept of ‘contrast of
hue’ emphasizes the distinctiveness among fundamental colors, with
primary hues like yellow, red, and blue exhibiting the most pronounced
differences [14]. Because skin cancer appears differently on different
colored skin, it is important to acknowledge a full range of colors
present in both healthy skin and suspicious lesions within datasets used
to train skin cancer detection ML tools. <span class="citation"
data-cites="Montoya_2025"></span></p></li>
<li><p>These findings should correlate to AI for melanoma detection
since the contrast between skin color and skin lesions is a preliminary
marker during feature extraction. Although the Fitzpatrick Skin Tone
(FST) measurement scale is not diverse enough and leads to biased AI
tools, it is continually used and has even been used to test a recently
FDA-approved AI device for detecting melanoma. <span class="citation"
data-cites="Montoya_2025"></span></p></li>
<li><p>We advocate for the adoption of improved scales like the Monk and
L’Oreal maps. Future studies should ensure equitable representation and
testing across skin tones to guarantee AI’s effectiveness for all.
Please refer to Tables 2 through 7 in the discussion section for further
recommendations for curating a diverse dataset, including purpose,
ownership, funding, and data annotation, as well as recommendations for
each stage of the data life cycle. <span class="citation"
data-cites="Montoya_2025"></span></p></li>
<li><p>This study found that while using skin tone instead of race for
fairness evaluations in computer vision seems objective, the annotation
process remains biased by human annotators. Untested scales, unclear
procedures, and a lack of awareness about annotator backgrounds and
social context significantly influence skin tone labeling. This study
exposes how even minor design choices in the annotation process, like
scale order (dark to light instead of light to dark) or image context
(face or no face, skin lesion presence), can sway agreement and
introduce uncertainty in skin tone assessments. ... The researchers
emphasize the need for greater transparency, standardized procedures,
and careful consideration of annotator biases to mitigate these
challenges and ensure fairer and more robust evaluations in computer
vision. <span class="citation" data-cites="Montoya_2025"></span> -
demographic dermatology bias</p></li>
</ul>
<h1 data-number="3" id="ideas-and-concepts"><span
class="header-section-number">3</span> Ideas and Concepts</h1>
<p><span style="color: blue">Hier geht es um die Fragestellung, wie Sie
die formulierten Ziele der Arbeit erreichen wollen. Sie halten z.B.
erste, grobe Ideen, skizzenhafte Lösungsansätze fest. Gibt es mehrere
Wege, Ansätze um dieses Ziel zu erreichen, begründen Sie hier, warum Sie
einen bestimmten Weg einschlagen. Beispiel für ein Softwareprojekt:
Erste Gedanken über eine grobe Systemarchitektur. Ist z.B. eine
Microservice-Architektur angebracht? Welche Alternativen bestehen, wo
gibt es Problempunkte? Die Umsetzung, die Beurteilung der Machbarkeit
und die detaillierte Beschreibung der umgesetzten Architektur sind dann
Teil der Realisierung.</span></p>
<h2 data-number="3.1" id="passion-dataset-1"><span
class="header-section-number">3.1</span> PASSION Dataset</h2>
<ul>
<li><p>Include more details in gender attribute - transgender have
probably different genes / hormones, and should be indicated for more
accurraccy</p></li>
<li><p>include profession / at least an adapted version to indicate high
risk patients for certain diseases? -&gt; might lead to other
biases?</p></li>
<li><p>change country of origin to ethnicity (less of a proxy
variable)</p></li>
<li><p>are the data collectors specialized in some fields? That could
lead to bias towards the center’s country and the diagnosed
diseases</p></li>
<li><p>include images of healthy skin</p></li>
</ul>
<h2 data-number="3.2" id="broad-methodology"><span
class="header-section-number">3.2</span> Broad Methodology</h2>
<ul>
<li><p>Divide and Conquer vs. All-In-One-Model</p>
<ul>
<li><p>An algorithm per ethnicity / subgroup running at the same
time</p></li>
<li><p>Running 1 Algorithm chosen based on Fitzpatrick skin
type</p></li>
<li><p>Running 1 Algorithm which detects first the demographic subgroup
(<span data-acronym-label="FST"
data-acronym-form="singular+short">FST</span>, gender, age, …) and runs
the specific subgroup algorithm afterwards</p></li>
<li><p>Hint Ludovic: Still not of data, maybe also others; often limited
because the data is missing, you are missing data from others</p></li>
</ul></li>
<li><p>BLIND performance vs. Including the demographic data</p>
<ul>
<li><p>Idea to try if the labels are not relevant for the diagnosis and
should only be used for evaluating fairness purposes as some papers
suggest</p></li>
<li><p>Might be obsolete after demographic biases in dermatology
research, since melanin response and melanoma risk is different in male
and female according to research
https://pmc.ncbi.nlm.nih.gov/articles/PMC4797181/</p></li>
</ul></li>
<li><p>Hint Ludovic: Maybe Focal Loss more relevant –&gt; emphasis on
data vs model</p></li>
</ul>
<ul>
<li><p>Divide and Conquer vs. All-In-One-Model (either by etnicity x
algorithms at a time or one which seperates the imgs first by
demographic subgroup (incl. Fitzpatrick skin type))</p></li>
<li><p>BLIND performance vs. Including the demographic data</p></li>
</ul>
<h1 data-number="4" id="methods"><span
class="header-section-number">4</span> Methods</h1>
<p><span style="color: blue">Hier halten Sie fest und begründen, welches
Vorgehensmodell Sie für Ihr Projekt wählen. Sie verweisen allenfalls auf
die daraus entstandenen, konkreten Terminpläne mit Meilensteinen, welche
z.B. unter Realisierung (Kapitel 5) oder im Anhang versorgt sind. Bei
Projekten mit einer verlangten wissenschaftlichen Tiefe werden hier die
geplanten Forschungsmethoden wie quantitative/qualitative Interviews,
Befragungen, Beobachtungen, Feldexperiment etc. beschrieben und
begründet. Warum ist in Ihrer Situation ein Interview besser als eine
Umfrage? Wer soll interview werden?</span> <span style="color: blue">Die
gewählten Methoden sind nachvollziehbar und begründet. Eine methodische
Übersicht (Methodisches BigPicture) wurde aufgezeigt und Abgrenzungen
erläutert.</span></p>
<h1 data-number="5" id="execution"><span
class="header-section-number">5</span> Execution</h1>
<p><span style="color: blue">Dies ist das Hauptkapitel Ihrer Arbeit!
Hier wird die Umsetzung der eigenen Ideen und Konzepte (Kapitel 3)
anhand der gewählten Methoden (Kapitel 4) beschrieben, inkl. der dabei
aufgetretenen Schwierigkeiten und Einschränkungen.</span> <span
style="color: blue">Die gewählten Methoden werden systematisch,
konsistent und korrekt auf den Kontext der Arbeit angewendet. Die
Bearbeitungs- bzw. Forschungsobjekte sind einheitlich benannt, im
Kontext dargestellt und sinnvoll in die Arbeit integriert. Praxis- und
Erfahrungswissen (z.B. aus Interviews) wird zur Validierung und
Ergänzung der erarbeiteten Ergebnisse herangezogen. </span></p>
<h1 data-number="6" id="evaluation-and-validation"><span
class="header-section-number">6</span> Evaluation and Validation</h1>
<p><span style="color: blue">Auswertung und Interpretation der
Ergebnisse. Nachweis, dass die Ziele erreicht wurden, oder warum welche
nicht erreicht wurden.</span> <span style="color: blue">Die Ziele /
Forschungsfragen sind dem Umfang der Arbeit entsprechend sehr klar
abgegrenzt; sie sind präzise, überprüfbar und nach den Standards der
Zielformulierung definiert. Die Zielerreichung wurde systematisch und
korrekt validiert.</span> <span style="color: blue">Die Herleitung und
Bedeutung der Ergebnisse, mögliche Varianten, Gütekriterien und eine
Validierung allgemein werden nachvollziehbar diskutiert</span></p>
<h1 data-number="7" id="outlook"><span
class="header-section-number">7</span> Outlook</h1>
<p><span style="color: blue">Reflexion der eigenen Arbeit, ungelöste
Probleme, weitere Ideen.</span> <span style="color: blue">Die Ergebnisse
und Empfehlungen schaffen einen konkreten Mehrwert für die
Auftraggebenden. Einschränkungen und Grenzen werden kritisch diskutiert
und die nächsten Schritte im Ausblick festgehalten, so dass die
Ergebnisse direkt in der Praxis weiterverwendet und/oder angewendet
werden können.</span></p>
<h1 data-number="8" id="glossary"><span
class="header-section-number">8</span> Glossary</h1>
<h1 data-number="9" id="bibliography"><span
class="header-section-number">9</span> Bibliography</h1>
<p><span style="color: blue">Projektspezifisch können weitere
Dokumentationsteile angefügt werden wie: Aufgabenstellung,
Projektmanagement-Plan/Bericht, Testplan/Testbericht,
Bedienungsanleitungen, Details zu Umfragen, detaillierte
Anforderungslisten, Referenzen auf projektspezifische Daten in externen
Entwicklungs- und Datenverwaltungstools etc.</span></p>
