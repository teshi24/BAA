\documentclass[12pt, a4paper, oneside]{book}   	% document style definition


\usepackage{hslu}                               % apply HSLU style
\usepackage{comment}                            % having comment sections \begin{comment} \end{comment}
\usepackage[utf8]{inputenc}						% charactere interpretation
\usepackage{amsmath}							% math package
\usepackage{amsfonts}							% font package for math symbols
\usepackage{amssymb}							% symbols package - definition of math symbols
\usepackage{listings}							% package for code representation

\usepackage{csquotes}       % Quotation support
\usepackage[style=apa,backend=biber]{biblatex}       % Bibliography, 
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{references.bib} % Bibliography file

\usepackage{graphicx}							% for inclusion of image
\setlength {\marginparwidth }{2cm}
\usepackage{todonotes}
\renewcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
%\presetkeys{todonotes}{inline, textcolor=red, color=none, noinlinepar}{}

%\let\todoold\todo
%\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\todo[prepend, caption={TODO: #1}]{}}

%\renewcommand{\todo}[1]{\todo[inline]{\textcolor{red}{TODO: #1}}}
%\renewcommand{\todo}[1]{\todo{\textcolor{red}{TODO: #1}}}




\usepackage{booktabs}       % Better tables
\usepackage{caption}        % Better captions
\usepackage{subfig}								% to arrange figures next to each other
\usepackage{float}								% text style surrounding images
\usepackage{threeparttable}
\usepackage{tikz}								% used to place logos on title page
% \usepackage{gensymb}							% for special characters such as °
\usepackage{titlesec}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{tabularx}
\usepackage{tikzscale}
\usepackage{csvsimple}

\usepackage{enumitem}

\setlist[itemize]{itemsep=3pt, parsep=0pt}
\setlist[enumerate]{itemsep=3pt, parsep=0pt}


\usepackage{appendix}

% PDF/A Compliance, todo: enable and remove hyperref usepackage afterwards
% \usepackage[a-1b]{pdfx}
% \catcode30=12

\usepackage{hyperref}
\hypersetup{hidelinks}

\newcommand{\linkchap}[1]{\hyperref[#1]{chapter~\ref{#1}~\nameref{#1}}}
\newcommand{\linkapp}[1]{\hyperref[#1]{appendix~\ref{#1}~\nameref{#1}}}

\usepackage[acronym]{glossaries}         				% package for glossary

\setcounter{tocdepth}{1}                        % hide subsections from TOC
\makenoidxglossaries
\input{acronyms}                                % include acronyms.txt file
\input{glossary}                                % include glossary.txt file
\graphicspath{{figures/}}						    % set path of graphics folder



% Format chapter titles without "Chapter X" prefix
\titleformat{\chapter}[hang]
{\normalfont\LARGE\bfseries}  % Style: Large bold text
{\thechapter}                 % Number format: Just the number
{1em}                         % Space between number and title
{}                            % Code before the title (empty)


% changed paragraph and subsection appearance
\setcounter{secnumdepth}{3}
\renewcommand{\paragraph}[1]{%
	\subsubsection*{#1}%
%	\addcontentsline{toc}{subsection}{#1}%
}


% mentioned in header
\newcommand{\tblWidthDescription}{\hsize=0.6\hsize\raggedright}
\newcommand{\tblWidthContext}{\hsize=0.2\hsize}


%improved basic functionality
\newcommand{\bolditalic}[1]{\textbf{\textit{{#1}}}}

%indicate citations
% Define a flag to track whether we're inside a raw citation block
\newif\ifrawcitationactive
\rawcitationactivefalse % Default: Not inside a raw citation block

% Define color commands with conditional checking
\newcommand{\rawcitationstart}{
	\color{purple}\rawcitationactivetrue
}
\newcommand{\rawcitationend}{
	\color{black}\rawcitationactivefalse
}

\newcommand{\rawcitationusedstart}{\color{violet}}
\newcommand{\rawcitationusedend}{%
	\ifrawcitationactive
	\color{purple}  % If inside rawcitation, reset to purple
	\else
	\color{black}  % Otherwise, reset to black
	\fi
}


% indicate info about criteria
\newcommand{\baaCriteria}[1]{\textcolor{blue}{#1}}


%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------
\author{Nadja Stadelmann}                       % author name
\city{Lucerne (Switzerland)}                    % author's place of origin
\title{Demographic Biases in\linebreak Dermatology Models}   % thesis title
\subtitle{\large}               % thesis subtitle

\date{2025}                                     % the year when the thesis was written (for the titlepage)
\defensedate{June 23th, 2025}                % the date of the private defense
\defencelocation{Lucerne}                       % location of defence
\extexpert{Dr. Jürg Schelldorfer}                         % name of external expert
\indpartner{Applied AI Research Lab}                       % name of industry partner

% jury, supervisor and dean are only relevant if acceptance sheet is enabled with the next line
% \acceptsheet
\jury{                                          % members of the jury
    \begin{itemize}
        \item Prof. Dr. Name Surname from Lucerne University of Applied Sciences and Arts, Switzerland (President of the Jury);
        \item Prof. Dr. Name Surname from Lucerne University of Applied Sciences and Arts, Switzerland (Thesis Supervisor);
        \item Prof. Dr. Name Surname from Lucerne University of Applied Sciences and Arts, Switzerland (External Expert).
    \end{itemize}
}

\supervisor{Dr. Ludovic Amruthalingam}             % name of supervisor
\dean{Prof. Dr. René Hüsler}                   % name of faculty dean

\acknowledgments{Thanks to my family, relatives and friends for all the support given to finish this thesis.
	\todo{add thanks and gratitude}
	Ludovic Amruthalingam
	Simone Lionetti - deputy Ludovic
	Pascal Baumann - LaTeX
	Philippe Gottfrois - information and work on PASSION project
	Proofreaders \todo{do you want to be mentioned with name or not?}
}


\begin{document}
	\english                                        % define thesis language: \german or \german
	\maketitle
	
	
	%----------------------------------------------------------------------------------------
	%	PREAMBLE
	%----------------------------------------------------------------------------------------
	\begin{abstractstyle}{\hsummary}
		\todo{Your abstract here.}
	    The content of your thesis in brief.
	\end{abstractstyle}
	
	\tableofcontents
	
	\listoftodos
	\todo{also solve todos in the code ;)} 

	
	\todo{remove all \textbackslash rawcitationstart \textbackslash rawcitationend  \textbackslash baaCriteria}
	
	\todo{TEXT MISTAKES ensure fine-tuning ResNet-50, overview of instead of overview over, accuracy (you got all other versions of rr and cc); coma after e.g., point after vs., decision not desicion, decision-making not decision making ...}
	
	\todo{when you got to many pages: fix in order to - to, for the purpose of - For}
	
	
	\baaCriteria{Alle Fakten (fundiertes Wissen Dritter) sind korrekt zitiert. Es werden verschiedene Zitierweisen verwendet und teilweise mehrere Interpretationen gegenübergestellt. Der gemeinsam definierte Zitierstil im Text, in Abbildungen und Tabellen sowie im Literaturverzeichnis wird korrekt und durchgängig angewendet. Eigene Leistungen (sowie Bewertungen) und Fremdquellen sowie Recherchen sind klar unterscheidbar.} 
	
	
	\baaCriteria{Die erstellten Artefakte sind von sehr hoher Qualität. Das trifft u.a. auf Diagramme, Skizzen sowie Notationen (z.B. BPMN/UML) zu. Darstellungen sind einwandfrei, alle statistisch notwendigen Qualitätskriterien sind erfüllt. Beschriftungen etc. sind vorhanden, keine Einwände, Text und Bild stimmen beschreibend gut überein. Es wurden angemessene Dokumentationsmethoden und -arten korrekt verwendet. Vereinbarte Interview Transkripte, Beobachtungsprotokolle bzw. Zusammen-fassungen sind vorhanden. Daten, Ort, Kontext, Beschreibung, Zeilennummer, Verweise, Strukturen sind erkennbar, gut formatiert und korrekt mit dem Text/ der Analyse verknüpft. Alle Elemente und Themen sind im methodischen Teil/Text erklärt und verständlich, keine technischen oder strukturellen Einwände. Auch Zwischenanalysen, Zwischenschritte oder Gesamtauswertungen wurden durchgeführt, die Herkunft der Daten ist erkennbar und professionell aufbereitet.} 
	
	
	\baaCriteria{Der Schreibstil aller Dokumente entspricht hohen Standards und enthält keine Übertreibungen oder unbegründete Beurteilungen. Die Sprache ist aussagekräftig, prägnant und präzise. Die Fachterminologie ist konsistent, d.h. für gleiche Gegenstände und Themen werden immer die gleichen Begriffe verwendet. Der Sprachgebrauch ist durchgängig geschlechtergerecht, einheitlich und sachlich.}
	
	\listoffigures
	\listoftables
	% print list of acronyms and glossary
	\printnoidxglossaries
	
	\todo{fix citations in glossary}
	
	%----------------------------------------------------------------------------------------
	%	MAIN CONTENT
	%----------------------------------------------------------------------------------------
	\mainmatter
	
	% write or compose the main document here
	
	\chapter{Problem Statement}
		\baaCriteria{Welche Ziele, Fragestellungen werden mit dem Projekt verfolgt? Die Bedeutung, Auswirkung und Relevanz dieses Projektes für die unterschiedlichen Beteiligten soll aufgeführt werden. Typischerweise wird hier ein Verweis auf die Aufgabenstellung im Anhang gemacht.}
		
		In Sub-Saharan Africa dermatology treatment is inaccessible according to \textcite{Gottfrois2024}. There is fewer than one dermatologist available per one million people. Despite this, up to 80\% of the children and adolescents in the area are affected by skin conditions. \Gls{teledermatology} based on \gls{AI} promises to close this gap of specialists per case, for example by serving as a triage option. Potential patients could upload pictures to diagnostic dermatology \glspl{AI} which can indicate whether the person should indeed visit a dermatologist or promote other treatment options. However, current dermatology \glspl{AI} tend to fail to deliver accurate results for patients with highly pigmented skin tones. This is mainly due to demographic biases in existing \gls{AI} models. The models are trained on established datasets which mainly feature low pigmented skin. Therefore, the datasets lack representation of highly pigmented skin, leading to AI models which do not generalize to the population in Sub-Saharan Africa \autocite{Gottfrois2024}.
		
		These biases result in unequal access to treatment and especially affect underrepresented groups. Such biased results must be avoided, especially in AI models which impact life-changing decisions \autocite{Mehrabi_2021}.
		
		According to \textcite{Diaz2022}, demographic biases are especially important in dermatology. Demographic differences in patients influence the appearance of dermatological conditions. The differences in appearance can be developed depending on genetic factors, such as skin tone, age and sex \autocite{Diaz2022}. Research showed, that in patients with lower socio-economic status the disease progression is more advanced at time of diagnosis, which in turn can lead to different appearances for the same disease \autocite{BAD2021}. Since the AI models use pictures as the inputs and can only learn to diagnose diseases according to their appearances in the data, the factors which affects the disease appearances must be considered when creating an inclusive dataset.
		
		In order to overcome these issues, the PASSION research team founded the PASSION project. The projects vision is to make dermatology treatment accessible in Africa by enabling the AI-supported \gls{teledermatology} for triage by reducing the demographic biases in the dermatology AI models. For this bias mitigation, the researcher collected a dataset in Sub-Saharan Africa, focusing on patients with highly pigmented skin and the most common regional \gls{pediatric} skin conditions. The PASSION dataset is complementary to existing datasets and improves their diversity. With this dataset, the PASSION team trained a ResNet-50 model which was pretrained on ImageNet. This thesis refers to this trained model as the PASSION model. It should serve as a benchmark model to assess other dermatology models in regards of fairness \autocite{Gottfrois2024}. \todo{check sources, maybe, for the last sentence, the midterm protocol must be cited instead}
		
		So that the PASSION model can become an unbiased benchmark model, potential demographic biases in it must be reduced as far as possible. To reach this goal, demographic biases in the model as well as the limitation of the gathered dataset must be identified and mitigated. This thesis supports the PASSION team in this process. The main objective of the thesis is to assess the effectiveness of mitigation strategies to reduce demographic biases in context of PASSION.
		
		
		\begin{comment}
		\begin{enumerate}
			\item Identify demographic biases in dermatology AI models, using established fairness metrics.
			\item Identify mitigation strategies to minimize these biases.
			\item Assess the effectiveness of the mitigation strategies.
		\end{enumerate}
		It is important to identify the existent biases first, so that the mitigation strategies can be \todo{proceed here to reason why you chose those objectives}
		\end{comment}
		
	    \rawcitationstart
		\begin{itemize}
			\item With the advent of telemedicine, developing countries are learning newer ways of leveraging their information and communication technologies (ICTs) to play an increasingly
			vital role in the health care industry. Telemedicine is defined as a health care delivery mechanism where physicians and other medical personnel can examine patients remotely
			using information and telecommunication technologies (ICTs; Bashshur, Sanders, and Shannon, 1997). \autocite{Kifle_2024}
			\rawcitationusedstart
			\item AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations \autocite{Mehrabi_2021}.
			\rawcitationusedend
			\item There are clear benefits to algorithmic decision-making; unlike people, machines do not become tired or bored [45, 119], and can take into account orders of magnitude more factors than people can. However, like people, algorithms are vulnerable to biases that render their decisions “unfair” [6, 121]. In the context of decision-making, fairness is \textit{the absence of any prejudice or favoritism toward an individual or group based on their inherent or acquired characteristics}. Thus, an unfair algorithm is one whose decisions are skewed toward a particular group of people. \autocite{Mehrabi_2021}.
			\item it is important for researchers and engineers to be concerned about the downstream applications and their potential harmful effects when modeling an algorithm or a system \autocite{Mehrabi_2021}.
			\item We should think responsibly, and recognize that the application of these tools, and their subsequent decisions affect peoples’ lives; therefore, considering fairness constraints is a crucial task while designing and engineering these types of sensitive tools \autocite{Mehrabi_2021}.
		\end{itemize}
		\rawcitationend
	
		\begin{comment}
			
		This thesis is part of the PASSION project. The PASSION research team identified that in Africa, dermatology treatment is not accessible. There is less than one dermatologist per one million citizens. In contrast, there is high demand for dermatology treatment, especially among children and adolescents. 80\% of the \gls{pediatric} population is affected. The goal of PASSION is to make dermatology treatment more accessible by using AI supported telemedicine for triage \autocite{Gottfrois2024}.
		
		For AI supported triage, demographic biases in existing dermatology models is a problem since the corresponding datasets lack diversity, especially regarding skin tones \autocite{Gottfrois2024}. This type of bias is important in dermatology, since different diseases present themselves differently depending on the skin-color \autocite{Diaz2022}. Further, skin diseases are more advanced or severe at diagnosis in patients with lower socioeconomic status \autocite{BAD2021}.
		
		PASSION tries to mitigate the demographic bias by providing a dataset of pigmented skin images of patients from Sub-Saharan Africa. The PASSION team focused on gathering data with \gls{FST} IV, V and VI. Further, the covered conditions represent up to 80\% of the conditions in the \gls{pediatric} population, the demographic group who is most affected by skin disease \autocite{Gottfrois2024}. \gls{HSLU}
		
		The PASSION dataset is complementary to the existing datasets and improves the diversity in a combined dataset. Within the dataset itself, there could potentially be further demographic biases, e.g. related to age or gender.

		\section{Objective}
			The goal of this research is to
			\begin{enumerate}
				\item Identify demographic biases in dermatology AI models, using established fairness metrics.
				\item Identify mitigation strategies to minimize these biases.
				\item Assess the effectiveness of the mitigation strategies.
			\end{enumerate}
			It is important to identify the existent biases first, so that the mitigation strategies can be \todo{proceed here to reason why you chose those objectives}
		\end{comment}
	
	\chapter{State of Research}
		\baaCriteria{Bezogen auf die eigenen Zielsetzungen und Fragestellungen soll aufgezeigt werden, wie andere dieses oder ähnliche Probleme gelöst haben. Worauf können Sie aufbauen, was müssen Sie neu angehen?	Wodurch unterscheidet sich Ihre Lösung von anderen Lösungen? Für wissenschaftlich orientierte Arbeiten sei hier explizit auf (Balzert, S. 66 ff) verwiesen.}
		\baaCriteria{Relevante, aktuelle und fundierte Fachliteratur wurde identifiziert, kritisch geprüft und verwendet. Die Begriffe der Fragestellung sind definiert und referenziert. Der gesamte Kontext ist verknüpft und eine Abgrenzung wurde vorgenommen. All dies ist in einer leicht verständlichen Struktur formuliert und überprüft.}
		
		This chapter provides a review of existing work in the field of bias mitigation in \gls{AI}. The main focus lies on a literature review of existing papers from other researchers in this area, highlighting the key findings which are connected to this thesis. Bias mitigation in \gls{AI} has already been investigated by different researchers, who crafted fitting mitigation methods \todo{citation?}. This thesis aims to assess those existing methods in the context of PASSION.
		
		Therefore, this chapter first presents an overview of the PASSION project based on the PASSION paper and dataset. Then, the general knowledge in the literature about existing biases, fairness metrics and mitigation methods is summarized. The review process was divided into two main contexts: \gls{ML} in general, and \gls{ML} in dermatology. This approach ensures that the technical and dermatological perspectives are considered when applying the knowledge to PASSION. The tables in this chapter indicate which points were found in which context. This is important, since what may be an issue in general might not be relevant for a specific use case or vice versa. For example, in theory, all age groups should be represented in datasets to account for demographic diversity. However, for car insurance, age representation is not important, because age does not affect how well a driver can drive \todo{either cite this example from the expert or find another example related to dermatology}.
		
		The various studies present different bias sources and suggest diverse methods to mitigate them. During the literature review, several biases and mitigation methods were identified that may be relevant to the PASSION project. Since it is not feasible to assess all of them during the duration of this thesis, the thesis focuses on those which are related to skin type, age and gender. The chosen methods are explained in \linkchap{chap:methodology}. The other items are passed to the PASSION research team as a list for further investigation. The list can be found in the appendix \todo{add link}.
		
		
		\todo{put the evaluation stuff in the execution / analysis section!!}
		
		
		\section{PASSION for Dermatology}
			This section provides an overview of the PASSION project regarding its medical scope and technical components.
			
			While the overall goal remains to improve the accessibility of dermatological care by building fair and inclusive AI systems, PASSION specifically addresses common \gls{pediatric} skin conditions in Sub-Saharan Africa. To create a dataset which represents patients with highly pigmented skin, they collected data from patients with \gls{FST} III to VI. Based on this dataset, the PASSION team fine-tuned a ResNet-50 model using transfer learning. With the dataset and trained model, the researchers published data analysis scripts and initial insights on the model performance in a MICCAI \todo{add to glossary} publication \autocite{Gottfrois2024}.
			
			For the purpose of this thesis, it is essential to understand the dataset's metadata, the architecture and fine-tuning process of the PASSION model and which bias mitigation methods have already been applied. The dataset can influence which biases could arise in the model or rather which ones can be measured. The labels which should be predicted, and the model architecture give insight into the \gls{ML} task. All this information affects which mitigation methods are feasible to be used for the project. \todo{add sources}
			
			\subsection{PASSION Dataset}
				The PASSION dataset contains data from patients from four African countries in dermatology clinics. It contains 4901 images of 1653 dermatology cases with the corresponding demographic and clinical metadata. Each patient is represented by one record, with images linked to the record via filename. The images were captured with mobile phones to ensure that the training data complies with a \gls{teledermatology} setting regarding image quality \autocite{Gottfrois2024}.
				
			    A predefined 80/20 stratified train-test split at patient level ensures reproducibility and fair comparison, while preventing information leaking \autocite{Gottfrois2024}.
			    
			    Stratified splitting is a method to split  datasets while maintaining the original class distribution within the subsets. This is important for imbalanced datasets to maintain minority class representation  \autocite{Balde_2023}.
			    
				The metadata, as listed in \autoref{tab:PASSION_metadata}, includes demographic attributes such as \textit{age}, \textit{sex}, and \textit{\gls{FST}}. These are essential for identifying potential demographic biases lateron.	The labels \textit{impetig} and \textit{conditions\_PASSION} represent dermatology diagnosis as evaluated by dermatologists \autocite{Gottfrois2024}, and are the target variables the PASSION model learns to predict. Therefore, this \gls{ML} task is a multilabel classification problem. PASSION addresses this by training separate models for each label \autocite{Gottfrois2024}. The prediction of conditions\_PASSION is a multiclass classification task, while predicting impetig is a binary classification task. 
				
				\begin{table}[H]
					\centering
					\begin{tabularx}{\textwidth}{>{\hsize=.27\hsize}X>{\hsize=.27\hsize\raggedright}X>{\hsize=.46\hsize}X}
						\toprule
						\textbf{Metadata Attribute}       & \textbf{Data Type} & \textbf{Description}       \\ \midrule
						subject\_id          & string & Participant's unique identifier        \\
						country              & string & Country of data origin \\
						age                  & integer & Age of the participant in years       \\
						sex                  & m/f/o & Gender of the participant               \\
						fitzpatrick          & integer & \gls{FST}                \\
						body\_loc            & string (list; null-able, semicolon-separated) & Specifically affected body locations \\
						impetig              & 0/1  & Presence of impetigo (1=present), may occur alone or with other conditions, affects the treatment options for coexisting conditions        \\
						conditions\_PASSION  & Eczema, Scabies, Fungal, Others & Primary diagnosed skin condition \\
						\bottomrule
					\end{tabularx}
					\caption{PASSION dataset - metadata attributes and descriptions \autocite{Gottfrois2024}}
					\label{tab:PASSION_metadata}
				\end{table}
				
				The PASSION team also provides a set of \gls{JupyterNotebook}-based data analysis scripts. For example, one script analyses the correlation between the clinical conditions and location of the data collection. A full list of these scripts is included in \linkapp{app:PASSIONdataAnalysisScripts}. Additionally, the paper visualizes demographic analyzes related to age, sex and \gls{FST} as shown in \autoref{fig:PASSIONDistr}.
				
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.9\textwidth]{figures/PASSIONDatasetDistribution.png}
					\caption{PASSION data distributions \autocite{Gottfrois2024}}
					\label{fig:PASSIONDistr}
				\end{figure}
				
				
				Due to the sensitivity of patient data, the dataset is confidential. Access to it can be requested via the project website: \href{https://passionderm.github.io/}{https://passionderm.github.io/} \autocite{Gottfrois2024}.
				
			\subsection{PASSION Model}
			  The model architecture is a ResNet-50 model which is pretrained on ImageNet. The model was fine-tuned by replacing the last fully connected classification layer with a dropout layer with a 0.3 dropout rate followed by batch normalization. The class activation is done by a single linear layer. To minimize the weighted cross-entropy loss, Adam optimization is used. For improved generalization and to avoid overfitting, data augmentations were applied. The methods used were random resizing, cropping, flipping, and rotating. For training, the model uses 5-fold cross-validation \textcite{Gottfrois2024}.
			  			
			\subsection{PASSION Experiments}
			  The PASSION team conducted various experiments to evaluate the classifiers on the test set with the following schemes \autocite{Gottfrois2024}:
			  \begin{itemize}
			  	\item Performance for skin condition prediction
			  	\item Performance for impetigo detection
			  	\item Generalization from two centers to a wider population (test set contains data from the known centers and one unknown center)
			  	\item Generalization from different age groups (test set contains data from the known age groups and one unknown)
			  	\item Subject level analysis over the predictions of multiple pictures, using majority voting
			  \end{itemize}
			  
			  The code for those experiments is available in the PASSION evaluation GitHub repo. This repo can serve as a starting point, since reproducing the results helps to verify that the provided setup works the same on my side. Also, they can be used as examples for further experiments. \todo{mention which ones I really used why for the thesis and move the others to the appendix}
			  
			  The paper indicates lower performance when evaluating the model on a subject level (performance per case/patient) rather than a sample level (performance per image). The authors emphasize the importance of assessing classifier performance on both levels for completeness \autocite{Gottfrois2024}. Therefore, the subject level performance should also be considered during this thesis.
			  \todo{challenge this to be tested again in the outlook bc of the inproper metadata linkage}
	
	
		\subsection{Limitations}
				\todo{maybe move to execution phase}
				\todo{write in more details}
			   - multiple executions showed inconsistent results for the different group evaluations on the same model checkpoint. It turned out that the metadata linkage did not work consistently. I resolved the issue was resolved by providing the image name in the data loader and link the metadata directly from the source file instead of using the indexes. probably related to different shuffling between data loader and metadata loader
		
			\begin{comment}
				\todo{probably remove this}
		
			\subsection{Telemedicine \todo{is this chapter needed?}}
				\rawcitationstart
				\begin{itemize}
					\item Teledermatology. Telemedicine may be one of the first fields to embrace AI, driven by demand for services, the necessity of collecting fit-for-purpose high-quality images, and the availability of existing technology (Xiong et al., 2019). Face-to-face diagnostic accuracy exceeds that of teledermatology (Finnane et al., 2017); however, inequalities surrounding access to dermatological care persist. Teledermatology has the potential to increase access by facilitating referrals and offering convenience and decreased wait times (Finnane et al., 2017), as well as providing diagnostic support at the time of case review. For teledermatology cases, the accuracy of a DL classifier (0.67) matched dermatologists’ (0.63) and was higher than primary care physicians’ (0.45) for 26 skin conditions (Liu et al., 2019b). AI may be integrated into smartphone apps to photograph skin lesions, collect relevant clinical information, and generate a referral if appropriate. Many smartphones already support on-device DL with Google’s TensorFlow Lite (TensorFlow, 2020) or Apple’s Core\gls{ML} (Apple Inc, 2020), preserving privacy by keeping health information on the device. A systematic review found nine studies that evaluated six algorithm-based smartphone apps and concluded that evidence of diagnostic accuracy was poor and does not support current implementation, despite two apps having obtained the CE marking; no apps are Food and Drug Administration approved (Freeman et al., 2020). AI may also assist in automatic tracking and monitoring of skin lesions; although preliminary results are promising, existing studies used small datasets with little description, and there is no established standard metric of change (Navarro et al., 2019). Further study hinges on the prospective collection of large datasets. \autocite{Young_2020}
					\item \autocite{Tsetsi_2017} on smartphone / internet access divide between populations
					\item https://www.tandfonline.com/doi/full/10.1080/08870446.2019.1579330 on how open people are to use AI
				\end{itemize}
				\rawcitationend
			\end{comment}
		
		
		\section{Bias}
			This chapter provides an overview of biases and related demographic characteristics mentioned in \gls{ML}- and dermatology-related research. It also explains their relevance for PASSION.
			
			Algorithmic decisions made by \gls{AI} systems can directly affect peoples' lives. In healthcare applications such as PASSION, these decisions are especially sensitive, as they influence diagnoses and treatment outcomes. Diverse studies have shown that \gls{AI} application's decisions can hold biases that affect underrepresented groups. This leads to unfair or even harmful consequences. Therefore, it is essential for \gls{AI} engineers to identify, address, and mitigate such biases in order to develop fair applications. This requires an understanding of what bias is in general, which concrete biases exist, and where they originate \autocite{Mehrabi_2021}.
			
			
			\begin{comment}
			\rawcitationusedstart
			\begin{itemize}
				\item Bias in facial recognition systems [128] and recommender systems [140] have also been largely studied and evaluated and in many cases shown to be discriminative towards certain populations and subgroups. In order to be able to address the bias issue in these applications, it is important for us to know where these biases are coming from and what we can do to prevent them.\autocite{Mehrabi_2021}.
				\item We should think responsibly, and recognize that the application of these tools, and their subsequent decisions affect peoples’ lives; therefore, considering fairness constraints is a crucial task while designing and engineering these types of sensitive tools \autocite{Mehrabi_2021}.
			\end{itemize}
			\rawcitationusedend
			
			\end{comment}
		
			\subsection{Definition of Bias in \gls{ML}}
		    In the context of \gls{ML}, bias can be defined as \textit{a systematic error that causes a model or estimator to consistently deviate from the true value or relationship} \autocite{Delgado-Rodriguez_2004, Taylor_2023}. In practice, this often results in models that make less accurate predictions for specific subgroups within the population \todo{cite this}.
		    			    
		  	\todo{make sure the following is cited correctly}
			
			\subsection{Demographic Biases in the Context of Dermatology} \label{chap:demographicBiasesDermatology}
			Biases in dermatology in general can lead to unequal outcomes for different groups, which can result in unfair outcomes for certain groups. Demographic biases are particularly relevant in the context of dermatology \glspl{AI}, as they can cause differences in diagnostic accuracy and treatment outcomes among different demographic (sub-)groups.
			From the literature review, three main ways have been identified in which demographic differences may introduce bias in dermatology \gls{ML} models:
			\todo{cite all that, from presentation}
			
			\begin{itemize}
				\item \textbf{Disease Presentation}. \textit{Skin type} affects how diseases appear on the skin. As Gottfrois notes, "any condition linked to inflammation is less visible if the skin is more pigmented" \todo{cite mail from philippe}. This directly influences training and evaluating image-based \gls{ML} models like those used in PASSION. For example, a model trained predominantly on images with low pigmented skin may perform poorly on images of highly pigmented skin.
				
				\item \textbf{Disease Prevalence}. Factors such as \textit{age} and \textit{sex} do not tend to affect disease presentation, but they can influence disease prevalence \todo{cite mail from philippe}. Also, \textit{geographic location} can influence the prevalence of skin conditions (e.g., tropical vs. dry climates) \todo{add source}. Therefore, these factors could introduce bias if certain conditions are underrepresented in the dataset due to demographic imbalances. \todo{consider adding smt like the car driver example here, indicating that it is not necessarily a problem due to the same disease presentation}
				
				\item \textbf{Access to Healthcare}. \textit{Socioeconomic status} or \textit{geographic location} can also introduce bias. Research shows that patients with lower socioeconomic status are often diagnosed at later stages of the disease, which may alter the visual presentation of the disease. If such cases are missing in training data, the model may fail to recognize them, leading to misdiagnosis. \todo{add example for geographic location?}.
			\end{itemize}
			
			
			To build a robust and fair \gls{ML} model, it is essential to identify and address biases linked to such protected characteristics \autocite{Mehrabi2022}.
			\todo{check that there is no duplication between PASSION dataset feature description and here}
			\todo{probably remove}
			Due to time constraints, this thesis focuses on three protected characteristics: skin type, age, and sex. These were selected based on their presence in the PASSION dataset and their influence on dermatological diagnosis and disease prevalence. Other potentially relevant features, such as geographic location and socioeconomic status, should be evaluated in future work by the PASSION team.
			
			
			\begin{comment}
				\todo{if citing is an issue: check the comment}
						
				It captures three distinct pathways through which demographic differences can introduce bias in dermatological machine learning systems:
				
				Disease Presentation — covers how diseases manifest differently on various skin types, directly affecting the visual input to image-based models.
				Disease Prevalence — focuses on who is more likely to have certain conditions, which affects label distribution in the dataset.
				Access to Healthcare — reflects when and how people enter the medical system, influencing data collection quality and representativeness.
				
				Each of these groups addresses a different layer of the data generation and learning process:
				
				Input variability (visual features),
				Target/label imbalance (class representation),
				Data collection bias (who gets diagnosed and when).
				
				This structure is also supported in literature on medical \gls{AI} fairness (e.g., in works by Obermeyer et al. or Adamson & Smith).
			\end{comment}
			
			\subsection{Types of Biases and Their Relevance for PASSION}
			The literature describes numerous types of bias. Over 60 were identified during this research. These factors were grouped into categories to provide an overview, and their relevance to the PASSION context was assessed.
			
			Among them, \textit{sampling biases} and \textit{representation biases} are particularly relevant, as they relate directly to the inclusion or exclusion of demographic subgroups in the dataset. For example, \textit{ascertainment bias}, a subtype of sampling bias, occurs when parts of the target population are unintentionally excluded. A common example is healthcare studies conducted in public hospitals only, which excludes patients from higher socioeconomic backgrounds who visit private clinics. This skews the data and can lead to incorrect conclusions, such as overestimating disease prevalence in specific groups.
			
			Other relevant categories include \textit{medical biases} and \textit{imaging biases}, especially in the teledermatology setting of PASSION. These include clinical labeling errors, variations in image quality or lighting conditions which lead to bias.
			
			
			This thesis focuses on the most relevant bias types. An extensive list is provided in \linkapp{app:listOfBiases} and will be shared with the PASSION team for further evaluation.
			
			\todo{add the 5-10 most important biases here}
			
			\subsection{Sensitive Features}
			Research has identified sensitive features that are particularly prone to bias. These features have already caused biases in existing \gls{AI} applications and should therefore be carefully evaluated during model development \autocite{Mehrabi_2021}.
			
			\autoref{tab:biases_features} summarizes sensitive features mentioned in the literature. The categorization in the table was done based on the research described in \autoref{chap:demographicBiasesDermatology}. For completeness, the table also contains sensitive demographic features which appear unrelated to dermatology according based on current research.
			
			\begin{comment}
			\todo{check what to do with those additional features:}
			Other important features according to (\autocite{Montoya_2025} 13):
			lesion type, anatomical location of lesion, img characteristics such as source, imaging techniques, resolution, real vs. artificially generated
			
			In addition to demographic factors, domain-specific variables such as lesion type, anatomical location, and image characteristics (e.g., imaging technique, resolution, device source, or whether an image is real vs. artificially generated) can also influence model behaviour \autocite{Montoya_2025}. These features are important considerations for dataset curation and model evaluation in dermatology-focused applications like PASSION.
			\end{comment}
			
			
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Bias-Sensitive Features} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%\midrule
						\multicolumn{3}{l}{\textbf{Related to Disease Presentation}} \\
						Skin Type & X\tnote{1,2,7} & X\tnote{12,13}\\
						Skin Undertones & & X\tnote{13} \\
						Socio-Economic Status & X\tnote{6} & X\tnote{12} \\
						Geographic Location \todo{double check this!} & X\tnote{1,3} & \\
						
						\multicolumn{3}{l}{\bolditalic{Related to Disease Prevalence}} \\
						Age & X\tnote{7,11} &  X\tnote{13} \\
						Gender/Sex & X\tnote{1,2,7,8,9,10,11} & X\tnote{13} \\
						Gender and Skin Type Subgroups & X\tnote{1,2} & \\
						
						\multicolumn{3}{l}{\bolditalic{Related to Access to Healthcare}} \\
						Geographic Location & X\tnote{1,3} & \\
						Socio-Economic Status & X\tnote{6} & X\tnote{12} \\
						
						\multicolumn{3}{l}{\bolditalic{Relation to Dermatology to be Checked}} \\
						Ethnicity/Race & X\tnote{1,2,4,5,6,7,11}&  X\tnote{12,13} \\
						Disabilities & X\tnote{7,11} & \\
						
						\multicolumn{3}{l}{\bolditalic{Unrelated to Dermatology}} \\
						Familial status & X\tnote{7} & \\
						Marital status & X\tnote{7,11} & \\
						Nationality/National origin & X\tnote{7,11} & \\
						Recipient of public assistance & X\tnote{7} & \\
						Religion & X\tnote{7,11} & \\
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M24_Buolamwini_2018}
							\item[3] \autocite{M142_Shankar_2017}
							\item[4] \autocite{M98_Manrai_2016}
							\item[5] \autocite{M54_Fry_2017}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[6] \autocite{M150_Vickers_2014}
							\item[7] \autocite{M30_Chen_2019}
							\item[8] \autocite{M167_Zhao_2017}
							\item[9] \autocite{M20_Bolukbasi_2016}
							\item[10] \autocite{M168_Zhao_2018}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[11] \autocite{M62_Hajian_2013}
							\item[12] \autocite{Young_2020}
							\item[13] \autocite{Montoya_2025}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Commonly used features which often are affected by biases}
				\label{tab:biases_features}
			\end{table}
			
			
			\begin{comment}
			\todo{decide which table to use, more or less extensive citations?}
			
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Bias-Sensitive Features} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%\midrule
						\multicolumn{3}{l}{\textbf{Dermatology Related Features}} \\
						Skin Type & X\tnote{1,3} & X\tnote{5,6}\\
						Skin Undertones & & X\tnote{6} \\
						
						\multicolumn{3}{l}{\textbf{Demographic Features}} \\						\multicolumn{3}{l}{\bolditalic{Relevant for Skin Disease Detection}} \\
						Age & X\tnote{3,4} &  X\tnote{6} \\
						Gender/Sex & X\tnote{1,3,4} & X\tnote{6} \\
						Gender and Skin Type Subgroups & X\tnote{1} & \\
						Ethnicity/Race & X\tnote{1,2,3,4}&  X\tnote{5,6} \\
						
						\multicolumn{3}{l}{\bolditalic{Potentially Relevant for Skin Disease Detection}} \\
						Geographic Location & X\tnote{1} & \\
						Socio-Economic Status & X\tnote{2} & X\tnote{5} \\
						Disabilities & X\tnote{3,4} & \\
						
						\multicolumn{3}{l}{\bolditalic{Not Relevant for Skin Disease Detection}} \\
						Familial status & X\tnote{3} & \\
						Marital status & X\tnote{3,4} & \\
						Nationality/National origin & X\tnote{3,4} & \\
						Recipient of public assistance & X\tnote{3} & \\
						Religion & X\tnote{3,4} & \\
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.30\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M150_Vickers_2014}
						\end{minipage}%
						\begin{minipage}{0.40\textwidth}\raggedright
							\item[3] \autocite{M30_Chen_2019}
							\item[4] \autocite{M62_Hajian_2013}
						\end{minipage}%
						\begin{minipage}{0.30\textwidth}\raggedright
							\item[5] \autocite{Young_2020}
							\item[6] \autocite{Montoya_2025}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Features which often hold biases}
				\label{tab:biases_sensitive_features}
			\end{table}
			\end{comment}
			
		\section{Fairness Metrics}
		This chapter introduces the concept of fairness in \gls{ML}, as fairness is a way to detect whether and what biases exist in a model. As there is no universally accepted definition of fairness, various fairness metrics have been proposed in the literature, each based on different assumptions and goals \autocite{Mehrabi_2021}.
		
		
		\subsection{Definition of Fairness in \gls{ML}}
		
		In research, there is currently no common agreement regarding a fairness definition in \gls{ML}. Broadly, fairness \textit{is the absence of bias towards individuals or groups in a decision-making context}. To assess how fair \gls{AI} models are, multiple fairness metrics have been proposed in the literature, each reflecting different interpretations of fairness. The choice of metric largely depends on the specific use case of the application \autocite{Mehrabi_2021}.
		
		\subsection{Fairness Metrics} \label{chap:FairnessMetrics}
		
			\textcite{Mehrabi_2021} summarized the fairness metrics and grouped them into the categories group fairness, subgroup fairness and individual fairness, depending on the main mechanics of the metrics. They are listed in \autoref{tab:fairness_definitions}.
		
			\begin{table}[H]
			\centering
			\begin{threeparttable}
				\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
					\toprule
					\textbf{Fairness Definitions} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
					& \textbf{\gls{ML}} & \textbf{Dermatology} \\
					%	\midrule
					\multicolumn{3}{l}{\textbf{Group Fairness}} \\ 
					Conditional Statistical Parity    & X &   \\
					Demographic/Statistical Parity  & X & \\
					Equal Opportunity& X &   \\
					Treatment Equality & X &   \\
					Test Fairness         & X &   \\
					Equalized Odds     & X &   \\
					%	\midrule
					\multicolumn{3}{l}{\textbf{Subgroup Fairness}} \\ 
					Subgroup Fairness    & X &   \\
					%\midrule
					\multicolumn{3}{l}{\textbf{Individual Fairness}} \\ 
					Counterfactual Fairness     & X &   \\
					Fairness Through Awareness     & X &   \\
					Fairness Through Unawareness        & X &   \\
					%\midrule
					\multicolumn{3}{l}{\textbf{Not Categorized}} \\ 
					Fairness in Relational Domains& X &   \\
					\bottomrule
				\end{tabularx}
			\end{threeparttable}
			\caption{Fairness definitions based on \textcite{Mehrabi_2021}}
			\label{tab:fairness_definitions}
		\end{table}
		
		To better understand how fairness can be formally defined, consider the example of equalized odds, introduced by \textcite{M63_Hardt_2016}: \newline
		"\textit{A predictor $\hat{Y}$ satisfies equalized odds with respect to protected attribute $A$ and outcome $Y$, if $\hat{Y}$ and $A$ are independent conditional on $Y$. \newline
			\(
			P(\hat{Y} = 1 \mid A = 0, Y = y) = P(\hat{Y} = 1 \mid A = 1, Y = y), \quad \forall y \in \{0, 1\}
			\)"} \todo{add formula list} \newline
		In other words, the probability of predicting a positive outcome should be the same across protected and unprotected groups, given the true label $Y$. This ensures that both \gls{TPR} and \gls{FPR} are equal across different demographic groups. If these rates are the same, like in the example of \autoref{fig:eqOdds}, the model satisfies equalized odds, and fairness is achieved. Since equalized odds compares conditional probability distributions across groups, it is a group fairness metrics.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.9\textwidth]{figures/EqualizedOddsIllustration.png}
			\caption{Equalized odds mechanics, inspired by \textcite{M80_Kearns_2019}.}
			\label{fig:eqOdds}
		\end{figure}
		
		The mechanics of the other fairness metrics are described broadly in \linkapp{app:fairnessMetrics}.
		
		There are Python libraries like \textit{\gls{Fairlearn}} available, which can be used for the computation of the fairness metric \autocite{Agarwal_2018}. They tend to support the most popular metrics for binary classification \autocite{Fairlearn_nodate}.
		
		\subsection{Limitations of Group Fairness}
		
		Despite its usefulness, equalized odds and similar group fairness metrics have limitations. These metrics can hide inequalities that exist within more specific subgroups. For example, a model might appear fair when assessed across broad groups such as age or skin type (\autoref{fig:eqOdds}) but still exhibit substantial disparities within subgroups, such as older individuals with darker skin tones (\autoref{fig:eqOddsLimits}) \autocite{M79_Kearns_2018,M80_Kearns_2019}.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.5\textwidth]{figures/EqualizedOddsSubgroupsIssueIllustration.png}
			\caption{Equalized odds violations on subgroups, inspired by \textcite{M80_Kearns_2019}.}
			\label{fig:eqOddsLimits}
		\end{figure}
		
		To address this issue, subgroup fairness metrics have been proposed. These extend group fairness metrics by explicitly evaluating fairness across subgroups. This ensures that fairness assessments do not overlook hidden biases that could affect smaller populations \autocite{M79_Kearns_2018,M80_Kearns_2019}.
		
			
		\section{Mitigation Methods}
			 \todo{still to be written}
			 
			 see text from bias chapter - Further, \gls{AI} engineers need to know what prevention methods are available to reduce the biases \autocite{Mehrabi_2021}.
			
			\subsection{Mitigation Methods Overview}
				\todo{write definitions of pre-in and post-processing, see Methods for fair machine learning below [43, 11, 14]}


				\todo{add stratified split}
				\todo{double check and futher improve groups}
				\begin{table}[H]
				\centering
				\begin{threeparttable}
						\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Mitigation Methods - Unbiasing Data (Pre-Processing)} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						\multicolumn{3}{l}{\textbf{Documentation and Transparency}} \\
						Good Practices while using Data & X\tnote{1,2,3} &   \\
						Datasheets as supporting document for dataset creation method, characteristics, motivations and skews & X\tnote{1,2,3} &   \\
						Datasheets as supporting document for model method, characteristics, motivations and skews & X\tnote{1,4} &   \\
						Dataset (Nutrition) Labels & X\tnote{1,5,6} & X\tnote{18, \todo{add spec source}}   \\
						
						\multicolumn{3}{l}{\textbf{Communication and Reporting}} \\
						Messaging & X\tnote{1,12} &   \\
						
						\multicolumn{3}{l}{\textbf{Bias Detection and Evaluation}} \\
						Test for Simpson's Paradox \todo{Discribe Simpson's Paradox} & X\tnote{1,7,8,9} &   \\
						Detect Direct Discrimination with Causal Models and Graphs & X\tnote{1,10} &   \\					
						Out-of-Distribution Detection in Dermatology Using Input Perturbation and Subset Scanning & & X\tnote{19} \\
						Check confidence interval and p-curve analysis instead of p-value & & X\tnote{17} \\
						 
						\multicolumn{3}{l}{\textbf{Study Design}} \\ 
						Allocation concealment and blinding & & X\tnote{17} \\
						Preventing Direct and Indirect Discrimination & X\tnote{1,11} &   \\
						
						\multicolumn{3}{l}{\textbf{Data Gathering}} \\ 
						Data Collection from diverse sources (incl. primary care clinics) & X\tnote{18} & \\
						Robust standards for external validation & X\tnote{18} & \\
						Preferential Sampling & X\tnote{1,13,14} &   \\
						Geographical Diversity and Inclusion for Dataset creation & X\tnote{16} & \\
						Balanced Representation accross skin tones and genders & & X\tnote{19} \\
						Disparate Impact Removal & X\tnote{1,15} &   \\
						
						\multicolumn{3}{l}{\textbf{Labeling}} \\ 
						Multidimensional Scale for Skin Tones & & X\tnote{19} \\
						
						
						\multicolumn{3}{l}{\textbf{Data Availability and Open Science}} \\ 
						Publish Datasets accessible for the public & & X\tnote{18, \todo{add source}} \\						
						
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M13_}
							\item[3] \autocite{M55_}
							\item[4] \autocite{M110_}
							\item[5] \autocite{M66_}
							\item[6] \autocite{M66Successor_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[7] \autocite{M81_}
							\item[8] \autocite{M3_}
							\item[9] \autocite{M4_}
							\item[10] \autocite{M163_}
							\item[11] \autocite{M62_Hajian_2013}
							\item[12] \autocite{M74_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[13] \autocite{M75_}
							\item[14] \autocite{M76_}
							\item[15] \autocite{M51_}
							\item[16] \autocite{M142_Shankar_2017}
							\item[17] \autocite{Chakraborty_2024}
							\item[18] \autocite{Young_2020}
							\item[19] \autocite{Montoya_2025}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Mitigation Methods - Unbiasing Data - Mentioned in Contextual Research, grouped like in \textcite{Mehrabi_2021}, the author cannot guarantee for completeness}
				\label{tab:mitigation_methods_unbiasing_data}
			\end{table}
				
				
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Mitigation Methods - Fair Classification} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%	\midrule
						
						\multicolumn{3}{l}{\textbf{Satisfy Fairness Definitions}} \\ 
						Satisfy Subgroup Fairness  \todo{unclear if \tnote{*} in \tnote{3} as well, or if \tnote{2} also handles \tnote{*}} & X\tnote{1,2} &   \\
						Satisfy Equality of Opportunity\tnote{*} & X\tnote{1,3,6} & \\					
						Satisfy Equalized Odds\tnote{*} & X\tnote{1,3} &   \\
						Disparate Treatment\tnote{**} & X\tnote{1,4,5} &  \\
						Disparate Impact\tnote{**} & X\tnote{1,4,5} &  \\
						\todo{find out exact method} & X\tnote{1,7} &  \\
						\todo{find out exact method} & X\tnote{1,8} &  \\
						\todo{find out exact method} & X\tnote{1,9} &  \\
						\todo{find out exact method} & X\tnote{1,10} &  \\
						
						\multicolumn{3}{l}{\textbf{Satisfy Fairness and Stability Under Distribution Shifts}} \\ 
						\todo{find out exact method} & X\tnote{1,11} &  \\
						
						\multicolumn{3}{l}{\textbf{Fair Representation Learning (Pre/In-processing)}} \\ 
						Representation Learning by Disentanglement & X\tnote{1,2} &   \\
						Variational Fair Autoencoder & X\tnote{1,3,15} &   \\
						VAE without adversarial training & X\tnote{1,4} &   \\
						Adversial Learning with FairGAN & X\tnote{1,16} &   \\
						Removing correlation between protected and unprotected features with a geometric solution & X\tnote{1,17} &   \\
						
						\multicolumn{3}{l}{\textbf{Algorithmic Adaptions for Fairness}} \\ 
						Modified Discrimination-Free Naive Bayes Classifier & X\tnote{1,12} &  \\
						
						\multicolumn{3}{l}{\textbf{Fairness-Aware \gls{ML} Frameworks}} \\ 
						Fairness-Aware Classification Framework & X\tnote{1,13} &  \\
						Fairness Constraints in Multitask Learning (MTL) Framework & X\tnote{1,14} &  \\
						Decoupled Classification System with Transfer Learning & X\tnote{1,15} &  \\
						
						\multicolumn{3}{l}{\textbf{Preferential Data Selection and Representation}} \\ 
						Wasserstein Distance Measure for Dependence Mitigation & X\tnote{1,16} &  \\
						Preferential Sampling (PS) for Discrimination-Free Training Data & X\tnote{1,17} &  \\
						
						\multicolumn{3}{l}{\textbf{Model Interpretability}} \\ 
						Post-Processing with Attention Mechanism & X\tnote{1,18} &  \\
						Use Brier Score and Response Rate Accuracy & & X\tnote{19, \todo{add clear source}} \\
						some more methods \todo{describe} & & X\tnote{19} \\
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[*] possible to satisfy together
							\item[**] possible to satisfy together
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M147_}
							\item[3] \autocite{M63_Hardt_2016}
							\item[4] \autocite{M2_}
							\item[5] \autocite{M159_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[6] \autocite{M154_}
							\item[7] \autocite{M57_}
							\item[8] \autocite{M78_}
							\item[9] \autocite{M85_}
							\item[10] \autocite{M106_}
							\item[11] \autocite{M69_}
							\item[12] \autocite{M25_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[13] \autocite{M155_}
							\item[14] \autocite{M12_}
							\item[15] \autocite{M49_}
							\item[16] \autocite{M73_}
							\item[17] \autocite{M75_}
							\item[18] \autocite{M102_}
							\item[19] \autocite{Young_2020}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Mitigation Methods - Fair Classification - Mentioned in Contextual Research, grouped like in \textcite{Mehrabi_2021}, the author cannot guarantee for completeness}
				\label{tab:mitigation_methods_fair_classification}
			\end{table}
			
			\todo{check categorization}
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Mitigation Methods - not so relevant for us} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%	\midrule
						\multicolumn{3}{l}{\textbf{Fair NLP}} \\ 
						Fair Word-Embedding & X\tnote{1,5,6,7} &   \\
						Train-Time Data Augmentation & X\tnote{1,8} &   \\
						Test-Time Neutralization & X\tnote{1,8} &   \\
						
						%	\midrule	
						\multicolumn{3}{l}{\textbf{Fair Regression (In-processing)}} \\ 
						Price of Fairness (POF) & X\tnote{1,10} & \\
						XY \todo{check this} and bounded group loss & X\tnote{1,11} & \\
						Decision Tree for Disparate Impact and Treatment & X\tnote{1,12} & \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Structured Prediction (In-processing)}} \\ 
						Reducing Bias Amplification (RBA) as calibration algorithm & X\tnote{1,13} & \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Principal Component Analysis (PCA) (In-processing)}} \\ 
						Fair PCA & X\tnote{1,14} & \\
						
						\multicolumn{3}{l}{\textbf{Graph-Based Fairness Methods}} \\ 
						Community Detection / Graph Embedding  \todo{how to proceed with this} & X\tnote{} & \\
						
						\multicolumn{3}{l}{\textbf{Causal Fairness and Disparate Learning}} \\ 
						Disparate Learning Processes (DLP) & X\tnote{1,9} &   \\
						Causal Approach to Fairness \todo{how to proceed with this} & X\tnote{\todo{add clear source}}  & \\
						Disregard path in causal graph which result in sensitive attributes affecting decision outcome & X\tnote{1} &   \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Removing Sensitive Attributes}} \\ 
						Disregard sensitive attributes in effect on decision-making & X\tnote{1} &   \\						
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M42_}
							\item[3] \autocite{M97_}
							\item[4] \autocite{M112_}
							\item[5] \autocite{M20_Bolukbasi_2016}
							\item[6] \autocite{M58_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[7] \autocite{M169_}
							\item[8] \autocite{M166_}
							\item[9] \autocite{M94_}
							\item[10] \autocite{M14_}
							\item[11] \autocite{M1_}
							\item[12] \autocite{M2_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[13] \autocite{M167_Zhao_2017}
							\item[14] \autocite{M137_}
							\item[15] \autocite{M5_}
							\item[16] \autocite{M90_}
							\item[17] \autocite{M65_}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Mitigation Methods - Others - Mentioned in Contextual Research, grouped like in \textcite{Mehrabi_2021}, the author cannot guarantee for completeness}
				\label{tab:mitigation_methods_others}
			\end{table}
			
			\todo{mention also the IBM AI Fairness 360 toolkit [11] and that authors evaluated their work in benchmark datasets [65], [72], [158], [159]}
			
			
			
			\todo{draft for presentation}
			satisfy Equalized Odds / Subgroup fairness
			highlight allocation concealment and blinding and data collection from diverse sources and Preferential Sampling
			\subsection{Mitigation Methods Overview}
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Mitigation Methods} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						\multicolumn{3}{l}{\bolditalic{Unbiasing Data}} \\
						Documentation and Transparency & X\tnote{1} & X\tnote{3} \\
						Bias Detection and Evaluation & X\tnote{1} & X\tnote{2,4} \\ % simpsons paradoxon, subset scanning, input pertubation
						Study Design & X\tnote{1} & X\tnote{2} \\ % allocation concealment and blinding, preventing direct and indirect discrimination
						Data Gathering & X\tnote{1} & X\tnote{3,4} \\ % data collection from diverse sources, robuster standards, 
						Data Availability and Open Science &  & X\tnote{3} \\
					    Removing Sensitive Attributes & X\tnote{1} &  \\
						\multicolumn{3}{l}{\bolditalic{Fair Classification}} \\
						Satisfy Fairness Definitions & X\tnote{1} &  \\ % satisfy Equalized Odds / Subgroup fairness
						Satisfy Fairness and Stability Under Distribution Shifts & X\tnote{1} & \\
						Fair Representation Learning & X\tnote{1} & \\
						Fairness-Aware \gls{ML} Frameworks & X\tnote{1} & \\
						Preferential Data Selection and Representation & X\tnote{1} & \\
						Model Interpretability & X\tnote{1} & X\tnote{3} \\
						\multicolumn{3}{l}{\bolditalic{For Other \gls{ML} Algorithm Types}} \\
						Fair NLP & X\tnote{1} &  \\
						Fair Regression & X\tnote{1} &  \\
						Structured Prediction & X\tnote{1} &  \\
						Fair Principal Component Analysis & X\tnote{1} &  \\
						Graph-Based Fairness Methods & X\tnote{1} &  \\
						Causal Fairness and Disparate Learning & X\tnote{1} &  \\
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{Chakraborty_2024}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[3] \autocite{Young_2020}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[4] \autocite{Montoya_2025}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Mitigation Methods - Draft}
				\label{tab:mitigation_methods_unbiasing_data_praesi}
			\end{table}
			
			\todo{check categorization}
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Mitigation Methods - not so relevant for us} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%	\midrule
						\multicolumn{3}{l}{\textbf{Fair NLP}} \\ 
						Fair Word-Embedding & X\tnote{1,5,6,7} &   \\
						Train-Time Data Augmentation & X\tnote{1,8} &   \\
						Test-Time Neutralization & X\tnote{1,8} &   \\
						
						%	\midrule	
						\multicolumn{3}{l}{\textbf{Fair Regression (In-processing)}} \\ 
						Price of Fairness (POF) & X\tnote{1,10} & \\
						XY \todo{check this} and bounded group loss & X\tnote{1,11} & \\
						Decision Tree for Disparate Impact and Treatment & X\tnote{1,12} & \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Structured Prediction (In-processing)}} \\ 
						Reducing Bias Amplification (RBA) as calibration algorithm & X\tnote{1,13} & \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Principal Component Analysis (PCA) (In-processing)}} \\ 
						Fair PCA & X\tnote{1,14} & \\
						
						\multicolumn{3}{l}{\textbf{Graph-Based Fairness Methods}} \\ 
						Community Detection / Graph Embedding  \todo{how to proceed with this} & X\tnote{} & \\
						
						\multicolumn{3}{l}{\textbf{Causal Fairness and Disparate Learning}} \\ 
						Disparate Learning Processes (DLP) & X\tnote{1,9} &   \\
						Causal Approach to Fairness \todo{how to proceed with this} & X\tnote{\todo{add clear source}}  & \\
						Disregard path in causal graph which result in sensitive attributes affecting decision outcome & X\tnote{1} &   \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Removing Sensitive Attributes}} \\ 
						Disregard sensitive attributes in effect on decision-making & X\tnote{1} &   \\						
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M42_}
							\item[3] \autocite{M97_}
							\item[4] \autocite{M112_}
							\item[5] \autocite{M20_Bolukbasi_2016}
							\item[6] \autocite{M58_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[7] \autocite{M169_}
							\item[8] \autocite{M166_}
							\item[9] \autocite{M94_}
							\item[10] \autocite{M14_}
							\item[11] \autocite{M1_}
							\item[12] \autocite{M2_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[13] \autocite{M167_Zhao_2017}
							\item[14] \autocite{M137_}
							\item[15] \autocite{M5_}
							\item[16] \autocite{M90_}
							\item[17] \autocite{M65_}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Mitigation Methods - Others - Mentioned in Contextual Research, grouped like in \textcite{Mehrabi_2021}, the author cannot guarantee for completeness}
				\label{tab:mitigation_methods_others}
			\end{table}
			
			
			
			
		\rawcitationstart
		\subsection{Mitigation Methods Extensive Sources}
			
			\paragraph{Bias Examples and Mitigation Ideas}
			Data bias examples and mitigation ideas
			\begin{itemize}
				\item Bias in \gls{ML} Data - \autocite{M24_Buolamwini_2018} IJB-A / Adience imbalanced (mainly light-skinned subjects) - Bias towards dark-skinned groups (underrepresented). Other instance - when we do not consider different subgroups in the data. Considering only male-female groups not enough, use race to further subdivide gender groups. Only then, clear biases in sub groups can be found, since otherwise part of the groups would  compromise the other group and hide the underlaying bias towards that subgroup \autocite{Mehrabi_2021}
				\rawcitationusedstart
				\item Popular machine-learning datasets that serve as a base for most of the developed algorithms and tools can also be biased—which can be harmful to the downstream applications that are based on these datasets. ... In [\autocite{M142_Shankar_2017}, researchers showed that these datasets suffer from representation bias and advocate for the need to incorporate geographic diversity and inclusion while creating such datasets. \autocite{Mehrabi_2021}
				\rawcitationusedend
				\item Examples of Data Bias in Medical Applications. These data biases can be more dangerous in other sensitive applications. For example, in medical domains there are many instances in which the data studied and used are skewed toward certain populations—which can have dangerous consequences for the underrepresented communities. [98] showed how exclusion of African-Americans resulted in their misclassification in clinical studies, so they became advocates for sequencing the genomes of diverse populations in the data to prevent harm to underrepresented populations \autocite{Mehrabi_2021} \todo{What does sequencing data mean?, is it relevant}
			\end{itemize}
			
			\paragraph{Methods for Fair Machine Learning}
			\begin{itemize}
				\item While this section is largely domain-specific, it can be useful to take a cross-domain view. Generally, methods that target biases in the algorithms fall under three categories \autocite{Mehrabi_2021}
				\item Pre-processing. Pre-processing techniques try to transform the data so that the underlying discrimination is removed [43]. If the algorithm is allowed to modify the training data, then pre-processing can be used [11].\autocite{Mehrabi_2021}
				\item In-processing. In-processing techniques try to modify and change state-of-the-art learning algorithms in order to remove discrimination during the model training process [43]. If it is allowed to change the learning procedure for a machine learning model, then in-processing can be used during the training of a model— either by incorporating changes into the objective function or imposing a constraint [11, 14].\autocite{Mehrabi_2021}
				\item Post-processing. Post-processing is performed after training by accessing a holdout set which was not involved during the training of the model [43]. If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-processing can be used in which the labels assigned by the black-box model initially get reassigned based on a function during the post-processing phase [11, 14].\autocite{Mehrabi_2021}
				\item we concentrate on discrimination prevention based on preprocessing, because the preprocessing approach seems the most flexible one: it does not require changing the standard data mining algorithms, unlike the inprocessing approach, and it allows data publishing (rather than just knowledge publishing), unlike the postprocessing approach. \autocite{M62_Hajian_2013} --> \todo{this is an important point which we should consider for PASSION, also, some more insight in regards of the different phases can be found in this paper}
				
				
				\item From learning fair representations [42, 97, 112] to learning fair word embeddings [\autocite{M20_Bolukbasi_2016}, 58, 169], debiasing methods have been proposed in different \gls{AI} applications and domains. \autocite{Mehrabi_2021} --> seems to refer mostly to NLP domains
				\item Most of these methods try to avoid unethical interference of sensitive or protected attributes into the decision-making process, while others target exclusion bias by trying to include users from sensitive groups. \autocite{Mehrabi_2021}
				\item However, a recent paper [58] argues against these debiasing techniques and states that many recent works on debiasing word embeddings have been superficial, that those techniques just hide the bias and don’t actually remove it. \autocite{Mehrabi_2021}
				\item some works try to satisfy one or more of the fairness notions in their methods, such as disparate learning processes (DLPs) which try to satisfy notions of treatment disparity and impact disparity by allowing the protected attributes during the training phase but avoiding them during prediction time [94].\autocite{Mehrabi_2021}
				\item Some of the existing work tries to treat sensitive attributes as noise to disregard their effect on decision-making, while some causal methods use causal graphs, and disregard some paths in the causal graph that result in sensitive attributes affecting the outcome of the decision.\autocite{Mehrabi_2021}
				\item Different bias-mitigating methods and techniques are discussed below for different domains—each targeting a different problem in different areas of machine learning in detail. \autocite{Mehrabi_2021}
			\end{itemize}
			
			\subparagraph{Unbiasing Data}
				\begin{itemize}
					\item Every dataset is the result of several design decisions made by the data curator. Those decisions have consequences for the fairness of the resulting dataset, which in turn affects the resulting algorithms. In order to mitigate the effects of bias in data, some general methods have been proposed that advocate having good practices while using data, such as having datasheets that would act like a supporting document for the data reporting the dataset creation method, its characteristics, motivations, and its skews [13, 55]. A similar suggestion has been proposed for models in [110].\autocite{Mehrabi_2021}
					\item Authors in [66] also propose having labels, just like nutrition labels on food, in order to better categorize each data for each task. \autocite{Mehrabi_2021}
					\item some work has targeted more specific types of biases. For example, [81] has proposed methods to test for cases of Simpson’s paradox in the data, and [3, 4] proposed methods to discover Simpson’s paradoxes in data automatically. \autocite{Mehrabi_2021}
					\item Causal models and graphs were also used in some work to detect direct discrimination in the data along with its prevention technique that modifies the data such that the predictions would be absent from direct discrimination [163].\autocite{Mehrabi_2021}
					\item in [\autocite{M62_Hajian_2013}] also worked on preventing discrimination in data mining, targeting direct, indirect, and simultaneous effects.\autocite{Mehrabi_2021}
					\item Other pre-processing approaches, such as messaging [74], preferential sampling [75, 76], disparate impact removal [51], also aim to remove biases from the data. \autocite{Mehrabi_2021}
					
					
					\item Image quality. Several barriers to \gls{AI} implementation in the clinic need to be overcome with regards to imaging (Figure 1). These include technical variations (e.g., camera hardware and software) and differences in image acquisition and quality (e.g., zoom level, focus, lighting, and presence of hair). For example, the presence of surgical ink markings is associated with decreased specificity (Winkler et al., 2019), field of view can significantly affect prediction quality (Mishra et al., 2019), and classification performance improves when hair and rulers are removed (Bisla et al., 2019). We have developed a method to measure how model predictions might be biased by the presence of a visual artifact (e.g., ink) and proposed methods to reduce such biases (Pfau et al., 2019). Poor quality images are often excluded from studies, but the problem of what makes an image adequate is not well studied. Ideally, models need to be able to express a level of confidence in a prediction as a function of image quality and appropriately direct a user to retake photos if needed. \autocite{Young_2020} - dermatology
				\end{itemize}
			
			\subparagraph{Fair Classification}
				\begin{itemize}
					\item certain methods have been proposed [57, 78, 85, 106] that satisfy certain definitions of fairness in classification. For instance, in [147] authors try to satisfy subgroup fairness in classification, equality of opportunity and equalized odds in [63], both disparate treatment and disparate impact in [2, 159], and equalized odds in [154]. \autocite{Mehrabi_2021}
					\item Other methods try to not only satisfy some fairness constraints but to also be stable toward change in the test set [69] \autocite{Mehrabi_2021}
					\item The authors in [155], propose a general framework for learning fair classifiers. This framework can be used for formulating fairness-aware classification with fairness guarantees.
					In another work [25], authors propose three different modifications to the existing Naive Bayes classifier for discrimination-free classification.\autocite{Mehrabi_2021}
					\item paper [122] takes a new approach into fair classification by imposing fairness constraints into a Multitask learning (MTL) framework. In addition to imposing fairness during training, this approach can benefit the minority groups by focusing on maximizing the average accuracy of each group as opposed to maximizing the accuracy as a whole without attention to accuracy across different groups. In a similar work [49], authors propose a decoupled classification system where a separate classifier is learned for each group. They use transfer learning to reduce the issue of having less data for minority groups.\autocite{Mehrabi_2021}
					\item In [73] authors propose to achieve fair classification by mitigating the dependence of the classification outcome on the sensitive attributes by utilizing the Wasserstein distance measure.\autocite{Mehrabi_2021}
					\item In [75] authors propose the Preferential Sampling (PS) method to create a discrimination free train data set. They then learn a classifier on this discrimination free dataset to have a classifier with no discrimination.\autocite{Mehrabi_2021}
					\item In [102], authors propose a post-processing bias mitigation strategy that utilizes attention mechanism for classification and that can provide interpretability. \autocite{Mehrabi_2021}
				\end{itemize}
				
			\subparagraph{Fair Regression}
				\todo{only summarize briefly, as PASSION is a classification and not a regression task}
				\begin{itemize}
					\item “price of fairness” (POF) to measure accuracy-fairness trade-offs, 3 penalites: Individual fairness, group fairness and hybrid fairness [14] \autocite{Mehrabi_2021}
					\item In addition to the previous work, [1] considers the fair regression problem formulation with regards to two notions of fairness statistical (demographic) parity and bounded group loss. [2] uses decision trees to satisfy disparate impact and treatment in regression tasks in addition to classification. \autocite{Mehrabi_2021}
				\end{itemize}
			\subparagraph{Structured Prediction}
				\todo{only summarize briefly, as PASSION is a classification task}
				\begin{itemize}
					\item RBA (reducing bias amplification) as calibration algorithm to prevent risk of leveraging social bias, distributions in training data are followed in the predictions. multi-label obeject and visual semantic role labeling classification amplify existing bias in data [\autocite{M167_Zhao_2017}] \autocite{Mehrabi_2021} --> \todo{be careful with this if the approach would be to generate new images for training!!}
				\end{itemize}
			\subparagraph{Fair PCA}
				\todo{only summarize briefly, as PASSION is a classification task with only like 10 features}
				\begin{itemize}
					\item Pincipal Component Analysis (PCA) https://www.geeksforgeeks.org/principal-component-analysis-pca/ --> dimensionality reduction, statistical technic, high-dimensional data into lower-dimensional space while maximising variance in new space -> most important patterns and relationships is preserved
					\item vanilla PCA exaggerate error in reconstruction for one group of people [137] \autocite{Mehrabi_2021}
					\item And their proposed algorithm is a two-step process listed below: (1) Relax the Fair PCA objective to a semidefinite program (SDP) and solve it. (2) Solve a linear program that would reduce the rank of the solution. [137] \autocite{Mehrabi_2021}
				\end{itemize}
			\subparagraph{Community Detection}
				\todo{use this as an example for out of scope text, - Ludovic approved}
				Community detection algorithms are specifically tailored to analyze network data and find connections in such datasets. For example, they can be used to detect groups of people with similar interest in social networks \autocite{Jayawickrama_2021}. This kind of data is not found in the context of PASSION, which is a classification task. Please refer to \textcite{Mehrabi_2021} for more information on bias mitigation in community detection algorithms.
				
			\subparagraph{Causal Approach to Fairness}
				\todo{only relevant, if our variables have a dependency on the variables, e.g. age / gender determines how the disease is presenting itself in the images; check \autocite{Mehrabi_2021} page 18 if relevant}
				
			\subparagraph{Fair Representation Learning}
				https://medium.com/superlinear-eu-blog/representation-learning-breakthroughs-what-is-representation-learning-5dda2e2fed2e
				\begin{itemize}
					\item Variational Auto encoders --> Variational Fair Autoencoder introduced in [97]. Here,they treat the sensitive variable as the nuisance variable, so that by removing the information about this variable they will get a fair representation. They use a maximum mean discrepancy regularizer to obtain invariance in the posterior distribution over latent variables. Adding this maximum mean discrepancy (MMD) penalty into the lower bound of their VAE architecture satisfies their proposed model for having the Variational Fair Autoencoder. \newline
					In [5] authors also propose a debiased VAE architecture called DB-VAE which learns sensitive latent variables that can bias the model (e.g., skin tone, gender, etc.) and propose an algorithm on top of this DB-VAE using these latent variables to debias systems like facial detection systems. \newline
					In [112] authors model their representation-learning task as an optimization objective that would minimize the loss of the mutual information between the encoding and the sensitive variable. The relaxed version of this assumption is shown in Equation 1. They use this in order to learn fair representation and show that adversarial training is unnecessary and in some cases even counter-productive. \newline
					In [42], authors introduce flexibly fair representation learning by disentanglement that disentangles information from multiple sensitive attributes. Their flexible and fair variational autoencoder is not only flexible with respect to downstream task labels but also flexible with respect to sensitive attributes. They address the demographic parity notion of fairness, which can target multiple sensitive attributes or any subset combination of them. \autocite{Mehrabi_2021}
					\item Adversarial Learning - In [90] authors present a framework to mitigate bias in models learned from data with stereotypical associations. using adversarial networks by introducing FairGAN which generates synthetic data that is free from discrimination and is similar to the real data. They use their newly generated synthetic data from FairGAN, which is now debiased, instead of the real data for training and testing. They do not try to remove discrimination from the dataset, unlike many of the existing approaches, but instead generate new datasets similar to the real one which is debiased and preserves good data utility. \autocite{Mehrabi_2021} \todo{address challenges in creating synthetic data in dermatology?}
				\end{itemize}
			
			\subparagraph{Fair NLP}
				\todo{for PASSION irrelevant, if it wants to stick to ResNet50 Architecture \autocite{Gottfrois2024} and not use Visual Encoders, which would make sense bc of the small dataset}
				\begin{itemize}
					\item Word Embedding \todo{potentially relevant, if the labels are used in training, e.g. age / gender determines how the disease is presenting itself in the images; check \autocite{Mehrabi_2021} page 21 if relevant}
					\item Coreference Resolution "Coreference resolution involves identifying when two or more expressions in a text refer to the same entity, be it a person, place, or thing." https://medium.com/@datailm/the-key-to-unlocking-true-language-understanding-coreference-resolution-c01d569e2e87 \todo{irrelevant for the PASSION Context}
				\end{itemize}
				
			\paragraph{comparison of different mitigation algorithms}
				\begin{itemize}
					\item The field of algorithmic fairness is a relatively new area of research and work still needs to be done for its improvement. With that being said, there are already papers that propose fair \gls{AI} algorithms and bias mitigation techniques and compare different mitigation algorithms using different benchmark datasets in the fairness domain. For instance, authors in [65] propose a geometric solution to learn fair representations that removes correlation between protected and unprotected features. The proposed approach can control the trade-off between fairness and accuracy via an adjustable parameter. In this work, authors evaluate the performance of their approach on different benchmark datasets, such as COMPAS, Adult and German, and compare them against various different approaches for fair learning algorithms considering fairness and accuracy measures [65, 72, 158, 159]. In addition, IBM’s \gls{AI} Fairness 360 (AIF360) toolkit [11] has implemented many of the current fair learning algorithms and has demonstrated some of the results as demos which can be utilized by interested users to compare different methods with regards to different fairness measures. \autocite{Mehrabi_2021}
				\end{itemize}			
			
		\subsection{Statistical biases}
			https://data36.com/statistical-bias-types-explained/
			\begin{itemize}
				\item 
			\end{itemize}	
	
		\subsection{Dermatology Bias}
			\begin{itemize}
				\item https://ijdvl.com/biases-in-dermatology-a-primer/ 29 biases, 4 reasons to know about it, 7 mitigation methods \autocite{Chakraborty_2024} - dermatology
				
				\item A recent study reported mean top-1 and top-5 model accuracy of 44.8\% and 78.1\%, respectively, for the classification of 134 diseases (Han et al., 2019b). Most datasets are proprietary, often with minimal description, and datasets collected in dermatology clinics may be skewed toward more complex cases, to those patients with better access to care, or by the choice of camera used in one clinic versus another. Data should be collected from as many diverse sources as possible, including primary care clinics, and robust standards for external validation are needed. \autocite{Young_2020}
				\item There have been successful efforts to support reproducibility and open access. For example, the study by Han et al. (2018a) details the number and characteristics of images from each data source and makes thumbnails of the images publicly available. Additionally, several studies classifying dermoscopic images use the publicly available International Skin Imaging Collaboration archive (Gutman et al., 2016). By making datasets public, it becomes possible to examine them for bias (Bissoto et al., 2019). Alternatively, reporting a model training database’s patient demographics and disease classes would be helpful in predicting model performance on external populations. \autocite{Young_2020}
				\item Metrics of model performance. Standard metrics are needed to assess the performance of different models (Figure 1). Currently, standard performance metrics such as accuracy and area under the receiver operating characteristic and precision recall curves are routinely reported. However, for use in the clinic, studies should additionally describe how well their models deal with uncertainty by reporting (i) the Brier Score, or mean-squared calibration error (Rufibach, 2010), which measures how reliably a model can forecast its accuracy, and (ii) area under the response rate accuracy curve, which measures how capably a model can identify examples it is likely to predict falsely and thus abstain from predicting (Hendrycks et al., 2019) \autocite{Young_2020}
				\item Model interpretability. Acceptance of \gls{AI} in clinical decision-making hinges on being able to understand the decisionmaking process fundamental to its predictions. DL models are inherently difficult to interpret because they are complex, routinely containing millions of learned parameters; interpretation of DL models’ output is an active field of research (Murdoch et al., 2019). One approach for interpreting model diagnoses is contentbased image retrieval, a method for retrieving training images that are visually similar to a test image (Tschandl et al., 2019a). This method may reassure the physician if all the retrieved training images have the same diagnosis as the predicted diagnosis but is less helpful if the test image looks similar to two or more training images with conflicting diagnoses. A second approach is to highlight pixels in an image most relevant for a model’s prediction, using methods such as saliency mapping (Figure 1). However, it is often the case that highlighted pixels correspond to the entire lesion or visually distinctive features that are already obvious to clinicians without indication as to why these pixels are important to the diagnosis. A third approach is to see through the eyes of a model by plotting an activation atlas (Carter et al., 2019), which shows how subtle changes, in particular visual features, may tip the model over into choosing one diagnosis over another. These activation atlases are experimental and have yet to be applied in dermatology. Understanding a model’s predictions and how the prediction is applicable to the patient at hand is necessary to build trust. As \gls{AI} exceeds human performance in various tasks, interpreting models may help to advance scientific knowledge by understanding what the machine sees that is relevant to its predications \autocite{Young_2020}
			\end{itemize}
		\subsubsection{Demographic Bias in Dermatology}
		\paragraph{fairness melanoma detection}
		\begin{itemize}
			\item Some biases can be easily detected and countered, such as through appropriate data curation; for example, having a balanced representation across skin tones and genders in training sets. However, in other cases, biases are hidden and untraceable [9]. \autocite{Montoya_2025}
			\item whether information on demographic diversity (age, gender, race, or ethnicity of patients), clinical diversity (skin type, lesion type, anatomical location of lesion), or image characteristics (source, imaging techniques, resolution, and whether the images were real or artificially generated) \autocite{Montoya_2025}
			\item The most popular skin color scale currently being used for data annotation for image recognition techniques is the Fitzpatrick Skin Tone Scale (FST) [10]which has six skin tones. Dating from the 1970s, it originally featured just 4 light tones and was designed for detecting photo sensitivity for white skin, with two darker tones added later [11]. The Monk Skin Scale was recently developed and still needs testing, but promisingly has 10 tones, 5 light and 5 dark [12].\autocite{Montoya_2025} \todo{highlight this (FST alternatives)}
			\item Fig. 4. Comparison of skin tone scales that can be used for skin cancer detection utilizing \gls{AI}. Recreation of fitzpatrick skin type scale, monk skin tone scale, and sampling of L’Oreal color chart map for reference. \autocite{Montoya_2025} \todo{include this figure}
			\item While this systemic review provides a comprehensive review of the literature on fairness in \gls{AI} for melanoma detection, it is primarily based on existing research. To validate the proposed recommendations or frameworks, continuing work is necessary to complete empirical analysis and experiments. Additionally, the suggested adoption of new skin tone scales, while beneficial, may face practical challenges in implementation. Furthermore, while the paper strongly advocates for specific skin tone scales, it’s important to note that other methods or tools might also effectively address fairness issues in \gls{AI} for melanoma detection. Finally, while the study addresses fairness in \gls{AI}, it could benefit from further exploration of the practical implementation of these recommendations in real-world clinical settings. Potential obstacles and the feasibility of widespread adoption should be considered to ensure that the proposed solutions are not only theoretically sound but also practically viable. \autocite{Montoya_2025} \todo{also mention the limitations regarding FST alternatives}
			\item Recent research [13] adds another axis, skin hue, which is described as ranging from red to yellow. This offers a more complete representation of variations of skin color by providing a multidimensional scale [13]. \autocite{Montoya_2025}
			\item The effect of hue (blue, red, yellow, green) on skin tones adds depth to each face producing a range of undertones (cold, neutral, warm, and olive). In the realm of color theory, the concept of ‘contrast of hue’ emphasizes the distinctiveness among fundamental colors, with primary hues like yellow, red, and blue exhibiting the most pronounced differences [14]. Because skin cancer appears differently on different colored skin, it is important to acknowledge a full range of colors present in both healthy skin and suspicious lesions within datasets used to train skin cancer detection \gls{ML} tools. \autocite{Montoya_2025}
			\item These findings should correlate to \gls{AI} for melanoma detection since the contrast between skin color and skin lesions is a preliminary marker during feature extraction. Although the Fitzpatrick Skin Tone (FST) \gls{FST} measurement scale is not diverse enough and leads to biased \gls{AI} tools, it is continually used and has even been used to test a recently FDA-approved \gls{AI} device for detecting melanoma. \autocite{Montoya_2025}
			\item We advocate for the adoption of improved scales like the Monk and L’Oreal maps. Future studies should ensure equitable representation and testing across skin tones to guarantee \gls{AI}’s effectiveness for all. Please refer to Tables 2 through 7 in the discussion section for further recommendations for curating a diverse dataset, including purpose, ownership, funding, and data annotation, as well as recommendations for each stage of the data life cycle. \autocite{Montoya_2025} \todo{Link for further mitigation methods}
			\item This study found that while using skin tone instead of race for fairness evaluations in computer vision seems objective, the annotation process remains biased by human annotators. Untested scales, unclear procedures, and a lack of awareness about annotator backgrounds and social context significantly influence skin tone labeling. This study exposes how even minor design choices in the annotation process, like scale order (dark to light instead of light to dark) or image context (face or no face, skin lesion presence), can sway agreement and introduce uncertainty in skin tone assessments. ... The researchers emphasize the need for greater transparency, standardized procedures, and careful consideration of annotator biases to mitigate these challenges and ensure fairer and more robust evaluations in computer vision. \autocite{Montoya_2025} - demographic dermatology bias
		\end{itemize}
	\rawcitationend
	
	
	\chapter{Ideas and Concepts}
		\baaCriteria{Hier geht es um die Fragestellung, wie Sie die formulierten Ziele der Arbeit erreichen wollen. Sie halten z.B. erste, grobe Ideen, skizzenhafte Lösungsansätze fest. Gibt es mehrere Wege, Ansätze um dieses Ziel zu erreichen, begründen Sie hier, warum Sie einen bestimmten Weg einschlagen. Beispiel für ein Softwareprojekt: Erste Gedanken über eine grobe Systemarchitektur. Ist z.B. eine Microservice-Architektur angebracht? Welche Alternativen bestehen, wo gibt es Problempunkte? Die Umsetzung, die Beurteilung der Machbarkeit und die detaillierte Beschreibung der umgesetzten Architektur sind dann Teil der Realisierung.}
					
		This chapter outlines initial thoughts and conceptual considerations for addressing potential biases in the PASSION project. It sketches the general methodology used in this thesis.
		
		\section{Broad Methodology}
			The evaluation and mitigation of bias in the PASSION model is planned to consists of four stages:
			\begin{enumerate}
				\item \textbf{Literature Review.} A literature review will be conducted to get an overview of what biases, fairness metrics and mitigation strategies are known in medical \gls{AI}.
				
				\item \textbf{Contextualization and Scope Definition.} The findings' relevance for PASSION's \gls{teledermatology} context will be evaluated. Based on this, relevant types of bias, applicable fairness metrics and mitigation methods will be selected. Aspects not feasible to address within the scope of this thesis will documented for future work.
				
				\item \textbf{Baseline Fairness Assessment.} The current PASSION model will be evaluated using the selected fairness metrics. This will provide a baseline for comparison after mitigation methods are applied.
				
				\item \textbf{Mitigation and Evaluation.} Selected mitigation strategies will be implementend individually. Their effect on model fairness and performance will assessed relative to the baseline.
			\end{enumerate}
		
		\section{PASSION Dataset Assessment}
			In order to decide about the scope and feasibility of the findings in the literature review, the dataset must be assessed.
			The PASSION dataset was created to improve the representation of highly pigmented skin, which is underrepresented in many traditional dermatology datasets. Nevertheless, it may still lack adequate representation of specific subgroups. Such gaps in representativeness could potentially lead to biased model outputs. However, as \textcite{Mehrabi_2021} states, this is not necessarily the case. Therefore, a detailed assessed for representativeness can be postponed until the model output indeed proofs to be biased.
			
			Furthermore, the available metadata determines which biases can identified and what mitigation methods are possible. E.g., if metadata on age is missing, fairness with respect to age cannot be assessed.
			
			Therefore, the dataset will be reviewed with regards to:
			\begin{itemize}
				\item Representation of the main groups to get a first impression
				\item Representation of relevant subgroups if the model output proves to be biased
				\item Completeness of metadata relevant for fairness evaluation
				\item Presence of \glspl{proxyVar} that might complicate fairness assessments
			\end{itemize}
			
			These aspects will help determine the extent to which the dataset supports meaningful fairness analysis and subgroup-level model evaluation. It also provides guidance on how to potentially adapt the dataset in the future.
		
			
%		\begin{comment}
%			\section{some general mitigation method ideas}
%				\todo{add infos from the midterm presentation}
%			
%				\todo{write things to consider more precisely:}
%				\begin{itemize}
%					\item Divide and Conquer vs. All-In-One-Model
%					\begin{itemize}
%						 \item An algorithm per ethnicity / subgroup running at the same time
%						\item Running 1 Algorithm chosen based on Fitzpatrick skin type
%						\item Running 1 Algorithm which detects first the demographic subgroup (\gls{FST}, gender, age, …) and runs the specific subgroup algorithm afterwards
%						\item Hint Ludovic: Still not of data, maybe also others; often limited because the data is missing, you are missing data from others
%					\end{itemize}
%					\item BLIND performance vs. Including the demographic data
%					\begin{itemize}
%						\item Idea to try if the labels are not relevant for the diagnosis and should only be used for evaluating fairness purposes as some papers suggest 
%						\item Might be obsolete after demographic biases in dermatology research, since melanin response and melanoma risk is different in male and female according to research https://pmc.ncbi.nlm.nih.gov/articles/PMC4797181/
%					\end{itemize}
%					\item Hint Ludovic: Maybe Focal Loss more relevant --> emphasis on data vs. model
%				\end{itemize}
%				\begin{itemize}
%					\item Divide and Conquer vs. All-In-One-Model (either by etnicity x algorithms at a time or one which seperates the imgs first by demographic subgroup (incl. Fitzpatrick skin type))
%					\item BLIND performance vs. Including the demographic data
%				\end{itemize}
%		
%		\end{comment}	
	\chapter{Methods}\label{chap:methodology}
		\baaCriteria{Hier halten Sie fest und begründen, welches Vorgehensmodell Sie für Ihr Projekt wählen. Sie verweisen allenfalls auf die daraus entstandenen, konkreten Terminpläne mit Meilensteinen, welche z.B. unter Realisierung (Kapitel 5) oder im Anhang versorgt sind. Bei Projekten mit einer verlangten wissenschaftlichen Tiefe werden hier die geplanten Forschungsmethoden wie quantitative/qualitative Interviews, Befragungen, Beobachtungen, Feldexperiment etc. beschrieben und begründet. Warum ist in Ihrer Situation ein Interview besser als eine Umfrage? Wer soll interview werden?}
		\baaCriteria{Die gewählten Methoden sind nachvollziehbar und begründet. Eine methodische Übersicht (Methodisches BigPicture) wurde aufgezeigt und Abgrenzungen erläutert.}
		
		
		This chapter describes the methodological approach and project organization used in this thesis. It outlines the selected process model, planned research methods, and relevant conditions. The focus lies on ensuring that the chosen methods are appropriate, transparent, and justified in the context of evaluating and mitigating bias in the PASSION project.
		
		\section{Project Management}
		This chapter illustrates the used process model, how the progress and risk are managed and what technical constraints are available, to get a sense of the constraints and the general plan of this thesis. 
		
		\subsection{Process Model}
		The project follows the waterfall model. This means the work is done sequentially and each sequence is based on the one before \autocite{Petersen_2009}. This model has been chosen for the project, since it provides a solid base for the main project while keeping the project management overhead small.
		This project is separated in two phases:
		
		\textbf{Phase 1 – Literature Review and Methodology Planning.} This phase includes the literature review, the selection and justification of fairness metrics and bias mitigation techniques, and the assessment of the dataset's structure and limitations. Based on these results, a detailed plan for the second phase is developed.
		
		\textbf{Phase 2 – Execution and Evaluation.} In the second phase, the planned assessments and mitigation strategies are implemented. The PASSION model is evaluated against the selected fairness metrics, and improvements are measured and discussed.
		
		The detailed project plan is included in the provided zip-file.
		
		\subsection{Progress Monitoring and Risk Management}
		To ensure project transparency and timely delivery, bi-weekly status meetings with the advisor are scheduled. Each meeting is prepared beforehand. Discussed are:
		\begin{itemize}
			\item Work completed in the last period
			\item Planned work for the next period
			\item Current project status and comparison with planned schedule
			\item Top three project risks and planned mitigation strategies
		\end{itemize}
		
		Meeting protocols, including the risk reports are included in the appendix. \todo{add to appendix}
		
		\subsection{Technical Constraints}
		Model training is performed on \gls{HSLU}'s \gls{gpuhub} infrastructure, while code development is carried out on a personal notebook. The code is written in Python and builds upon the existing PASSION project architecture.
		The code base for this thesis is a fork of the PASSION GitHub Project.
		\begin{itemize}
			\item Original Project: \href{https://github.com/Digital-Dermatology/PASSION-Evaluation}{https://github.com/Digital-Dermatology/PASSION-Evaluation}
			\item Fork: \href{https://github.com/teshi24/PASSION-Bias-Evaluation}{https://github.com/teshi24/PASSION-Bias-Evaluation}
		\end{itemize}
		
		
		\section{Literature Review}
		The literature review targets known bias types, fairness metrics, and mitigation techniques in medical \gls{AI}, with special attention to \gls{teledermatology} and demographic factors. Sources include scientific publications, surveys, and technical documentation of relevant libraries. The goal is to build a conceptual and methodological foundation for subsequent analysis.
		
		To ensure the thesis follows scientific standards while still being feasible, the literature review is conducted based on the pragmatic method of \textcite{Alake_2021} as suggested by my advisor. First, the focus is on survey and taxonomy papers, which provide an overview over the existing research. Them, more detailed papers in the area of dermatology \gls{AI} is conducted to get more insight in the healthcare context. Such a 2-step approach has also been done by \textcite{Chen_2024}. In general, the papers are filtered by focusing on title, abstract and conclusion. Only relevant papers are read in full. \todo{cite protocol in appendix, week1}
		
		\section{Contextualization and Scope Definition} \label{chap:contextMethod}
		The relevance of the literature findings is evaluated in the context of the PASSION project. This includes analyze the findings from the literature review in terms of their relevance to \gls{teledermatology} and similar healthcare applications, taking into account the available metadata in the PASSION dataset. Limitations due to dataset constraints or the available time are documented for future work.
		
		The relevance will be categorized into the following groups:
		\begin{itemize}
			\item \textbf{High.} Directly applicable to PASSION, both in terms of the \gls{teledermatology} setting and available metadata; likely to provide valuable insights or improvements. 
			\item \textbf{Medium.} Generally relevant to diagnostic \gls{AI}, but requires adaptations of the PASSION metadata or project in general to be feasible.
			\item \textbf{Low.} Related to PASSION, but only limited.
			\item \textbf{Not Applicable.} Not relevant for PASSION due to fundamental differences in domain, type of data, or type of model.
		\end{itemize}
		
		Based on this contextual analysis, the highly relevant bias types and mitigation methods are investigated further using the most relevant fairness metrics. The selection process follows domain-specific requirements identified in the literature. Such considerations guide the identification of suitable metrics, which are then justified and evaluated in detail during the execution phase.
		
		This contextual analysis is important, as the context and application of fairness metrics and as well as the effect and therefore importance of potential biases can vary by the use case of the \gls{AI} application \autocite{Mehrabi_2021,Barr_2025}.
		
		\section{PASSION Dataset Assessment}\label{chap:datasetAssessmentMethod}
		The assessment of the PASSION dataset focuses on four core areas:
		
		\begin{itemize}
			\item \textbf{Metadata Completeness.} The metadata is reviewed to verify that all relevant demographic attributes, as identified in the contextualized literature review, are included. Missing attributes limit bias detection and mitigation strategies. They should be added to enable a thorough fairness analysis and bias mitigation. Therefore, potentially missing attributes are listed and passed on to the PASSION team for inclusion in the metadata.
			
			Further, the available sensitive attributes are identified to ensure that they are included in the subgroup fairness evaluation.
			
			\item \textbf{Presence of \glslink{proxyVar}{Proxy Variables}.} Available metadata attributes are assessed regarding their intended purpose and potential use as \glspl{proxyVar}. If \glspl{proxyVar} are identified, alternatives are proposed to be added to the data instead. This step is essential, as relying on \glspl{proxyVar} may introduce unintended bias into the analysis or model.
			
			\item \textbf{Representation of Main Groups.} To evaluate overall demographic distributions, the proportions of the values for each demographic attribute (age, sex, \gls{FST}) are analyzed to identify over- or underrepresented groups. This provides an initial indication of potential data skews, which then can be compared to the model's fairness assessment results. This grants first insight into whether potential unfairness stems from representation bias or other factors.
			
			\item \textbf{Representation of Relevant Subgroups.}
			If the fairness assessment of model outputs reveals unfairness on subgroup levels, the distribution of the subgroups is examined using the same method as for the main groups. As this is a more detailed analysis than the representation of main groups, it is done later in the process if biases regarding subgroups in the model indeed exist.
		\end{itemize}
		\todo{cite methods}
		
		\section{Reproducing PASSION Results}
		Before starting any evaluation on the model, the PASSION experiments must be reproduced on the \gls{gpuhub}, to ensure, that the code base and the data loading is working the same way as for the initial paper. Only then, the evaluation outcome can be used by the PASSION team.
		
				
		\section{Fairness Assessment}\label{chap:fairnessAssessmentMethod}
		To establish a reproducible foundation for fairness evaluations within the PASSION project, a baseline fairness assessment with the original project setup is done. For the assessment, the fairness metric selected in \autoref{chap:ContextFairnessMetrics} is implemented to analyze model performance across sensitive subgroups, to identify any potential biases in the model output.
		
		The same fairness assessment process is used to evaluate fairness on the model after applying each mitigation method. This ensures consistency and comparability of results across all experimental stages. A mitigation method is considered to hold potential if it significantly improves the fairness assessment results compared to the established baseline.
		
		The fairness is assessed on a subgroup level. For its computation, the \gls{Fairlearn} library is used where supported to use the standard implementation. Since \gls{Fairlearn} does not support multiclass analysis and multiple subgroup combinations out of the box, custom code must be developed to handle that part.
		The subgroups are defined by all unique combinations of the sensitive PASSION metadata attributes as evaluated using the method in \linkchap{chap:datasetAssessmentMethod}.
		
		The assessment is run based on the prediction outputs and linked metadata generated in the model evaluation phase, which are cached for later inspection. An independent evaluation class computes the required statistics, and reports fairness metrics. This implementation allows evaluation to be performed independently of model training and supports reproducibility of results.
		
		Alongside with \gls{Fairlearn}, the implementation builds upon \textit{pandas} and \textit{numpy} for data handling, and \textit{scikit-learn} for standard evaluation metrics.
		
		\subsection{Limitations}
			This method provides an initial understanding of fairness in the model output and potential mitigation impacts. However, for scientifically robust conclusions on the fairness impact of a mitigation method, more systematical testing is required.
			
			Ideally, multiple training and evaluation runs per mitigation method using different random seeds should be conducted. Also, the baseline assessment should be run multiple times, using the same seeds to ensure comparison. This approach ensures statistically significant results and accounts for variance due to randomization at diverse stages in the model training process. For instance, \textcite{Valentim_2019} ran each configuration 30 times using different random seeds.
			
			Due to technical limitations and time constraints, multiple runs were not feasible during this thesis. It is strongly recommended that the PASSION team executes the experiments with additional seeds using the provided scripts, to get a more established result.
		
		\section{Mitigation Method Evaluation} \label{chap:mitigationMethodsApplyMethod}
		The PASSION model uses a predefined train-test split. To prevent test set leakage and overfitting while applying mitigation methods, the training data is further divided into a training and a validation set. 
		
		If a mitigation method can be applied in multiple ways (e.g., with different parameters, configurations, or data splits), all these variants are evaluated using the train-validation split to prevent test data leakage. The training for all variants will be done without 5-fold cross-validation which allows for significantly faster iteration cycles. This is crucial given the time limitation for this thesis. The variant that performs best on the validation set is then used to evaluate the effectiveness of the mitigation method on the original test set. For this final assessment, 5-fold cross-validation, as setup by \textcite{Gottfrois2024}, is used again.
		
		This approach ensures that the final test results are comparable across different methods, while keeping the selection process short and independent of the test data.\todo{cite AI lectures}
		\begin{comment}
			AI lectures or textbook, e.g., Goodfellow et al., 2016
			Goodfellow, I., Bengio, Y., Courville, A. (2016). Deep Learning. MIT Press.
			Or a standard practice guideline like:
			Varma, S., Simon, R. (2006). Bias in error estimation when using cross-validation for model selection. BMC Bioinformatics, 7(1), 91.
		\end{comment}
		
		Selected bias mitigation strategies are applied to this setup individually, so that the impact on the fairness can be clearly assigned to the tested mitigation strategy. The impact is evaluated relative to the established baseline as described in \linkchap{chap:fairnessAssessmentMethod}.
		
		To get insight on how the mitigation method influences model performance, also the performance should be compared to the baseline.
		
		\section{Stratified Split Experiment}
		Stratified splitting is a bias mitigation method commonly applied using the target labels to ensure a balanced representation of classes. However, additional variables can also be included to maintain minority subgroup representation across train and test sets \autocite{Balde_2023}.
		
		This experiment investigates how different stratification strategies affect model fairness. While the PASSION dataset includes a predefined training-test split, the stratification criteria used are undocumented. To approximate the original criteria, the distribution of key attributes is analyzed across the original train and test sets, to get a better understanding of the baseline used.
		
		To maintain comparability with the baseline, the original test set is preserved. The training set is re-split using various stratification configurations. All splits include the target labels, and additional attributes based on known representation disparities are incorporated. A purely random split serves as a control configuration.
		
		Splits are generated using \texttt{sklearn.model\_selection.train\_test\_split} with the \texttt{stratify} parameter. The general evaluation follows the procedure described in \autoref{chap:mitigationMethodsApplyMethod}.
		
	\chapter{Execution}
		\baaCriteria{Dies ist das Hauptkapitel Ihrer Arbeit! Hier wird die Umsetzung der eigenen Ideen und Konzepte (Kapitel 3) anhand der gewählten Methoden (Kapitel 4) beschrieben, inkl. der dabei aufgetretenen Schwierigkeiten und Einschränkungen.}
		\baaCriteria{Die gewählten Methoden werden systematisch, konsistent und korrekt auf den Kontext der Arbeit angewendet. Die Bearbeitungs- bzw. Forschungsobjekte sind einheitlich benannt, im Kontext dargestellt und sinnvoll in die Arbeit integriert. Praxis- und Erfahrungswissen (z.B. aus Interviews) wird zur Validierung und Ergänzung der erarbeiteten Ergebnisse herangezogen. }
		
		\section{Contextualization and Scope Definition}
		This section applies the information found during the literature review to the PASSION project using the method described in \autoref{chap:contextMethod}. It also scopes what information can be assessed during this thesis and what should be passed on to the PASSION team.
		
		\subsection{Bias}
			
		\subsection{Sensitive Features}
			Some of the listed features in \autoref{tab:biases_features} were also mentioned in the dermatology context and/or are included as metadata in the PASSION dataset. Therefore, potential biases associated with them should be evaluated in the PASSION model.
			
			Since PASSION aims to improve classification of skin diseases based solely on image data without any metadata, it does not use these factors as features for training, except for characteristics that are implicitly visible in the images. This is primarily the \textit{skin type} (including the undertone). More broadly defined, the \textit{socioeconomic status} and \textit{geographic location} can also be leaked to the model through the images, due to their impact on disease presentation and progression. Since the model can access these characteristics during training, they can introduce bias and should therefore be closely examined.
			
			\textit{Age} and \textit{sex} are generally not visible in the images. Also, \textit{socioeconomic status} and \textit{geographic location} do not necessarily need to lead to visual effects. However, since they can influence disease prevalence and are prone to bias, the PASSION model should be evaluated for potential bias regarding these characteristics.
			
			The potential impact of \textit{ethnicity} and \textit{disabilities} on visual presentation or prevalence of dermatological conditions has not been assessed in this thesis, due to time constraints. It is recommended that the PASSION team investigates these aspects further.
			
			The other sensitive feature seem not to be further relevant for PASSION.
		
		\subsection{Fairness Metrics} \label{chap:ContextFairnessMetrics}
		This chapter focuses on those fairness metrics which are able to evaluate demographic fairness and are applicable to the dermatology context of PASSION.	Those are mainly \textit{equalized odds} by \textcite{M63_Hardt_2016} and \textit{subgroup fairness} by \textcite{M79_Kearns_2018}.
		
		In the context of PASSION, fairness metrics which consider both true positives and false positives are particularly relevant. A \textit{true positive} indicates that a disease was detected correctly, while a \textit{false positive} corresponds to a diagnosis of a disease that is not actually present. Including false positives helps to identify cases where individuals from certain demographic groups may be unfairly more likely to receive unjustified diagnoses. This has also been indicated by \textcite{Sabato_2024}.
		
		From the listed group fairness metrics in \autoref{tab:fairness_definitions}, only equalized odds considers true and false positives, which should therefore be used for the evaluation of PASSION. A detailed explanation of equalized odds is provided in \autoref{chap:FairnessMetrics}.
		
		Given the specific dermatology use case in the context of PASSION, it is not clear whether individual fairness metrics would be feasible to use. Certain metrics propose to change attributes. This approach is not feasible for the skin type which is passed on to the model implicitly through the picture. Therefore, this thesis focuses on the group fairness metrics for now.
		
		Given the demographic focus of this study and the composition of the PASSION dataset, subgroup fairness is particularly important. Therefore, this thesis aims to incorporate equalized odds on subgroups as a core metric for evaluation.
		
		\subsubsection{Limitations of Fairness Evaluation with Equalized Odds for PASSION}
		Fairness metrics such as equalized odds are originally defined for binary classification problems, typically considering binary labels and binary demographic groups. As a result, fairness libraries like \gls{Fairlearn} offer implementations of these fairness metrics only for binary classification tasks \autocite{Fairlearn_nodate}. To evaluate fairness in multiclass settings using these libraries, certain considerations are required. This chapter introduces the two key challenges for the fairness evaluation of PASSION, handling multiclass labels and multiple subgroups.
		
		\paragraph{Multiclass Labels}
		In binary settings, fairness can be evaluated through simple comparisons of false positive and false negative rates. However, in multiclass classification, fairness must account for the full structure of the confusion matrix. \textcite{Sabato_2024} generalizes equalized odds to multiclass classification by defining:
		\textit{"For each \( y, z \in \mathcal{Y} \), the value of \( \mathbb{P}[\hat{Y} = z \mid Y = y, G = g] \) is the same for all \( g \in \mathcal{G} \)."}
		
		In practice, this means the entire confusion matrix must be equal across groups to satisfy strict multiclass fairness under equalized odds \autocite{Sabato_2024}. The similar approach is purposed by \textcite{Putzel_2022}.
		
		More relaxed versions of multiclass equalized odds have also been proposed in the literature, such as applying equalized odds per class. However, researchers argue that such relaxations may not be suitable in all contexts, especially when different types of errors carry different consequences \autocites{Sabato_2024}{Putzel_2022}.
		
		For instance, when the type of misclassification matters, equality of error rates is essential to ensure fairness, as noted by \textcite{Putzel_2022}. Furthermore, as \textcite{Sabato_2024} explicitly states, a fair classifier in healthcare should avoid differences in diagnosis errors for specific diseases across subgroups, since misdiagnoses can lead to different treatment outcomes.
		
		Therefore, in PASSION, the strict version of the multiclass equalized odds should be preferred. However, the code provided by \textcite{Sabato_2024} was not easy reusable, and there is no such version included in libraries like \gls{Fairlearn}. Therefore, this thesis uses the more relaxed version, since this is implementable with \gls{Fairlearn} and is still able to provide first insights for PASSION.
		
		\paragraph{Non-Binary Sensitive Features}
		There can also be non-binary sensitive features leading to multiple subgroups. The original definition of equalized odds does not account for this complexity. To generalize fairness evaluation to such settings, a one-vs-rest strategy can be applied. In this approach, each group is individually compared against the rest of the population \autocite{Nezami_2024}.
		
		\subsubsection{Fairlearn Implementation and Interpretation of Equalized Odds}
		\gls{Fairlearn} provides the functionality to calculate equalized Odds by calculating \gls{EOD} and \gls{EOR} and the class \texttt{MetricFrame} for a disaggregated report. It allows for the calculation of performance metrics based on sensitive attributes and supports the configuration of aggregation functions for summarizing subgroup disparities \autocite{Fairlearn_nodate}.
		
		For the calculation of the metrics, \gls{Fairlearn} provides multiple configuration options. In this thesis, the settings \texttt{agg="mean"} and \texttt{method="to\_overall"} are particularly relevant. This configuration reports the average difference between each subgroup’s performance and the overall performance, for a given type of subgroups (e.g., all possible subgroups based on \gls{FST} and sex).
		
		While it is also possible to report the worst-case deviation instead of the mean, this thesis focuses on an initial fairness assessment of PASSION. Therefore, using the mean as an aggregate measure is considered sufficient. For a more critical or risk-focused analysis, worst-case metrics should also be considered.
		
		When comparing models, additional aggregation is necessary because \gls{Fairlearn} reports fairness metrics separately for each type of subgroup. To identify the fairest model based on aggregated statistics across all subgroups, the following indicators should be considered:
		\begin{itemize}
			\item \textbf{Lowest average and median \gls{EOD}}: reflects strong overall fairness across subgroups.
			\item \textbf{Low standard deviation of \gls{EOD}}: indicates consistent performance and minimal disparity among subgroups.
			\item \textbf{Lowest worst-case \gls{EOD}}: captures the fairness for the most disadvantaged subgroup by highlighting the largest deviation.
		\end{itemize}
		These metrics were selected based on the principle that a lower \gls{EOD} indicates higher fairness, as a difference of 0 represents perfect equalized odds. For \gls{EOR}, the interpretation is inverted: a value closer to one signifies higher fairness, while lower values indicate greater disparity \autocite{Fairlearn_nodate}.
		
		
		\subsection{Mitigation Methods}
		\todo{\textbf{write mitigation methods chapter}}
		
		
		
		
			
		\section{PASSION Dataset Assessment} \label{chap:datasetAssessmentExecution}
		The practical analysis is conducted according to the methods outlined in \autoref{chap:datasetAssessmentMethod}:
		
		\begin{itemize}
			\item \textbf{Metadata Completeness.}
			The available PASSION metadata listed in \autoref{tab:PASSION_metadata} is compared to the demographic factors which are relevant for bias detection. Missing attributes are listed in \autoref{chap:datasetAssessmentMetadataEvaluation}.
			
			For certain attributes, the impact on dermatology specific use case is not entirely clear based on the literature review. For the attributes sex and age which are used in the PASSION dataset, the author of PASSION was contacted to provide more insight about their impact. This information was incorporated in the literature review.
			
			In order to provide the most complete view possible, all attributes which might have an impact are listed for the PASSION team to double-check with a dermatologist.
			
			\item \textbf{Presence of \glslink{proxyVar}{Proxy Variables}.}
			Since the intended purpose of the variables are not mentioned in the paper, the analysis for \glspl{proxyVar} was more difficult then expected. The result is based on the sensitive features and biases mentioned in the literature.
			
			Also, what the country variable represents in PASSION is not entirely clear based on the documentation. To clarify its meaning, the main author of PASSION was contacted.
			For all variables which appear to potentially be used as a \gls{proxyVar}, recommendations are provided for more precise alternatives for the PASSION team to check.
			
			\item \textbf{Representation of Main Groups.}
			Since there is no \gls{JupyterNotebook} script provided by PASSION to gather the proportions in depth, a python script is created to gather this data, what increased the time effort for the detailed analysis. The script is part of the newly created \texttt{evaluator} class and is meant to be executed standalone.
			It prints the distribution as absolute support and percentage for all values of the attributes country, sex, fitzpatrick, impetig, conditions\_PASSION, and ageGroup. The age group contains the ages binned into 5 year intervals, like it has been done by \textcite{Gottfrois2024} in their distribution analysis.
			Also, it saves the distribution in a csv and prints a plot per attribute. The comparison between the values is done manually for now, since there are not too many values.
			
			\todo{ensure to discuss the evaluator class beforehand somewhere and add command to command in readme(evaluator.run\_split\_distribution\_evaluation)} 
			
			\item \textbf{Representation of Relevant Subgroups.} The demographic distribution figures of PASSION are briefly analyzed for an initial indication of the representation of age and sex.
		\end{itemize}
		
		
		\section{Reproducing PASSION Results}
		While attempting to reproduce the results reported in the PASSION paper, some issues in the provided codebase had to be addressed. First, the metadata filenames referenced in the code were outdated, and the linkage between images and metadata records seemed to not fit the provided metadata files, preventing proper data loading. This was resolved using the same method as in the "Linking CSV Data with Image Files" script included with the PASSION data analysis scripts, ensuring compatibility. After fixing the data linkage, the models for \texttt{conditions\_PASSION} and \texttt{impetig} were trained, and the results were compared with those reported in the PASSION publication.
		
		During the verification of group-level performance reproducibility, it was identified that the linkage between predictions and metadata was not functioning correctly in the evaluation pipeline. The original linkage used indices, which proved unreliable. To confirm the issue, the trained model was reloaded and the evaluation rerun. If group-level evaluation metrics changed despite identical model and data inputs, the linkage must be faulty.
		
		To allow for the model reloading, the checkpoint handling had to be revised. The evaluation process was encapsulated within a separate \texttt{Evaluator} class to improve code modularity and separation of concerns.
		
		The corrupted metadata linkage was resolved by adding the image filename into the dataloader, allowing the \texttt{Evaluator} to accurately link predictions to the correct metadata records.
		
		These unanticipated code fixes required significant time, but they were essential for ensuring the validity of the analysis.

		
		\section{PASSION Baseline Fairness Assessment} \label{chap:PASSIONFairnessAssessmentBaselineExecution}
		
		\subsection{Baseline Setup}
		This evaluation was conducted on the \textit{conditions\_PASSION} model. The binary \textit{impetig} model was excluded due to the already high complexity and runtime demands of the multiclass setup.
		
		The original PASSION model was trained using a \textit{ResNet50} architecture. However, due to its long training and evaluation time, a smaller model version, \textit{ResNet18}, was used for the experiments to enable faster iteration. To get insight in potential performance disparities based on this substitution, both models were evaluated using the same fairness assessment process as described in \linkchap{chap:fairnessAssessmentMethod}. This enabled a comparison to verify whether the smaller model produced comparable subgroup fairness insights and could be reliably used for the experimental phase.
		\todo{try to cite, or at least use protocols}
		
		To further improve runtime efficiency and flexibility during the experiments, the several modifications were made to the original pipeline and methodology:
		
		\begin{itemize}
			\item Temporarily enabled parallel data loading to accelerate experimentation (this change was later reverted for better reproducibility).
			\item Accelerated data loading by moving redundant checks out of a loop.
			\item Introducing the concept to check variants of a mitigation method without 5-fold cross validation to allow for faster iterations
		\end{itemize}
		
		
		\subsection{Fairness Assessment Implementation} \label{chap:fairnessAssessmentImpementation}
		Fairness was assessed using \textit{equalized odds} on sensitive subgroups defined by unique combinations of \textit{\gls{FST}, sex, age group, and country}, as introduced in previous chapters. The evaluation was implemented following the method described in \autoref{chap:fairnessAssessmentMethod}.
		
		Considering the findings in \autoref{chap:ContextFairnessMetrics}, \gls{Fairlearn} methods where combined with custom implementation to implement the relaxed version of multiclass equalized odds. The final evaluation consists of several steps:
		\begin{itemize}
			\item \textbf{Data Aggregation:} Prediction results and metadata are linked and saved into a unified CSV, which can be used for manual inspection and is loaded on evaluation reruns.
			\item \textbf{General Performance:} Overall performance metrics are reported, as implemented by the PASSION team.
			\item \textbf{(Sub-)group Evaluation:} For each combination of sensitive attributes, performance and fairness metrics are computed.
			\item \textbf{Class-Level Fairness Metric Computation:} Using \texttt{MetricFrame} from \gls{Fairlearn}, \gls{EOD} and \gls{EOR} are computed per class. Due to the binary limitation of \gls{Fairlearn}'s implementation, a one-vs-all strategy is applied to enable multiclass fairness evaluation.
			\item \textbf{Aggregation on Subgroup Level:} Class-level fairness metrics are further aggregated per subgroup using:
			\begin{itemize}
				\item Worst-case
				\item Mean
				\item Best-case
			\end{itemize}
			This aggregation approach is inspired by the \textit{summary} aggregation for subgroup reporting for one class provided by \gls{Fairlearn} \autocite{Fairlearn_nodate}
			\item \textbf{Aggregation on Model Level:} The subgroup level metrics are aggregated further, to report fairness across all subpopulations for easier model comparison, using:
			\begin{itemize}
				\item Worst-case
				\item Mean
				\item Median
				\item Best-case
				\item Standard deviation
			\end{itemize}
			This last step is done manually using an \textit{Excel}-file so far.
		\end{itemize}
		
		To identify all privileged and underprivileged subgroups, comparisons of subgroup \gls{TPR} and \gls{FPR} against macro-averages of the same type of subgroups were conducted. The rates where computed based on the confusion matrices. A relaxed threshold of 0.2 was used to ignore slight differences in this initial fairness assessment. Subgroups with better-than-average \gls{TPR} and lower-than-average \gls{FPR} were marked \textit{privileged}; the inverse as \textit{underprivileged}. Borderline groups were labeled \textit{unclear}, and those lacking support were tagged with \textit{no support}. Those outputs were cross-validated against manual calculations and \gls{Fairlearn}'s outputs for correctness. \todo{cite / Add reference to methods for multiclass fairness.}
		
		For comparing models, the aggregated values have to be compared. Currently, this step is also covered in the mentioned Excel file.
		
		
		\section{Stratified Split Experiment}
		To analyze the original split, the script from \autoref{chap:datasetAssessmentExecution} was extended to evaluate attribute distributions across each subsets.
		
		The following attribute combinations were used for stratification:
		\begin{enumerate}
			\item conditions\_PASSION, impetig
			\item conditions\_PASSION, impetig, country
			\item conditions\_PASSION, impetig, fitzpatrick
			\item conditions\_PASSION, impetig, country, fitzpatrick
			\item conditions\_PASSION, impetig, country, fitzpatrick, sex
			\item Random split without stratification
		\end{enumerate}
		
		A key challenge was the presence of subgroups with single records, which hinder stratification since at least two samples per subgroup are required for even distribution. These single-record instances were handled in two ways:
		\begin{itemize}
			\item Strategy A: Assigning them to the training set, ensuring the model learns from all subgroups but excluding them from fairness evaluation.
			\item Strategy B: Assigning them to the validation set, allowing subgroup inclusion in fairness analysis but excluding them from model training.
		\end{itemize}
		
		Both strategies were applied to each split, resulting in 12 models. The seeds got fixed to remain compatibility. Unfortunately, the seed was mistakenly changed between generating the different strategies. Therefore, the models were evaluated per strategy, to avoid improper comparisons.
		
		To evaluate fairness, the models were trained using PASSION's pipeline with each split configuration. The evaluation focused on fairness metrics alone, given that this was the primary objective. Final evaluations included both fairness and performance trade-offs using 5-fold cross-validation on the most promising splits.
		
		\subsection{Limitations}
		Fairness evaluation was done entirely based on \gls{EOD}, as \gls{EOR} reported mostly values close to zero across most subgroups. This trend is consistent over the models, rendering \gls{EOR} uninformative for this experiment. 
		
		Furthermore, skewed subgroup distributions often led to extreme \gls{TPR} and \gls{FPR} values, especially in small subgroups. This heavily affected the resulting \glspl{EOD}. Future work should address this by collecting more subgroup-specific data.
		
		Lastly, the evaluation was challenging due to the number of models to comare and the required manual intervention. The process in \autoref{chap:fairnessAssessmentImpementation} needs to be improved.
		
		
	\chapter{Evaluation and Validation}
		\baaCriteria{Auswertung und Interpretation der Ergebnisse. Nachweis, dass die Ziele erreicht wurden, oder warum	welche nicht erreicht wurden.}
		\baaCriteria{Die Ziele / Forschungsfragen sind dem Umfang der Arbeit entsprechend sehr klar abgegrenzt; sie sind präzise, überprüfbar und nach den Standards der Zielformulierung definiert. Die Zielerreichung wurde systematisch und korrekt validiert.}
		\baaCriteria{Die Herleitung und Bedeutung der Ergebnisse, mögliche Varianten, Gütekriterien und eine Validierung allgemein werden nachvollziehbar diskutiert}
		
		
		\section{PASSION Dataset Assessment}
		The PASSION dataset assessment results are described in this section.
		Overall, the dataset enables a foundational fairness analysis but does not support in-depth bias evaluation without augmentation or careful interpretation.
		
		\subsection{Metadata Completeness and \glslink{proxyVar}{Proxy Variables}} \label{chap:datasetAssessmentMetadataEvaluation}
		Based on the literature regarding sensitive features and potential biases, sensitive metadata is available in the dataset, namingly \gls{FST}, age, sex, and country. To obtain a feasible number of comparable subgroups, age can be grouped into age groups by using 5-year age groups, following the approach by \textcite{Gottfrois2024}.
		However, relevant metadata for a thorough fairness assessment and bias mitigation is missing. This limits what biases can be detected.
		
		The missing metadata attributes are:
		\begin{itemize}
			\item socioeconomic status
			\item geographic location / residence of the patient
			\item (type of) the clinic and their medical focus
			\item image quality or other image related information such as the phone used, whether the image contains hair, and so on
			\item ethnicity (if it proves to have an impact on dermatology conditions)
			\item disabilities (if it proves to have an impact on dermatology conditions)
		\end{itemize}
		
		The variable \textit{country} currently could theoretically serve as \gls{proxyVar} for \textit{geographic location}, which clinic the data is from and more broadly even for the \textit{image quality}. It is not clear if those usages are intended. According to the literature review, this should be prevented. \todo{ensure that this is indeed written somewhere in the literature section}
		Since the country only reflects the location of diagnosis, it is insufficient to determine the \textit{geographic location} or residence of the patient. More precise data would be preferable for robust bias analysis.
		Since the data is gathered only from one clinic per country, this \gls{proxyVar} usage is feasible for now. However, more clinics should be included into the data collection process to mitigate medical biases and ascertainment bias. Then, the clinic and some more data about it should be added to the dataset.
		The clinic again might be a \gls{proxyVar} for the picture quality. If this information can be quantified in another way, e.g., the used phone and camera settings, that would further improve the dataset by tackling image biases.
		The country information can still be used in the fairness assessment to see if there are fairness differences in those populations. However, in order to clearly identify related biases, the suggested changes to the metadata would need to be introduced.
		
		It is suggested to add the missing metadata attributes to the dataset. Given the sensitivity of those attributes, ethical considerations must be addressed before extending the dataset.
		
		\subsection{Demographic Representation}	\label{chap:PASSIONDatasetAssessmentEvalDemogRepr}
		The demographic distribution in the PASSION dataset shows clear imbalances across several attributes. The data is available in \linkapp{app:PASSIONdataDistributionAnalysis}.
		
		To summarize:
		
		\begin{itemize}
			\item \textbf{Country.} The dataset is heavily skewed towards samples from Madagascar (59.59\%), while Tanzania is significantly underrepresented (1.39\%). This imbalance may introduce geographic or clinic-specific biases.
			
			\item \textbf{Sex.} Male patients are overrepresented (58.2\%) compared to female patients (41.8\%). No data is available for individuals of other sexes.
			
			This thesis did not explore whether other biological sex differences or gender-affirming hormone therapies have any impact on dermatological conditions, since the main focus for PASSION is on inclusion regarding skin type. However, for a complete fairness evaluation, this factors should be explored in the future.
			
			\item \textbf{\gls{FST}.} The types III to VI are represented, with the distribution ranging from 21.42\% (type III) to 29.4\% (type IV). No data is available for type II and only one sample for type I.  Given PASSION's focus on highly pigmented skin, this distribution is somewhat justified. However, it limits applicability to lighter skin tones and could impair model generalizability. 
			
			An interesting future direction would be to combine PASSION with other dermatology datasets to evaluate fairness and performance across the full spectrum of \gls{FST}. Moreover, due to historical underrepresentation of highly-pigmented skin in dermatology datasets, the performance on types V (25.89\%) and VI (23.23\%) should be examined in more detail, to see if their representation in the dataset must be addressed further.
			\todo{cite https://academic.oup.com/bjd/article-abstract/185/1/198/6600283?redirectedFrom=fulltext, already mention this in dermatology bias section}
			
			
			\item \textbf{Age Groups.} Children aged 0–9 account for over 40\% of the dataset, whereas elderly patients (65+) are nearly absent. Although this skew reflects PASSION’s focus on \gls{pediatric} conditions, the lack of data for seniors may reduce fairness for those age groups.
			
			Nevertheless, PASSION's age-generalization experiments suggest that a model trained on primarily \gls{pediatric} images might generalize reasonably well \autocite{Gottfrois2024}.
			
			\item \textbf{Conditions.} The dataset is dominated by fungal infections (35.02\%), followed by scabies (28.49\%), and eczema (25.05\%). Other conditions account only for 11.43\%. Mo data is available for healthy skin.
			
			The ambiguous "other" category complicates fairness evaluations for specific conditions. Disaggregating this group into defined labels would improve clarity. Additionally, including healthy skin samples could reduce potential bias and enable better calibration of diagnostic models. \todo{find and add healthy-vs-disease bias here}
			
			\item \textbf{Impetigo Indicator.} The impetigo label is present in only 11.6\% of the cases, indicating class imbalance that may affect prediction reliability for this condition.
		\end{itemize}
		
		The \autoref{fig:PASSIONDistrImbalances} illustrates the overrepresentation of male children, based on the figures presented by \textcite{Gottfrois2024}. There are also condition-specific differences in \gls{FST} distribution. If this imbalances significantly affect model fairness, the dataset composition may need to be revised.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.9\textwidth]{figures/PASSIONDatasetDistributionPotentialImbalances.png}
			\caption{PASSION dataset distributions by \textcite{Gottfrois2024} - highlighting potential imbalances}
			\label{fig:PASSIONDistrImbalances}
		\end{figure}
		
		These findings highlight representation disparities across several demographic and clinical factors. Such disparities should be accounted for during training and fairness evaluation, especially when assessing subgroup-specific performance.
		
		It is important to note that the provided analysis only is a high-level overview at the group level. Detailed subgroup representation has not yet been assessed in details. Due to the time limits of this thesis, this was deferred in favor of executing the stratified split experiment.
		
		To enable subgroup-level representation analysis, group-level dataset representation script should be extended accordingly. As the script output will increase substantially, manual comparison may become impractical. Therefore, automating the comparison and generating summaries of the largest disparities is recommended.
		
		
		\section{Reproducing PASSION Results}
		The overall model performance was consistent with the results reported in the PASSION paper.
		
		However, the group-level performance results could not be reproduced. Multiple inference runs with the same model and dataset produced inconsistent results. Introducing metadata linkage via filenames resolved this issue and provided stable, reproducible results. This confirms a reliable association between predictions and metadata, which is critical for fairness analysis.
		
		Currently, the checkpoint handling supports only evaluation. Additional adjustments are needed to fully support resumed training, particularly to ensure correct and reproducible handling of epochs and cross-validation folds.
		
		Those changes will be contributed to the PASSION code base to make the reproduction easier for others.
		
		While these extensive code improvements reduced the time available for fairness analysis, they are a critical enhancement to the robustness and usability of the PASSION evaluation.
		
		\section{PASSION Baseline Fairness Assessment} \label{sec:evaluation}
		
		The baseline fairness performance of \texttt{ResNet50} and \texttt{ResNet18} was assessed. This evaluation serves as a reproducible reference against which the impact of fairness mitigation strategies can be compared.
		
		\begin{itemize}
			\item \textbf{Baseline ResNet50:} \todo{[Insert Equalized Odds Difference/Ratio metrics here per subgroup and model level aggregation]}
			\item \textbf{Baseline ResNet18:} \todo{[Insert Equalized Odds Difference/Ratio metrics here per subgroup and model level aggregation]}
		\end{itemize}
		
		Although minor performance differences between the two model versions were found (e.g., balanced accuracy of 0.70 for ResNet50 and 0.69 for ResNet18), the subgroup-level fairness trends proved largely consistent. Therefore, ResNet18 was considered a valid substitution model for experimentation in this thesis.
		\todo{add performance metrics as table}
		
		Further conclusions are summarized in the following subchapters.
		
		\subsection{Bias in the Baseline}
		\todo{link output}
		Subgroup fairness results revealed some slight disparities, but still indicates a trend:
		
		\begin{itemize}
			\item \textbf{Sex:} The small model (\texttt{ResNet18}) showed no clear sex-related bias, whereas the larger model (\texttt{ResNet50}) exhibited a slight bias toward women, with higher \gls{TPR} for female patients.
			\item \textbf{Skin Type (Fitzpatrick):} Both models consistently privileged Skin Types V and underprivileged Type VI. Notably, Skin Types III and VI showed different behavior across models, being more privileged in the big model.
			\item \textbf{Age:} The impact of age was relatively small overall. Nevertheless, age groups 0–14 and 25–29 were generally better off, whereas groups 20–24 and 30–69 were more often underprivileged. No samples from the 70+ group were available in the test data.
			\item \textbf{Intersectional Analysis (e.g., Sex × Age × Skin Type):} Subgroup-level analysis revealed distinct patterns, such as males aged 15–59 being consistently underprivileged across both models. Also, subgroups tend to have very low support, which makes the fairness analysis less stable.
			\item \textbf{Country:} Substantial differences emerged between countries. For example, Guinea performed better under the small model, while Malawi showed better results with the larger model. Tanzania remained underprivileged across both architectures.
		\end{itemize}
		
		Intersectional fairness issues also became apparent when combining protected attributes. For example, in the large model:
		\begin{itemize}
			\item \textbf{\gls{FST} VI in Madagascar and Tanzania} performed particularly poorly.
			\item \textbf{Guinea with \gls{FST} VI} still showed favorable outcomes, albeit slightly worse in ResNet50 compared to ResNet18. This indicates, that the country might impact the model's bias stronger than the skin type.
		\end{itemize}
		
		Overall, the clearest fairness disparities were observed in subgroups related to the attributes \gls{FST}, sex, and country. Given PASSION’s goal to mitigate bias against highly pigmented skin tones, fairness issues with \gls{FST} VI are especially concerning. That some subgroups including \gls{FST} VI and specific countries perform well indicates, that the bias could stem from other origins, such as the image quality or the process on how the data was gathered in those countries. While this analysis provides a first systematic fairness evaluation, deeper investigations are necessary, particularly into age-related intersectional effects. The provided scripts enable further detailed analysis and subgroup comparisons.
		
		For further work,  additional data collection efforts should prioritize Tanzania since the country is underrepresented which is consistently reflected in the model performance. Data quality or scarcity might be contributing to inconsistent results for this subgroup. Due to the sensitive medical nature of the images and personal limitations in handling such content, the images were not directly reviewed to support this hypothesis. It is, however, strongly recommended that the PASSION team conducts a thorough analysis of these cases.
		
		% \todo{Add link or reference to analysis scripts and generated data, possibly in appendix}
		
		\subsection{Subgroup-Level Insights}
		Using the aggregation of class-level equalized odds metrics, the assessment revealed substantial variance across subgroups. Privileged and underprivileged subgroups are consistently identifiable.
		
		While some groups showed stable behavior across classes, others shifted category depending on the evaluated class, which underlines the importance of per-class fairness computation in multiclass settings.	
		
		
		\subsection{Pipeline Challenges}
		
		Several technical limitations impacted the reliability and completeness of the fairness assessment:
		\begin{itemize}
			\item \gls{Fairlearn}’s default multiclass handling is limited. To overcome this, a custom implementation was required, which introduces complexity and potential inconsistencies with the intended methodology of researchers.
			\item In the report part where subgroups are classified regarding privilege level, some subgroups were suppressed. This affects fairness analysis negatively. The comparison to the \gls{Fairlearn} output revealed this issue. This proves that it is preferable to use well-established, tested libraries for whenever possible.
			\item Manual aggregation of subgroup-level to model-level metrics as well as the cross-model comparison is currently not automated, reducing reproducibility and increasing error risk.
			\item For model comparisons, the approach of \textcite{Valentim_2019} of creating fairness comparison rations could be used for the automated reporting.
		\end{itemize}
		
		Despite these challenges, the evaluation successfully surfaced subgroup disparities, supporting claims that fairness analysis on subgroups is important for reducing biases in dermatology models, including PASSION.
		
		\subsection{Aggregation Trade-offs}
		Aggregating fairness metrics at subgroup and model level provided helpful summaries but hide subgroup-specific effects. This must be considered when interpreting aggregated metrics.
		
		The proposed aggregation strategy was implemented due to the absence of ready-to-use multiclass equalized odds metrics in \gls{Fairlearn} or similar libraries. This illustrates the need for researchers to work together and implement suggested methodology improvements in the state of the art libraries.
		
		\section{Stratified Split Experiment}
		
		The evaluation of this experiment confirms that stratification strategies can influence fairness. The current findings suggest that including the attributes country and fitzpatrick in the stratification process can improve the fairness of models trained on the PASSION dataset. However, this improvement may come at the cost of overall model performance.
		
		These results should be interpreted with caution, as the experiment faced several limitations. To achieve more statistically robust findings, additional data is needed. Further experiments should be conducted using the available codebase. Based on the results, the current PASSION split could likely be refined. Moreover, analyzing subgroup-specific performance may help guide future data collection efforts toward fairer outcomes.
		
		
		\subsection{Demographic Representation Accross Subsets}
		Distribution analysis of the original PASSION split shows that the distributions between the subsets are balanced for some attributes (e.g., country, conditions\_PASSION), while others show notable discrepancies (fitzpatrick and sex). This suggests the original split may have used country and conditions\_PASSION for stratification, possibly including impetig and ageGroup.
		\todo{link to appendix or github} 	%C:\Users\nadja\OneDrive\HSLU_Nadja\BAA\baa_on_git\results\reproducing_PASSION_results\analyzing_dataset_split	
		
		Interestingly, the dataset shows male overrepresentation, especially in the training set, despite slight female bias in model performance (\autoref{tab:PASSIONSexDistribution}). Similarly, \gls{FST} IV and V are overrepresented in training data (\autoref{tab:PASSIONFstDistribution}), possibly contributing to the observed bias. However, \gls{FST} VI is evenly distributed accross the subsets, yet model performance remains poor. This suggests that data imbalance is not necessarily the sole cause of observed biases. However, a more detailed subgroup-level analysis across all subsets is still essential for a robust interpretation.
		
		\begin{table}[H]
			\centering
			\begin{tabularx}{\textwidth}{l *{3}{>{\centering\arraybackslash}X}}
				\toprule
				\textbf{Set} & \textbf{Female} & \textbf{Male} & \textbf{Total} \\
				\midrule
				Training set & 539 (40.74\%) & 784 (59.26\%) & 1323 \\
				Test set & 152 (46.06\%) & 178 (53.94\%) & 330 \\
				Overall & 691 (41.8\%) & 962 (58.2\%) & 1653 \\
				\bottomrule
			\end{tabularx}
			\caption{PASSION Dataset: Sex distribution (train, test, overall).}
			\label{tab:PASSIONSexDistribution}
		\end{table}
		
		\todo{consider to move tables in appendix}
				
		\begin{table}[H]
			\centering
			\begin{tabularx}{\textwidth}{l *{6}{>{\centering\arraybackslash}X} >{\centering\arraybackslash}X}
				\toprule
				\textbf{Set} & \textbf{I} & \textbf{II} & \textbf{III} & \textbf{IV} & \textbf{V} & \textbf{VI} & \textbf{Total} \\
				\midrule
				Training set & 1 (0.08\%) & -- & 275 (20.79\%) & 396 (29.93\%) & 344 (26.00\%) & 307 (23.20\%) & 1323 \\
				Test set & -- & -- & 79 (23.94\%) & 90 (27.27\%) & 84 (25.45\%) & 77 (23.33\%) & 330 \\
				Overall & 1 (0.06\%) & -- & 354 (21.42\%) & 486 (29.40\%) & 428 (25.89\%) & 384 (23.23\%) & 1653 \\
				\bottomrule
			\end{tabularx}
			\caption{PASSION Dataset: \glslink{FST}{FST} distribution (train, test, overall).}
			\label{tab:PASSIONFstDistribution}
		\end{table}	
		
		\subsection{Initial Training}
		
		The fairness assessment of the initial model training as shown in \autoref{tab:StratifiedSplitstratified-seed42} indicated that for strategy A, placing single records in training data, the configuration 4, using country and fitzpatrick, resulted in the fairest model overall, due to the analysis of reported \gls{EOD}:
		\begin{itemize}
			\item Lowest average and median
			\item Fairly low standard deviation
			\item Moderate worst-case fairness
		\end{itemize}
		
		
		For strategy B (\autoref{tab:StratifiedSplitstratified-seed32}), placing singletons in validation data, the fairest model was achieved by stratifying only on the target labels, due to:
		\begin{itemize}
			\item Low average and median
			\item Lowest standard deviation
			\item Moderate worst-case fairness
		\end{itemize}
		
		\begin{table}[H]
			\centering
			\scriptsize
			\begin{tabularx}{\textwidth}{l *{6}{>{\centering\arraybackslash}X}}
				\toprule
				\textbf{Metric} & \textbf{Split 1} & \textbf{Split 2} & \textbf{Split 3} & \textbf{Split 4} & \textbf{Split 5} & \textbf{Split 6} \\
				\midrule
				avg & 0.53 & 0.55 & \textcolor{red}{0.56} & \textcolor{teal}{0.48} & 0.54 & 0.55 \\
				best & 0.03 & 0.03 & 0.04 & 0.05 & \textcolor{teal}{0.01} & \textcolor{red}{0.08} \\
				worst & \textcolor{teal}{0.74} & 0.81 & \textcolor{red}{0.83} & 0.79 & 0.79 & 0.78 \\
				median & 0.55 & 0.54 & \textcolor{red}{0.60} & \textcolor{teal}{0.44} & 0.53 & 0.56 \\
				std. dev. sub pop. & 0.22 & 0.23 & \textcolor{red}{0.25} & 0.23 & 0.25 & \textcolor{teal}{0.22} \\
				std. dev. whole pop. & 0.21 & 0.22 & \textcolor{red}{0.24} & 0.23 & 0.24 & \textcolor{teal}{0.21} \\
				\bottomrule
			\end{tabularx}
			\caption{Stratified Split: Fairness summary (seed 42, single-record training stratification).}
			\label{tab:StratifiedSplitstratified-seed42}
		\end{table}
		
		\begin{table}[H]
			\centering
			\scriptsize
			\begin{tabularx}{\textwidth}{l *{6}{>{\centering\arraybackslash}X}}
				\toprule
				\textbf{Metric} & \textbf{Split 1} & \textbf{Split 2} & \textbf{Split 3} & \textbf{Split 4} & \textbf{Split 5} & \textbf{Split 6} \\
				\midrule
				avg & 0.51 & 0.55 & \textcolor{red}{0.57} & 0.55 & 0.55 & \textcolor{teal}{0.49} \\
				best & \textcolor{teal}{0.02} & 0.04 & 0.03 & 0.03 & \textcolor{red}{0.05} & 0.03 \\
				worst & 0.74 & 0.82 & \textcolor{red}{0.84} & 0.75 & \textcolor{teal}{0.71} & 0.73 \\
				median & 0.53 & 0.58 & 0.56 & 0.56 & \textcolor{red}{0.65} & \textcolor{teal}{0.50} \\
				std. dev. sub pop. & \textcolor{teal}{0.18} & \textcolor{red}{0.25} & 0.23 & 0.20 & 0.19 & 0.22 \\
				std. dev. whole pop. & \textcolor{teal}{0.17} & \textcolor{red}{0.25} & 0.22 & 0.20 & 0.18 & 0.21 \\
				\bottomrule
			\end{tabularx}
			\caption{Stratified Split: Fairness summary (seed 32, single-record validation stratification).}
			\label{tab:StratifiedSplitstratified-seed32}
		\end{table}
		
		Random splits also performed surprisingly well. Using strategy B, even the highest fairness was achieved in terms of average and median, though with higher variance. Using strategy A, the lowest standard deviation was achieved, but the fairness was lower overall.
		
		Due to these observations, and their overall well performance, splits 1, 4, and 6 were selected for 5-fold cross-validation.
		
		\subsection{Cross-Validation}
		
		Results of the 5-fold cross-validation step confirmed that including country and fitzpatrick in the stratification consistently resulted in the fairest models for both single-records-handling strategies (\autoref{tab:StratifiedSplitstratified-seed32-crossVal}, \autoref{tab:StratifiedSplitstratified-seed42-crossVal}).
		
		\begin{table}[H]
			\centering
			\begin{tabularx}{\textwidth}{l *{3}{>{\centering\arraybackslash}X}}
				\toprule
				\textbf{Metric} & \textbf{Split 1} & \textbf{Split 4} & \textbf{Split 6} \\
				\midrule
				avg & \textcolor{red}{0.54} & 0.50 & \textcolor{teal}{0.48} \\
				best & \textcolor{teal}{0.03} & \textcolor{teal}{0.03} & \textcolor{teal}{0.03} \\
				worst & \textcolor{red}{0.75} & \textcolor{teal}{0.65} & 0.71 \\
				median & \textcolor{red}{0.57} & 0.55 & \textcolor{teal}{0.53} \\
				std. dev. sub pop. & \textcolor{red}{0.20} & \textcolor{teal}{0.16} & \textcolor{red}{0.20} \\
				std. dev. whole pop. & 0.19 & \textcolor{teal}{0.16} & \textcolor{red}{0.20} \\
				\bottomrule
			\end{tabularx}
			\caption{Stratified Split: Fairness summary (5-fold CV, seed 32, validation stratification).}
			\label{tab:StratifiedSplitstratified-seed32-crossVal}
		\end{table}
		
		\begin{table}[H]
			\centering
			\begin{tabularx}{\textwidth}{l *{3}{>{\centering\arraybackslash}X}}
				\toprule
				\textbf{Metric} & \textbf{Split 1} & \textbf{Split 4} & \textbf{Split 6} \\
				\midrule
				avg & \textcolor{red}{0.55} & \textcolor{teal}{0.50} & \textcolor{red}{0.55} \\
				best & \textcolor{teal}{0.04} & \textcolor{teal}{0.04} & \textcolor{red}{0.06} \\
				worst & \textcolor{red}{0.82} & \textcolor{teal}{0.77} & \textcolor{teal}{0.77} \\
				median & 0.54 & \textcolor{teal}{0.51} & \textcolor{red}{0.58} \\
				std. dev. sub pop. & \textcolor{red}{0.25} & 0.23 & \textcolor{teal}{0.22} \\
				std. dev. whole pop. & \textcolor{red}{0.24} & \textcolor{teal}{0.22} & \textcolor{red}{0.22} \\
				\bottomrule
			\end{tabularx}
			
			\caption{Stratified Split: Fairness summary (5-fold CV, seed 42, training stratification).}
			\label{tab:StratifiedSplitstratified-seed42-crossVal}
		\end{table}
		
		
		\subsection{Baseline Comparison}
		Final evaluation on the original test set (\autoref{tab:StratifiedSplitBaselineComparison}) confirms that applying stratified splitting impacts model fairness, especially on subgroup levels.
		Stratifying also on country and fitzpatrick improved fairness, especially when single records are in the training set. For the other strategy, results are mixed, but also there, an impact is notable. Note the results are not directly comparable though due to different seeds.
		
		\begin{table}[H]
			\centering
			\begin{tabularx}{\textwidth}{l *{3}{>{\centering\arraybackslash}X}}
				\toprule
				\textbf{Metric} & \textbf{Baseline} & \textbf{Strategy A} & \textbf{Strategy B} \\
				\multicolumn{4}{l}{\textbf{Overall}} \\
				avg & 0.49 & \textcolor{teal}{0.47} & \textcolor{red}{0.51} \\
				best & 0.03 & \textcolor{teal}{0.02} & \textcolor{black}{0.03} \\
				worst & 0.73 & \textcolor{red}{0.75} & \textcolor{red}{0.74} \\
				median & 0.54 & \textcolor{teal}{0.51} & \textcolor{black}{0.54} \\
				std. dev. sub pop. & 0.24 & \textcolor{teal}{0.21} & \textcolor{teal}{0.22} \\
				std. dev. whole pop. & 0.23 & \textcolor{teal}{0.21} & \textcolor{teal}{0.22} \\
				
				\midrule
				\multicolumn{4}{l}{\textbf{Avg. Per Subgroup}} \\
				fitzpatrick & 0.10 & \textcolor{red}{0.14} & \textcolor{red}{0.19} \\
				sex & 0.03 & \textcolor{teal}{0.02} & \textcolor{black}{0.03} \\
				ageGroup & 0.46 & \textcolor{teal}{0.43} & \textcolor{red}{0.54} \\
				country & 0.34 & \textcolor{red}{0.36} & \textcolor{teal}{0.33} \\
				fitzpatrick, sex & 0.18 & \textcolor{red}{0.20} & \textcolor{red}{0.28} \\
				fitzpatrick, ageGroup & 0.67 & \textcolor{teal}{0.60} & \textcolor{red}{0.68} \\
				fitzpatrick, country & 0.51 & \textcolor{black}{0.51} & \textcolor{teal}{0.50} \\
				sex, ageGroup & 0.56 & \textcolor{teal}{0.53} & \textcolor{red}{0.62} \\
				sex, country & 0.38 & \textcolor{red}{0.41} & \textcolor{teal}{0.37} \\
				ageGroup, country & 0.73 & \textcolor{teal}{0.48} & \textcolor{teal}{0.64} \\
				fitzpatrick, sex, ageGroup & 0.70 & \textcolor{red}{0.75} & \textcolor{red}{0.74} \\
				fitzpatrick, sex, country & 0.54 & \textcolor{teal}{0.51} & \textcolor{black}{0.54} \\
				fitzpatrick, ageGroup, country & 0.73 & \textcolor{teal}{0.60} & \textcolor{teal}{0.70} \\
				sex, ageGroup, country & 0.73 & \textcolor{teal}{0.69} & \textcolor{red}{0.74} \\
				fitzpatrick, sex, ageGroup, country & 0.73 & \textcolor{red}{0.75} & \textcolor{red}{0.74} \\
				\bottomrule
			\end{tabularx}
			\caption{Stratified Split: Fairness comparison: baseline vs. stratified variants.}
			\label{tab:StratifiedSplitBaselineComparison}
		\end{table} 
		
		While fairness improved in the stratified variants, this came with a noticeable drop in overall model performance (\autoref{tab:StratifiedSplitBaselineComparisonPerformance}). This was somewhat expected, as the baseline used the full original training set, whereas the stratified variants employed an additional train-validation split, reducing the number of training samples. Both F1-score and balanced accuracy decreased compared to the baseline. Strategy~B, in particular, exhibited the lowest performance across most metrics, likely due to having the smallest training set and lacking certain rare cases.
		
		This illustrates the trade-off between fairness and predictive performance, which must be carefully managed in real-world applications.
		
		\begin{table}[H]
			\centering
			\begin{tabularx}{\textwidth}{l *{3}{>{\centering\arraybackslash}X}}
				\toprule
				\textbf{Metric} & \textbf{Baseline} & \textbf{Strategy A} & \textbf{Strategy B} \\
				\midrule
				Accuracy             & 0.69 & \textcolor{red}{0.61} & \textcolor{red}{0.59} \\
				Macro F1             & 0.69 & \textcolor{red}{0.61} & \textcolor{red}{0.59} \\
				Weighted F1          & 0.69 & \textcolor{red}{0.62} & \textcolor{red}{0.59} \\
				Balanced Accuracy    & 0.69 & \textcolor{red}{0.62} & \textcolor{red}{0.60} \\
				\bottomrule
			\end{tabularx}
			
			\caption{Stratified Split: Performance comparison: baseline vs. stratified variants.}
			\label{tab:StratifiedSplitBaselineComparisonPerformance}
		\end{table}
		
		
		
	\chapter{Outlook}
		\baaCriteria{Reflexion der eigenen Arbeit, ungelöste Probleme, weitere Ideen.}
		\baaCriteria{Die Ergebnisse und Empfehlungen schaffen einen konkreten Mehrwert für die Auftraggebenden. Einschränkungen und Grenzen werden kritisch diskutiert und die nächsten Schritte im Ausblick festgehalten, so dass die Ergebnisse direkt in der Praxis weiterverwendet und/oder angewendet werden können.}
		
		This chapter summarizes the concrete recommendations to overcome the limitations of the current work. This includes e.g., revising the metadata used in PASSION, and extending the analytical tools used.
		
		It also provides ideas, such as adding more diverse data and combining PASSION with other dermatology datasets to improve bias detection and aim for a more complete dataset.
		
		These measures aim to enhance the practical applicability of the results and support the development of fair, generalizable \gls{ML} models in dermatology.
	
		
		\section{PASSION Dataset Improvements}
		To improve the fairness assessment capabilities of the PASSION dataset, the following dataset improvements are proposed:
		\begin{itemize}
			\item Include the missing metadata attributes identified in \autoref{chap:datasetAssessmentMetadataEvaluation} (e.g., socioeconomic status, clinic type, image quality) to enable a more comprehensive fairness evaluation. Ensure to assess the ethical implications before collecting such data.
			\begin{itemize}
				\item Investigate whether \textit{ethnicity} and \textit{disabilities} influence the presentation or prevalence of dermatological conditions before adding them to the dataset.
			\end{itemize}
						
			\item Clarify the intended purpose of the \textit{country} variable, and replace or supplement it with more precise alternatives, as discussed in \autoref{chap:datasetAssessmentMetadataEvaluation}.
			
			\item Refine the "other" condition category by breaking it down into more specific labels to improve diagnostic granularity and fairness assessment per condition.
			
			\item Incorporate healthy skin samples into the dataset to allow for a more balanced classification task and to mitigate potential bias.
			
			\item Explore whether combining PASSION with other dermatology datasets enhances generalization across the full \gls{FST} range.
		\end{itemize}
		
		
		\section{Training Process Improvements}
		To enable full reproducibility and extensibility, further work should include:
		
		\begin{itemize}
			\item Finalizing checkpoint loading support for resumed training by correctly tracking and reloading epochs and folds.
			\item Incorporating automated tests to verify linkage integrity and model reproducibility.
		\end{itemize}
		
		
		\section{Fairness Assessment Process Improvements}
		The measures to improve the fairness assessment process further are:
		\begin{itemize}
			\item Extend the existing dataset representation script, as described in \autoref{chap:datasetAssessmentExecution}, to support subgroup-level analysis and automated comparison.
			\item Replace confusion-matrix-based fairness calculations with direct \texttt{MetricFrame}-based computation to streamline and unify the process.
			\item Improve subgroup handling in the fairness evaluation pipeline to include low-support groups more reliably.
			\item Automate all metric aggregation steps and document all assumptions clearly to enhance reproducibility.
			\item Introduce the model comparison ratio by \textcite{Valentim_2019}.
		\end{itemize}		
		
		\section{Fairness Assessment Results Extension}
		\todo{@Proofreaders: habt ihr einen besseren Namen für dieses Kapitel? Es geht mir darum, dass weitere Analysen / Fairness assessments gemacht werden sollten}
		The existing fairness assessment results can be extended with those actions:
		\begin{itemize}
			\item Perform the representation analysis of relevant subgroups, as described in \autoref{chap:datasetAssessmentMethod} using the extended script, to determine whether observed unfairness stems from distribution imbalances at subgroup level.	
			
			\item Evaluate model performance across \gls{FST} types V and VI more closely and take measures if bias exist. \todo{check if this will still be needed}
			
			\item Evaluate worst-case metrics and \gls{EOR} during the fairness assessment and model comparisons as suggested in \autoref{chap:ContextFairnessMetrics}. The metric computation is already included in the script but not yet useful due to missing data.
			
			\item Incorporate multiple training seeds for each experiment (also for the baseline) for drawing statistically valid conclusions about fairness across model variations and mitigation methods.
		\end{itemize}
		
		Implementing these measures will enhance the dataset’s ability to support fair, robust, and generalizable \gls{ML} models in dermatology.
	
		\section{Code Contribution}
		The code written during this thesis will be cleaned and provided as a pull request to the PASSION GitHub project, so that the team can us it for their future work.
	
	\chapter{currently working on}
	
	

	\chapter{writing ongoing TODO REMOVE THIS CHAPTER}\todo{remove this chapter}
	\todo{put this somewhere in the outlook} Checkout this paper which suggest further methods and a flowchart to select the right fairness metric \textcite{Barr_2025}
	
	
	\chapter{Outlook PASSION Baseline Fairness Assessment}
	\todo{this must be summarized heavily}
	In summary, the evaluation infrastructure and insights established in this thesis provide a meaningful first step toward robust fairness assessments in PASSION. With technical extensions and methodological refinements, it can evolve into a comprehensive toolset for bias detection and mitigation in dermatology and beyond.
	
	The fairness evaluation presented in this thesis revealed critical insights into subgroup disparities and pipeline limitations within the PASSION project setup. While the implemented approach provided a reproducible and structured baseline, several directions for future work have emerged to improve the robustness, generalisability, and reproducibility of fairness assessments.
	
	\subsection{Metric Implementation and Evaluation Consistency}
	
	As discussed, the current fairness evaluation relies in part on confusion-matrix-based metric computations. Replacing these with direct use of \gls{Fairlearn}’s \texttt{MetricFrame} functionality across all fairness metrics would ensure a unified calculation standard and reduce implementation complexity. This would also increase consistency with other fairness research and future-proof the pipeline against methodological updates in fairness literature.
	
	\subsection{Improved Subgroup Handling}
	
	The suppression of low-support subgroups due to \gls{Fairlearn}’s internal handling and manual filtering mechanisms may bias the evaluation. This highlights the need for a more sophisticated strategy to include or resample underrepresented subgroups without compromising statistical reliability. Future efforts should aim to ensure that all relevant subgroups are fairly assessed, particularly in the context of medical datasets where representation imbalances are common.
	
	\subsection{Automation and Documentation of Aggregation}
	
	Currently, the aggregation of fairness metrics from class to subgroup to model level involves manual steps. These introduce the risk of inconsistency and reduce reproducibility. Automating these aggregation procedures with fully transparent logic and clearly documented assumptions would significantly improve the interpretability and replicability of fairness assessments. Where applicable, sensitivity analyses of aggregation strategies should be added to evaluate their impact.
	
	\subsection{Statistical Robustness through Multiple Seeds}
	
	One key limitation of this work is the single-seed evaluation due to time and computational constraints. In fairness research, especially with deep learning models, results can be sensitive to random initialization and data splits. As recommended in \textcite{Valentim_2019}, future assessments should include multiple training and evaluation runs with different seeds to draw statistically significant conclusions about model fairness and mitigation impact. The current pipeline provides a foundation for such extensions.
	
	\subsection{Library and Methodology Development}
	
	The absence of well-established multiclass fairness implementations, particularly for metrics like equalized odds, required custom adaptations. This points to a broader need within the research community to extend existing libraries like \gls{Fairlearn} to better support multiclass and subgroup-level analyses. Contributing the implemented methods and findings back to these open-source tools would benefit both the PASSION project and the wider fairness research community.
	
	\subsection{Broader Context and Integration}
	
	While this thesis focused on subgroup-level fairness analysis, future work could integrate fairness metrics into clinical utility assessments and decision-making contexts. This would enable a more holistic evaluation of trade-offs between fairness and clinical performance, which is essential for practical deployment in medical settings.
	
	
				
	\todo{add somewhere that only the conditions classifier is used in this thesis, the impetig classifier should also be checked. (code is generalized, but must be tested)}
	
	\chapter{Bias Evaluation using Equalized Odds in PASSION}
		\todo{fix this writing}
			
		\section{Evaluation and Validation - baseline}
			A first analysis shows issues regarding xy in the big model and y in the small model.
			
			There are some differences in the metrics per class based on the model size. To investigate each class individually would require more effort which should be done later. The provided scripts can be used to generate the required data. \todo{add specific info in the attachement}
			Overall, the balanced accuracy for the small model = 0.69, big model = 0.7
			
							small	big
			Macro F1-Score: 0.69	0.71
			Precision: 		0.68	0.71
			Sensitivity:	0.69	0.71
			\todo{here were the scripts linked, check github}
	
			
	\chapter{übergang zu bibliography}

		% Lists and References
		\newpage
	
	
	\chapter{\bibname}
		\printbibliography[heading=none]
		
		%\bibliographystyle{ieeetr}
		%\footnotesize\bibliography{references}
		
		
		%----------------------------------------------------------------------------------------
		%	APPENDIX
		%----------------------------------------------------------------------------------------
		\newpage
		\appendix
		\begin{appendices}
			\baaCriteria{Projektspezifisch können weitere Dokumentationsteile angefügt werden wie: Aufgabenstellung, Projektmanagement-Plan/Bericht, Testplan/Testbericht, Bedienungsanleitungen, Details zu Umfragen, detaillierte Anforderungslisten, Referenzen auf projektspezifische Daten in externen Entwicklungs- und Datenverwaltungstools etc.}
			
			\todo{fix appendices chapters, wtf}
			\todo{also move the bibliography potentially after the appendices, idk}
			%\renewcommand{\thechapter}{Appendix \Alph{chapter}}
		
	
			\chapter{PASSION Data Analysis Scripts}\label{app:PASSIONdataAnalysisScripts}
			The PASSION team provides a \gls{JupyterNotebook} with code examples and analysis scripts. They are listed in \autoref{tab:PASSION_scripts} together with their relevance to this thesis. The most relevant scripts are those related to demographic distributions of the chosen attributes, since they help identifying potential data imbalances. Scripts that lay the foundation for further analysis are somewhat relevant, while all other scripts are irrelevant for this thesis.
			
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\hsize=.25\hsize\raggedright}X>{\hsize=.41\hsize}X>{\hsize=.34\hsize}X}
						\toprule
						\textbf{Script Title}       & \textbf{Description} & \textbf{Relevance - Reasoning}       \\ \midrule
						Distribution of \glspl{FST} &
						Counts and visualizes the skin type distribution  &
						\textbf{High} - Insight into demographic distributions \\
						\hline
						Regrouping Malawi and Tanzania to EAS &
						Data aggregation due to dataset size and geographical proximity &
						\textbf{Medium} - Might impact interpretation of the results of the following scripts \\
						\hline
						Linking CSV Data with Image Files & 
						Mapping between data records and images. &
						\textbf{Medium} - Basis for other analyses \\
						\hline
						Extracting and Comparing Subject IDs &
						Dataset verification regarding completeness &
						\textbf{Low} - No insight in regards of demographic distribution \\
						\hline
						Conditions by Country &
						Correlation between clinical conditions and country &
						\textbf{Low} - The attribute \textit{country} is out of scope of this thesis \\
						\hline
						Body Localizations by Conditions &
						Correlation between the condition and primarily affected body parts &
						\textbf{Low} - No insight in regards of demographic distribution \\
						\hline
						Impetigo Cases &
						Total count of impetigo cases and proportion to all cases &
						\textbf{Low} - No insight in regards of demographic distribution\tnote{*} \\
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\item[*] Research is divided on which demographic factors influence the prevalence of impetigo \autocites{Romani_2017}{Aleid_2024}.
					\end{tablenotes}
				\end{threeparttable}
				
				\caption{PASSION dataset - existing analysis scripts \autocite{Gottfrois2024}}
				\label{tab:PASSION_scripts}
			\end{table}
			
			\input{appendix_bias_list.tex}
			
			
			\chapter{Fairness Metrics}\label{app:fairnessMetrics}
			 According to \textcite{Mehrabi_2021}, fairness can be achieved on a group level, subgroup level or even for an individual. Group fairness is about treating different groups as equal. Individual fairness tries to achieve similar predictions for similar individuals. Subgroup fairness tries to incorporate the best properties of the other two levels to improve the outcome in larger collections of subgroups \autocite{Mehrabi_2021}. 
			 
			 \autoref{tab:fairness_definitions_appendix} shows the list of fairness definitions, structured in those categories.
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Fairness Definitions} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%	\midrule
						\multicolumn{3}{l}{\textbf{Group Fairness}} \\ 
						Conditional Statistical Parity    & X &   \\
						Demographic/Statistical Parity  & X & \\
						Equal Opportunity& X &   \\
						Treatment Equality & X &   \\
						Test Fairness         & X &   \\
						Equalized Odds     & X &   \\
						%	\midrule
						\multicolumn{3}{l}{\textbf{Subgroup Fairness}} \\ 
						Subgroup Fairness    & X &   \\
						%\midrule
						\multicolumn{3}{l}{\textbf{Individual Fairness}} \\ 
						Counterfactual Fairness     & X &   \\
						Fairness Through Awareness     & X &   \\
						Fairness Through Unawareness        & X &   \\
						%\midrule
						\multicolumn{3}{l}{\textbf{Not Categorized}} \\ 
						Fairness in Relational Domains& X &   \\
						\bottomrule
					\end{tabularx}
				\end{threeparttable}
				\caption{Fairness definitions based on \textcite{Mehrabi_2021}}
				\label{tab:fairness_definitions_appendix}
			\end{table}
			
			The specific fairness definitions can be found in \textcite{Mehrabi_2021}. In general, they try to get similar probability outcomes for 'unprotected' or 'protected' groups. This list summarizes how they work:
			\begin{itemize}
				\item \textit{Demographic/Statistical Parity} and \textit{Conditional Statistical Parity}: The parity checks that the likelihood of a positive outcome is the same for both protected groups \autocite{M48_Dwork_2012,Mehrabi_2021}. The conditional version adds legitimate factors before calculating the statistical parity \autocite{M41_Corbett-Davies_2017}.
				
				\item \textit{Equalized Odds}, \textit{Test Fairness}, and \textit{Equal Opportunity}: In all these methods, protected and unprotected groups should have equal rates of positive outcomes when belonging to the positive class. These methods essentially compare the groups' \glspl{TPR}. \textit{Equalized Odds} is a more restrictive since it also checks for similar false positive rates \autocite{M149_Verma_2018,Mehrabi_2021}.
				
				\item \textit{Treatment Equality}: It compares the false negative and false positive rates \autocite{M151_Wang_2014}
				
				\item \textit{Counterfactual Fairness}: This approach is different from the others as it is testing the same individual in both different demographic groups with the intention that the outcome is the same \autocite{M87_Kusner_2017,Mehrabi_2021}. It differs from the first group of fairness metrics since it does not compare the likelihoods of the outcomes for any person in a group, but checks how the exact same individual would be treated if it was in the other group.
				
				\item \textit{Fairness Through Awareness}: This method compares similar individuals based on similarity metrics to get a similar outcome \autocite{M48_Dwork_2012,Mehrabi_2021}
				
				\item \textit{Fairness Through Unawareness}: This measure is ensuring that protected attributes are not explicitly used in decision-making \autocite{M61_Grgic-Hlaca_2016, M87_Kusner_2017}.
				
				\item \textit{Fairness in Relational Domains}: This notion also takes into consideration relational structures between individuals \autocite{M50_Farnadi_2018}.
			\end{itemize}
			
			\chapter{PASSION Dataset Distribution Analysis}\label{app:PASSIONdataDistributionAnalysis}

			The data in \autoref{tbl:PASSIONDatasetDistrAnalysis} shows the distribution of the values of the individual metadata attributes in the PASSION dataset. The data has been generated with a python script \todo{add/refer to python script}. In \autoref{fig:PASSIONDatasetDistrAnalysis}, the data is visualized.
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.9\textwidth]{figures/PASSION_split_all_distributions.png}
				\caption{PASSION dataset distribution analysis on group level}
				\label{fig:PASSIONDatasetDistrAnalysis}
			\end{figure}
			
			\begin{table}[H]
				\centering
				{
					\catcode`\_=12
					\csvautobooktabular{csvs/distribution_PASSION_split.csv}
					\catcode`\_=8
				}
				\caption{Distribution of metadata attributes in the PASSION dataset}
				\label{tbl:PASSIONDatasetDistrAnalysis}
			\end{table}
		\end{appendices}
		
		
		
			
		\glsaddallunused                                % add all unused items to glossary
		\todo{check the gls all unused.}
		
	
\end{document}
