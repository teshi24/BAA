\documentclass[12pt, a4paper, oneside]{book}   	% document style definition


\usepackage{hslu}                               % apply HSLU style
\usepackage{comment}                            % having comment sections \begin{comment} \end{comment}
\usepackage[utf8]{inputenc}						% charactere interpretation
\usepackage{amsmath}							% math package
\usepackage{amsfonts}							% font package for math symbols
\usepackage{amssymb}							% symbols package - definition of math symbols
\usepackage{listings}							% package for code representation

\usepackage{csquotes}       % Quotation support
\usepackage[style=apa,backend=biber]{biblatex}       % Bibliography, 
% todo: original template uses ieee3
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{references.bib} % Bibliography file

\usepackage{graphicx}							% for inclusion of image
\setlength {\marginparwidth }{2cm}
\usepackage{todonotes}
\renewcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
%\presetkeys{todonotes}{inline, textcolor=red, color=none, noinlinepar}{}

%\let\todoold\todo
%\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\todo[prepend, caption={TODO: #1}]{}}

%\renewcommand{\todo}[1]{\todo[inline]{\textcolor{red}{TODO: #1}}}
%\renewcommand{\todo}[1]{\todo{\textcolor{red}{TODO: #1}}}




\usepackage{booktabs}       % Better tables
\usepackage{caption}        % Better captions
\usepackage{subfig}								% to arrange figures next to each other
\usepackage{float}								% text style surrounding images
\usepackage{threeparttable}
\usepackage{tikz}								% used to place logos on title page
% \usepackage{gensymb}							% for special characters such as °
\usepackage{titlesec}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{tabularx}
\usepackage{tikzscale}
\usepackage{csvsimple}

\usepackage{appendix}

% PDF/A Compliance, todo: enable and remove hyperref usepackage afterwards
% \usepackage[a-1b]{pdfx}
% \catcode30=12

\usepackage{hyperref}
\hypersetup{hidelinks}

\newcommand{\linkchap}[1]{\hyperref[#1]{chapter~\ref{#1}~\nameref{#1}}}
\newcommand{\linkapp}[1]{\hyperref[#1]{appendix~\ref{#1}~\nameref{#1}}}

\usepackage[acronym]{glossaries}         				% package for glossary

\setcounter{tocdepth}{1}                        % hide subsections from TOC
\makenoidxglossaries
\input{acronyms}                                % include acronyms.txt file
\input{glossary}                                % include glossary.txt file
\graphicspath{{figures/}}						    % set path of graphics folder



% Format chapter titles without "Chapter X" prefix
\titleformat{\chapter}[hang]
{\normalfont\LARGE\bfseries}  % Style: Large bold text
{\thechapter}                 % Number format: Just the number
{1em}                         % Space between number and title
{}                            % Code before the title (empty)


% changed paragraph and subsection appearance
\setcounter{secnumdepth}{3}
\renewcommand{\paragraph}[1]{%
	\subsubsection*{#1}%
%	\addcontentsline{toc}{subsection}{#1}%
}


% mentioned in header
\newcommand{\tblWidthDescription}{\hsize=0.6\hsize\raggedright}
\newcommand{\tblWidthContext}{\hsize=0.2\hsize}


%improved basic functionality
\newcommand{\bolditalic}[1]{\textbf{\textit{{#1}}}}

%indicate citations
% Define a flag to track whether we're inside a raw citation block
\newif\ifrawcitationactive
\rawcitationactivefalse % Default: Not inside a raw citation block

% Define color commands with conditional checking
\newcommand{\rawcitationstart}{
	\color{purple}\rawcitationactivetrue
}
\newcommand{\rawcitationend}{
	\color{black}\rawcitationactivefalse
}

\newcommand{\rawcitationusedstart}{\color{violet}}
\newcommand{\rawcitationusedend}{%
	\ifrawcitationactive
	\color{purple}  % If inside rawcitation, reset to purple
	\else
	\color{black}  % Otherwise, reset to black
	\fi
}


% indicate info about criteria
\newcommand{\baaCriteria}[1]{\textcolor{blue}{#1}}


%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------
\author{Nadja Stadelmann}                       % author name
\city{Lucerne (Switzerland)}                    % author's place of origin
\title{Demographic Biases in\linebreak Dermatology Models}   % thesis title
\subtitle{\large \todo{subtitle}}               % thesis subtitle

\date{2025}                                     % the year when the thesis was written (for the titlepage)
\defensedate{\todo{adapt date} October 27th, 2024}                % the date of the private defense
\defencelocation{Lucerne}                       % location of defence
\extexpert{Dr. Jürg Schelldorfer}                         % name of external expert
\indpartner{Applied AI Research Lab}                       % name of industry partner

% jury, supervisor and dean are only relevant if acceptance sheet is enabled with the next line
% \acceptsheet
\jury{                                          % members of the jury
    \begin{itemize}
        \item Prof. Dr. Name Surname from Lucerne University of Applied Sciences and Arts, Switzerland (President of the Jury);
        \item Prof. Dr. Name Surname from Lucerne University of Applied Sciences and Arts, Switzerland (Thesis Supervisor);
        \item Prof. Dr. Name Surname from Lucerne University of Applied Sciences and Arts, Switzerland (External Expert).
    \end{itemize}
}

\supervisor{Dr. Ludovic Amruthalingam}             % name of supervisor
\dean{Prof. Dr. René Hüsler}                   % name of faculty dean

\acknowledgments{Thanks to my family, relatives and friends for all the support given to finish this thesis.
	\todo{add thanks and gratitude}
	Ludovic Amruthalingam
	Simone Lionetti - deputy Ludovic
	Pascal Baumann - LaTeX
	Philippe Gottfrois - information and work on PASSION project
	Proofreaders \todo{do you want to be mentioned with name or not?}
}


\begin{document}
	\english                                        % define thesis language: \german or \german
	\maketitle
	
	
	%----------------------------------------------------------------------------------------
	%	PREAMBLE
	%----------------------------------------------------------------------------------------
	\begin{abstractstyle}{\hsummary}
		\todo{Your abstract here.}
	    The content of your thesis in brief.
	\end{abstractstyle}
	
	\tableofcontents
	
	\listoftodos
	\todo{solve todos} 
	
	
	\todo{also solve todos in the code ;)} 
	
	
	\todo{also fix metadata entry!!!} 
	
	
	\todo{Portfolio DB für Referenzarbeiten anschauen}
	
	\todo{remove all \textbackslash rawcitationstart \textbackslash rawcitationend  \textbackslash baaCriteria}
	
	\todo{fix the weird line breaks}
	
	\todo{TEXT MISTAKES ensure fine-tuning ResNet-50, overview of instead of overview over, accuracy (you got all other versions of rr and cc); coma after e.g., point after vs., decision not desicion, decision-making not decision making ...}
	
	\todo{fix gls mentions in list of figures, it breaks how the glossary works}
	
	\todo{when you got to many pages: fix in order to - to, for the purpose of - For}
	
	
	\baaCriteria{Alle Fakten (fundiertes Wissen Dritter) sind korrekt zitiert. Es werden verschiedene Zitierweisen verwendet und teilweise mehrere Interpretationen gegenübergestellt. Der gemeinsam definierte Zitierstil im Text, in Abbildungen und Tabellen sowie im Literaturverzeichnis wird korrekt und durchgängig angewendet. Eigene Leistungen (sowie Bewertungen) und Fremdquellen sowie Recherchen sind klar unterscheidbar.} 
	
	
	\baaCriteria{Die erstellten Artefakte sind von sehr hoher Qualität. Das trifft u.a. auf Diagramme, Skizzen sowie Notationen (z.B. BPMN/UML) zu. Darstellungen sind einwandfrei, alle statistisch notwendigen Qualitätskriterien sind erfüllt. Beschriftungen etc. sind vorhanden, keine Einwände, Text und Bild stimmen beschreibend gut überein. Es wurden angemessene Dokumentationsmethoden und -arten korrekt verwendet. Vereinbarte Interview Transkripte, Beobachtungsprotokolle bzw. Zusammen-fassungen sind vorhanden. Daten, Ort, Kontext, Beschreibung, Zeilennummer, Verweise, Strukturen sind erkennbar, gut formatiert und korrekt mit dem Text/ der Analyse verknüpft. Alle Elemente und Themen sind im methodischen Teil/Text erklärt und verständlich, keine technischen oder strukturellen Einwände. Auch Zwischenanalysen, Zwischenschritte oder Gesamtauswertungen wurden durchgeführt, die Herkunft der Daten ist erkennbar und professionell aufbereitet.} 
	
	
	\baaCriteria{Der Schreibstil aller Dokumente entspricht hohen Standards und enthält keine Übertreibungen oder unbegründete Beurteilungen. Die Sprache ist aussagekräftig, prägnant und präzise. Die Fachterminologie ist konsistent, d.h. für gleiche Gegenstände und Themen werden immer die gleichen Begriffe verwendet. Der Sprachgebrauch ist durchgängig geschlechtergerecht, einheitlich und sachlich.}
	
	\listoffigures
	\listoftables
	% print list of acronyms and glossary
	\printnoidxglossaries
	
	\todo{fix citations in glossary}
	
	%----------------------------------------------------------------------------------------
	%	MAIN CONTENT
	%----------------------------------------------------------------------------------------
	\mainmatter
	
	% write or compose the main document here
	
	\chapter{Problem Statement}
		\baaCriteria{Welche Ziele, Fragestellungen werden mit dem Projekt verfolgt? Die Bedeutung, Auswirkung und Relevanz dieses Projektes für die unterschiedlichen Beteiligten soll aufgeführt werden. Typischerweise wird hier ein Verweis auf die Aufgabenstellung im Anhang gemacht.}
		
		In Sub-Saharan Africa dermatology treatment is inaccessible according to \textcite{Gottfrois2024}. There is fewer than one dermatologist available per one million people. Despite this, up to 80\% of the children and adolescents in the area are affected by skin conditions. \Gls{teledermatology} based on \gls{AI} promises to close this gap of specialists per case, for example by serving as a triage option. Potential patients could upload pictures to diagnostic dermatology \glspl{AI} which can indicate whether the person should indeed visit a dermatologist or promote other treatment options. However, current dermatology \glspl{AI} tend to fail to deliver accurate results for patients with highly pigmented skin tones. This is mainly due to demographic biases in existing \gls{AI} models. The models are trained on established datasets which mainly feature low pigmented skin. Therefore, the datasets lack representation of highly pigmented skin, leading to AI models which do not generalize to the population in Sub-Saharan Africa \autocite{Gottfrois2024}.
		
		These biases result in unequal access to treatment and especially affect underrepresented groups. Such biased results must be avoided, especially in AI models which impact life-changing decisions \autocite{Mehrabi_2021}.
		
		According to \textcite{Diaz2022}, demographic biases are especially important in dermatology. Demographic differences in patients influence the appearance of dermatological conditions. The differences in appearance can be developed depending on genetic factors, such as skin tone, age and sex \autocite{Diaz2022}. Research showed, that in patients with lower socio-economic status the disease progression is more advanced at time of diagnosis, which in turn can lead to different appearances for the same disease \autocite{BAD2021}. Since the AI models use pictures as the inputs and can only learn to diagnose diseases according to their appearances in the data, the factors which affects the disease appearances must be considered when creating an inclusive dataset.
		
		In order to overcome these issues, the PASSION research team founded the PASSION project. The projects vision is to make dermatology treatment accessible in Africa by enabling the AI-supported \gls{teledermatology} for triage by reducing the demographic biases in the dermatology AI models. For this bias mitigation, the researcher collected a dataset in Sub-Saharan Africa, focusing on patients with highly pigmented skin and the most common regional \gls{pediatric} skin conditions. The PASSION dataset is complementary to existing datasets and improves their diversity. With this dataset, the PASSION team trained a ResNet-50 model which was pretrained on ImageNet. This thesis refers to this trained model as the PASSION model. It should serve as a benchmark model to assess other dermatology models in regards of fairness \autocite{Gottfrois2024}. \todo{check sources, maybe, for the last sentence, the midterm protocol must be cited instead}
		
		So that the PASSION model can become an unbiased benchmark model, potential demographic biases in it must be reduced as far as possible. To reach this goal, demographic biases in the model as well as the limitation of the gathered dataset must be identified and mitigated. This thesis supports the PASSION team in this process. The main objective of the thesis is to assess the effectiveness of mitigation strategies to reduce demographic biases in context of PASSION.
		
		
		\begin{comment}
		\begin{enumerate}
			\item Identify demographic biases in dermatology AI models, using established fairness metrics.
			\item Identify mitigation strategies to minimize these biases.
			\item Assess the effectiveness of the mitigation strategies.
		\end{enumerate}
		It is important to identify the existent biases first, so that the mitigation strategies can be \todo{proceed here to reason why you chose those objectives}
		\end{comment}
		
	    \rawcitationstart
		\begin{itemize}
			\item With the advent of telemedicine, developing countries are learning newer ways of leveraging their information and communication technologies (ICTs) to play an increasingly
			vital role in the health care industry. Telemedicine is defined as a health care delivery mechanism where physicians and other medical personnel can examine patients remotely
			using information and telecommunication technologies (ICTs; Bashshur, Sanders, and Shannon, 1997). \autocite{Kifle_2024}
			\rawcitationusedstart
			\item AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations \autocite{Mehrabi_2021}.
			\rawcitationusedend
			\item There are clear benefits to algorithmic decision-making; unlike people, machines do not become tired or bored [45, 119], and can take into account orders of magnitude more factors than people can. However, like people, algorithms are vulnerable to biases that render their decisions “unfair” [6, 121]. In the context of decision-making, fairness is \textit{the absence of any prejudice or favoritism toward an individual or group based on their inherent or acquired characteristics}. Thus, an unfair algorithm is one whose decisions are skewed toward a particular group of people. \autocite{Mehrabi_2021}.
			\item it is important for researchers and engineers to be concerned about the downstream applications and their potential harmful effects when modeling an algorithm or a system \autocite{Mehrabi_2021}.
			\item We should think responsibly, and recognize that the application of these tools, and their subsequent decisions affect peoples’ lives; therefore, considering fairness constraints is a crucial task while designing and engineering these types of sensitive tools \autocite{Mehrabi_2021}.
		\end{itemize}
		\rawcitationend
	
		\begin{comment}
			
		This thesis is part of the PASSION project. The PASSION research team identified that in Africa, dermatology treatment is not accessible. There is less than one dermatologist per one million citizens. In contrast, there is high demand for dermatology treatment, especially among children and adolescents. 80\% of the \gls{pediatric} population is affected. The goal of PASSION is to make dermatology treatment more accessible by using AI supported telemedicine for triage \autocite{Gottfrois2024}.
		
		For AI supported triage, demographic biases in existing dermatology models is a problem since the corresponding datasets lack diversity, especially regarding skin tones \autocite{Gottfrois2024}. This type of bias is important in dermatology, since different diseases present themselves differently depending on the skin-color \autocite{Diaz2022}. Further, skin diseases are more advanced or severe at diagnosis in patients with lower socioeconomic status \autocite{BAD2021}.
		
		PASSION tries to mitigate the demographic bias by providing a dataset of pigmented skin images of patients from Sub-Saharan Africa. The PASSION team focused on gathering data with \gls{FST} IV, V and VI. Further, the covered conditions represent up to 80\% of the conditions in the \gls{pediatric} population, the demographic group who is most affected by skin disease \autocite{Gottfrois2024}. \gls{HSLU}
		
		The PASSION dataset is complementary to the existing datasets and improves the diversity in a combined dataset. Within the dataset itself, there could potentially be further demographic biases, e.g. related to age or gender.

		\section{Objective}
			The goal of this research is to
			\begin{enumerate}
				\item Identify demographic biases in dermatology AI models, using established fairness metrics.
				\item Identify mitigation strategies to minimize these biases.
				\item Assess the effectiveness of the mitigation strategies.
			\end{enumerate}
			It is important to identify the existent biases first, so that the mitigation strategies can be \todo{proceed here to reason why you chose those objectives}
		\end{comment}
	
	\chapter{State of Research}
		\baaCriteria{Bezogen auf die eigenen Zielsetzungen und Fragestellungen soll aufgezeigt werden, wie andere dieses oder ähnliche Probleme gelöst haben. Worauf können Sie aufbauen, was müssen Sie neu angehen?	Wodurch unterscheidet sich Ihre Lösung von anderen Lösungen? Für wissenschaftlich orientierte Arbeiten sei hier explizit auf (Balzert, S. 66 ff) verwiesen.}
		\baaCriteria{Relevante, aktuelle und fundierte Fachliteratur wurde identifiziert, kritisch geprüft und verwendet. Die Begriffe der Fragestellung sind definiert und referenziert. Der gesamte Kontext ist verknüpft und eine Abgrenzung wurde vorgenommen. All dies ist in einer leicht verständlichen Struktur formuliert und überprüft.}
		
		This chapter provides a review of existing work in the field of bias mitigation in \gls{AI}. The main focus lies on a literature review of existing papers from other researchers in this area, highlighting the key findings which are connected to this thesis. Bias mitigation in \gls{AI} has already been investigated by different researchers, who crafted fitting mitigation methods \todo{citation?}. This thesis aims to assess those existing methods in the context of PASSION.
		
		Therefore, this chapter first presents an overview of the PASSION project based on the PASSION paper and dataset. Then, the general knowledge in the literature about existing biases, fairness metrics and mitigation methods is summarized. The review process was divided into two main contexts: \gls{ML} in general, and \gls{ML} in dermatology. This approach ensures that the technical and dermatological perspectives are considered when applying the knowledge to PASSION. The tables in this chapter indicate which points were found in which context. This is important, since what may be an issue in general might not be relevant for a specific use case or vice versa. For example, in theory, all age groups should be represented in datasets to account for demographic diversity. However, for car insurance, age representation is not important, because age does not affect how well a driver can drive \todo{either cite this example from the expert or find another example related to dermatology}.
		
		The various studies present different bias sources and suggest diverse methods to mitigate them. During the literature review, several biases and mitigation methods were identified that may be relevant to the PASSION project. Since it is not feasible to assess all of them during the duration of this thesis, the thesis focuses on those which are related to skin type, age and gender. The chosen methods are explained in \linkchap{chap:methodology}. The other items are passed to the PASSION research team as a list for further investigation. The list can be found in the appendix \todo{add link}.
		
		
		\todo{put the evaluation stuff in the execution / analysis section!!}
		
		
		\section{PASSION for Dermatology}
			This section provides an overview of the PASSION project regarding its medical scope and technical components.
			
			While the overall goal remains to improve the accessibility of dermatological care by building fair and inclusive AI systems, PASSION specifically addresses common \gls{pediatric} skin conditions in Sub-Saharan Africa. To create a dataset which represents patients with highly pigmented skin, they collected data from patients with \glspl{FST} III to VI. Based on this dataset, the PASSION team fine-tuned a ResNet-50 model using transfer learning. With the dataset and trained model, the researchers published data analysis scripts and initial insights on the model performance in a MICCAI \todo{add to glossary} publication \autocite{Gottfrois2024}.
			
			For the purpose of this thesis, it is essential to understand the dataset's metadata, the architecture and fine-tuning process of the PASSION model and which bias mitigation methods have already been applied. The dataset can influence which biases could arise in the model or rather which ones can be measured. The labels which should be predicted, and the model architecture give insight into the \gls{ML} task. All this information affects which mitigation methods are feasible to be used for the project. \todo{add sources}
			
			\subsection{PASSION Dataset}
				The PASSION dataset contains data from patients from four African countries in dermatology clinics. It contains 4901 images of 1653 dermatology cases with the corresponding demographic and clinical metadata. Each patient is represented by one record, with images linked to the record via filename. The images were captured with mobile phones to ensure that the training data complies with a \gls{teledermatology} setting regarding image quality \autocite{Gottfrois2024}.
				
			    A predefined 80/20 stratified train-test split at patient level ensures reproducibility and fair comparison, while preventing information leaking \autocite{Gottfrois2024}.
			    
			    Stratified splitting is a method to split  datasets while maintaining the original class distribution within the subsets. This is important for imbalanced datasets to maintain minority class representation  \autocite{Balde_2023}. \todo{add better source for the stratified data split (and the other stuff where medium was used)}.
				
				The metadata, as listed in \autoref{tab:PASSION_metadata}, includes demographic attributes such as \textit{age}, \textit{sex}, and \textit{\gls{FST}}. These are essential for identifying potential demographic biases lateron.	The labels \textit{impetig} and \textit{conditions\_PASSION} represent dermatology diagnosis as evaluated by dermatologists \autocite{Gottfrois2024}, and are the target variables the PASSION model learns to predict. Therefore, this \gls{ML} task is a multilabel classification problem. PASSION addresses this by training separate models for each label \autocite{Gottfrois2024}. The prediction of conditions\_PASSION is a multiclass classification task, while predicting impetig is a binary classification task. 
				
				\begin{table}[H]
					\centering
					\begin{tabularx}{\textwidth}{>{\hsize=.27\hsize}X>{\hsize=.27\hsize\raggedright}X>{\hsize=.46\hsize}X}
						\toprule
						\textbf{Metadata Attribute}       & \textbf{Data Type} & \textbf{Description}       \\ \midrule
						subject\_id          & string & Participant's unique identifier        \\
						country              & string & Country of data origin \\
						age                  & integer & Age of the participant in years       \\
						sex                  & m/f/o & Gender of the participant               \\
						fitzpatrick          & integer & \gls{FST}                \\
						body\_loc            & string (list; null-able, semicolon-separated) & Specifically affected body locations \\
						impetig              & 0/1  & Presence of impetigo (1=present), may occur alone or with other conditions, affects the treatment options for coexisting conditions        \\
						conditions\_PASSION  & Eczema, Scabies, Fungal, Others & Primary diagnosed skin condition \\
						\bottomrule
					\end{tabularx}
					\caption{PASSION dataset - metadata attributes and descriptions \autocite{Gottfrois2024}}
					\label{tab:PASSION_metadata}
				\end{table}
				
				The PASSION team also provides a set of \gls{JupyterNotebook}-based data analysis scripts. For example, one script analyses the correlation between the clinical conditions and location of the data collection. A full list of these scripts is included in \linkapp{app:PASSIONdataAnalysisScripts}. Additionally, the paper visualizes demographic analyzes related to age, sex and \gls{FST} as shown in \autoref{fig:PASSIONDistr}.
				
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.9\textwidth]{figures/PASSIONDatasetDistribution.png}
					\caption{PASSION data distributions \autocite{Gottfrois2024}}
					\label{fig:PASSIONDistr}
				\end{figure}
				
				
				Due to the sensitivity of patient data, the dataset is confidential. Access to it can be requested via the project website: \href{https://passionderm.github.io/}{https://passionderm.github.io/} \autocite{Gottfrois2024}.
				
			\subsection{PASSION Model}
			  The model architecture is a ResNet-50 model which is pretrained on ImageNet. The model was fine-tuned by replacing the last fully connected classification layer with a dropout layer with a 0.3 dropout rate followed by batch normalization. The class activation is done by a single linear layer. To minimize the weighted cross-entropy loss, Adam optimization is used. For improved generalization and to avoid overfitting, data augmentations were applied. The methods used were random resizing, cropping, flipping, and rotating \textcite{Gottfrois2024}. \todo{add individual citations for ResNet-50, ImageNet, Adam optimization, weighted cross-entropy loss}.
			  
			  \todo{add information regarding how many folds are there, how is the data split, ...}
			  			
			\subsection{PASSION Experiments}
			  The PASSION team conducted various experiments to evaluate the classifiers on the test set with the following schemes \autocite{Gottfrois2024}:
			  \begin{itemize}
			  	\item Performance for skin condition prediction
			  	\item Performance for impetigo detection
			  	\item Generalization from two centers to a wider population (test set contains data from the known centers and one unknown center)
			  	\item Generalization from different age groups (test set contains data from the known age groups and one unknown)
			  	\item Subject level analysis over the predictions of multiple pictures, using majority voting
			  \end{itemize}
			  
			  The code for those experiments is available in the PASSION evaluation GitHub repo. This repo can serve as a starting point, since reproducing the results helps to verify that the provided setup works the same on my side. Also, they can be used as examples for further experiments. \todo{mention which ones I really used why for the thesis and move the others to the appendix}
			  
			  The paper indicates lower performance when evaluating the model on a subject level (performance per case/patient) rather than a sample level (performance per image). The authors emphasize the importance of assessing classifier performance on both levels for completeness \autocite{Gottfrois2024}. Therefore, the subject level performance should also be considered during this thesis.
			  \todo{challenge this to be tested again in the outlook bc of the inproper metadata linkage}
	
	
		\subsection{Limitations}
				\todo{maybe move to execution phase}
				\todo{write in more details}
			   - multiple executions showed inconsistent results for the different group evaluations on the same model checkpoint. It turned out that the metadata linkage did not work consistently. I resolved the issue was resolved by providing the image name in the data loader and link the metadata directly from the source file instead of using the indexes. probably related to different shuffling between data loader and metadata loader
		
			\begin{comment}
				\todo{probably remove this}
		
			\subsection{Telemedicine \todo{is this chapter needed?}}
				\rawcitationstart
				\begin{itemize}
					\item Teledermatology. Telemedicine may be one of the first fields to embrace AI, driven by demand for services, the necessity of collecting fit-for-purpose high-quality images, and the availability of existing technology (Xiong et al., 2019). Face-to-face diagnostic accuracy exceeds that of teledermatology (Finnane et al., 2017); however, inequalities surrounding access to dermatological care persist. Teledermatology has the potential to increase access by facilitating referrals and offering convenience and decreased wait times (Finnane et al., 2017), as well as providing diagnostic support at the time of case review. For teledermatology cases, the accuracy of a DL classifier (0.67) matched dermatologists’ (0.63) and was higher than primary care physicians’ (0.45) for 26 skin conditions (Liu et al., 2019b). AI may be integrated into smartphone apps to photograph skin lesions, collect relevant clinical information, and generate a referral if appropriate. Many smartphones already support on-device DL with Google’s TensorFlow Lite (TensorFlow, 2020) or Apple’s Core\gls{ML} (Apple Inc, 2020), preserving privacy by keeping health information on the device. A systematic review found nine studies that evaluated six algorithm-based smartphone apps and concluded that evidence of diagnostic accuracy was poor and does not support current implementation, despite two apps having obtained the CE marking; no apps are Food and Drug Administration approved (Freeman et al., 2020). AI may also assist in automatic tracking and monitoring of skin lesions; although preliminary results are promising, existing studies used small datasets with little description, and there is no established standard metric of change (Navarro et al., 2019). Further study hinges on the prospective collection of large datasets. \autocite{Young_2020}
					\item \autocite{Tsetsi_2017} on smartphone / internet access divide between populations
					\item https://www.tandfonline.com/doi/full/10.1080/08870446.2019.1579330 on how open people are to use AI
				\end{itemize}
				\rawcitationend
			\end{comment}
		
		
		\section{Bias}
			This chapter provides an overview of biases and related demographic characteristics mentioned in \gls{ML}- and dermatology-related research. It also explains their relevance for PASSION.
			
			Algorithmic decisions made by \gls{AI} systems can directly affect peoples' lives. In healthcare applications such as PASSION, these decisions are especially sensitive, as they influence diagnoses and treatment outcomes. Diverse studies have shown that \gls{AI} application's decisions can hold biases that affect underrepresented groups. This leads to unfair or even harmful consequences. Therefore, it is essential for \gls{AI} engineers to identify, address, and mitigate such biases in order to develop fair applications. This requires an understanding of what bias is in general, which concrete biases exist, and where they originate \autocite{Mehrabi_2021}.
			
			
			\begin{comment}
			\rawcitationusedstart
			\begin{itemize}
				\item Bias in facial recognition systems [128] and recommender systems [140] have also been largely studied and evaluated and in many cases shown to be discriminative towards certain populations and subgroups. In order to be able to address the bias issue in these applications, it is important for us to know where these biases are coming from and what we can do to prevent them.\autocite{Mehrabi_2021}.
				\item We should think responsibly, and recognize that the application of these tools, and their subsequent decisions affect peoples’ lives; therefore, considering fairness constraints is a crucial task while designing and engineering these types of sensitive tools \autocite{Mehrabi_2021}.
			\end{itemize}
			\rawcitationusedend
			
			\end{comment}
		
			\subsection{Definition of Bias in \gls{ML}}
		    In the context of \gls{ML}, bias can be defined as \textit{a systematic error that causes a model or estimator to consistently deviate from the true value or relationship} \autocite{Delgado-Rodriguez_2004, Taylor_2023}. In practice, this often results in models that make less accurate predictions for specific subgroups within the population \todo{cite this}.
		    			    
		  	\todo{make sure the following is cited correctly}
			
			\subsection{Demographic Biases in the Context of Dermatology} \label{chap:demographicBiasesDermatology}
			Biases in dermatology in general can lead to unequal outcomes for different groups, which can result in unfair outcomes for certain groups. Demographic biases are particularly relevant in the context of dermatology \glspl{AI}, as they can cause differences in diagnostic accuracy and treatment outcomes among different demographic (sub-)groups.
			From the literature review, three main ways have been identified in which demographic differences may introduce bias in dermatology \gls{ML} models:
			\todo{cite all that, from presentation}
			
			\begin{itemize}
				\item \textbf{Disease Presentation}. \textit{Skin type} affects how diseases appear on the skin. As Gottfrois notes, "any condition linked to inflammation is less visible if the skin is more pigmented" \todo{cite mail from philippe}. This directly influences training and evaluating image-based \gls{ML} models like those used in PASSION. For example, a model trained predominantly on images with low pigmented skin may perform poorly on images of highly pigmented skin.
				
				\item \textbf{Disease Prevalence}. Factors such as \textit{age} and \textit{sex} do not tend to affect disease presentation, but they can influence disease prevalence \todo{cite mail from philippe}. Also, \textit{geographic location} can influence the prevalence of skin conditions (e.g., tropical vs. dry climates) \todo{add source}. Therefore, these factors could introduce bias if certain conditions are underrepresented in the dataset due to demographic imbalances. \todo{consider adding smt like the car driver example here, indicating that it is not necessarily a problem due to the same disease presentation}
				
				\item \textbf{Access to Healthcare}. \textit{Socioeconomic status} or \textit{geographic location} can also introduce bias. Research shows that patients with lower socioeconomic status are often diagnosed at later stages of the disease, which may alter the visual presentation of the disease. If such cases are missing in training data, the model may fail to recognize them, leading to misdiagnosis. \todo{add example for geographic location?}.
			\end{itemize}
			
			
			To build a robust and fair \gls{ML} model, it is essential to identify and address biases linked to such protected characteristics \autocite{Mehrabi2022}.
			\todo{check that there is no duplication between PASSION dataset feature description and here}
			\todo{probably remove}
			Due to time constraints, this thesis focuses on three protected characteristics: skin type, age, and sex. These were selected based on their presence in the PASSION dataset and their influence on dermatological diagnosis and disease prevalence. Other potentially relevant features, such as geographic location and socioeconomic status, should be evaluated in future work by the PASSION team.
			
			
			\begin{comment}
				\todo{if citing is an issue: check the comment}
						
				It captures three distinct pathways through which demographic differences can introduce bias in dermatological machine learning systems:
				
				Disease Presentation — covers how diseases manifest differently on various skin types, directly affecting the visual input to image-based models.
				Disease Prevalence — focuses on who is more likely to have certain conditions, which affects label distribution in the dataset.
				Access to Healthcare — reflects when and how people enter the medical system, influencing data collection quality and representativeness.
				
				Each of these groups addresses a different layer of the data generation and learning process:
				
				Input variability (visual features),
				Target/label imbalance (class representation),
				Data collection bias (who gets diagnosed and when).
				
				This structure is also supported in literature on medical \gls{AI} fairness (e.g., in works by Obermeyer et al. or Adamson & Smith).
			\end{comment}
			
			\subsection{Types of Biases and Their Relevance for PASSION}
			The literature describes numerous types of bias. Over 60 were identified during this research. These factors were grouped into categories to provide an overview, and their relevance to the PASSION context was assessed.
			
			Among them, \textit{sampling biases} and \textit{representation biases} are particularly relevant, as they relate directly to the inclusion or exclusion of demographic subgroups in the dataset. For example, \textit{ascertainment bias}, a subtype of sampling bias, occurs when parts of the target population are unintentionally excluded. A common example is healthcare studies conducted in public hospitals only, which excludes patients from higher socioeconomic backgrounds who visit private clinics. This skews the data and can lead to incorrect conclusions, such as overestimating disease prevalence in specific groups.
			
			Other relevant categories include \textit{medical biases} and \textit{imaging biases}, especially in the teledermatology setting of PASSION. These include clinical labeling errors, variations in image quality or lighting conditions which lead to bias.
			
			
			This thesis focuses on the most relevant bias types. An extensive list is provided in \linkapp{app:listOfBiases} and will be shared with the PASSION team for further evaluation.
			
			\todo{add the 5-10 most important biases here}
			
			\subsection{Sensitive Features}
			Research has identified sensitive features that are particularly prone to bias. These features have already caused biases in existing \gls{AI} applications and should therefore be carefully evaluated during model development \autocite{Mehrabi_2021}.
			
			\autoref{tab:biases_features} summarizes sensitive features mentioned in the literature. The categorization in the table was done based on the research described in \autoref{chap:demographicBiasesDermatology}. For completeness, the table also contains sensitive demographic features which appear unrelated to dermatology according based on current research.
			
			\begin{comment}
			\todo{check what to do with those additional features:}
			Other important features according to (\autocite{Montoya_2025} 13):
			lesion type, anatomical location of lesion, img characteristics such as source, imaging techniques, resolution, real vs. artificially generated
			
			In addition to demographic factors, domain-specific variables such as lesion type, anatomical location, and image characteristics (e.g., imaging technique, resolution, device source, or whether an image is real vs. artificially generated) can also influence model behaviour \autocite{Montoya_2025}. These features are important considerations for dataset curation and model evaluation in dermatology-focused applications like PASSION.
			\end{comment}
			
			
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Bias-Sensitive Features} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%\midrule
						\multicolumn{3}{l}{\textbf{Related to Disease Presentation}} \\
						Skin Type & X\tnote{1,2,7} & X\tnote{12,13}\\
						Skin Undertones & & X\tnote{13} \\
						Socio-Economic Status & X\tnote{6} & X\tnote{12} \\
						Geographic Location \todo{double check this!} & X\tnote{1,3} & \\
						
						\multicolumn{3}{l}{\bolditalic{Related to Disease Prevalence}} \\
						Age & X\tnote{7,11} &  X\tnote{13} \\
						Gender/Sex & X\tnote{1,2,7,8,9,10,11} & X\tnote{13} \\
						Gender and Skin Type Subgroups & X\tnote{1,2} & \\
						
						\multicolumn{3}{l}{\bolditalic{Related to Access to Healthcare}} \\
						Geographic Location & X\tnote{1,3} & \\
						Socio-Economic Status & X\tnote{6} & X\tnote{12} \\
						
						\multicolumn{3}{l}{\bolditalic{Relation to Dermatology to be Checked}} \\
						Ethnicity/Race & X\tnote{1,2,4,5,6,7,11}&  X\tnote{12,13} \\
						Disabilities & X\tnote{7,11} & \\
						
						\multicolumn{3}{l}{\bolditalic{Unrelated to Dermatology}} \\
						Familial status & X\tnote{7} & \\
						Marital status & X\tnote{7,11} & \\
						Nationality/National origin & X\tnote{7,11} & \\
						Recipient of public assistance & X\tnote{7} & \\
						Religion & X\tnote{7,11} & \\
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M24_Buolamwini_2018}
							\item[3] \autocite{M142_Shankar_2017}
							\item[4] \autocite{M98_Manrai_2016}
							\item[5] \autocite{M54_Fry_2017}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[6] \autocite{M150_Vickers_2014}
							\item[7] \autocite{M30_Chen_2019}
							\item[8] \autocite{M167_Zhao_2017}
							\item[9] \autocite{M20_Bolukbasi_2016}
							\item[10] \autocite{M168_Zhao_2018}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[11] \autocite{M62_Hajian_2013}
							\item[12] \autocite{Young_2020}
							\item[13] \autocite{Montoya_2025}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Commonly used features which often are affected by biases}
				\label{tab:biases_features}
			\end{table}
			
			
			\begin{comment}
			\todo{decide which table to use, more or less extensive citations?}
			
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Bias-Sensitive Features} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%\midrule
						\multicolumn{3}{l}{\textbf{Dermatology Related Features}} \\
						Skin Type & X\tnote{1,3} & X\tnote{5,6}\\
						Skin Undertones & & X\tnote{6} \\
						
						\multicolumn{3}{l}{\textbf{Demographic Features}} \\						\multicolumn{3}{l}{\bolditalic{Relevant for Skin Disease Detection}} \\
						Age & X\tnote{3,4} &  X\tnote{6} \\
						Gender/Sex & X\tnote{1,3,4} & X\tnote{6} \\
						Gender and Skin Type Subgroups & X\tnote{1} & \\
						Ethnicity/Race & X\tnote{1,2,3,4}&  X\tnote{5,6} \\
						
						\multicolumn{3}{l}{\bolditalic{Potentially Relevant for Skin Disease Detection}} \\
						Geographic Location & X\tnote{1} & \\
						Socio-Economic Status & X\tnote{2} & X\tnote{5} \\
						Disabilities & X\tnote{3,4} & \\
						
						\multicolumn{3}{l}{\bolditalic{Not Relevant for Skin Disease Detection}} \\
						Familial status & X\tnote{3} & \\
						Marital status & X\tnote{3,4} & \\
						Nationality/National origin & X\tnote{3,4} & \\
						Recipient of public assistance & X\tnote{3} & \\
						Religion & X\tnote{3,4} & \\
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.30\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M150_Vickers_2014}
						\end{minipage}%
						\begin{minipage}{0.40\textwidth}\raggedright
							\item[3] \autocite{M30_Chen_2019}
							\item[4] \autocite{M62_Hajian_2013}
						\end{minipage}%
						\begin{minipage}{0.30\textwidth}\raggedright
							\item[5] \autocite{Young_2020}
							\item[6] \autocite{Montoya_2025}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Features which often hold biases}
				\label{tab:biases_sensitive_features}
			\end{table}
			\end{comment}
			
		\section{Fairness Metrics}
		This chapter introduces the concept of fairness in \gls{ML}, as fairness is a way to detect whether and what biases exist in a model. As there is no universally accepted definition of fairness, various fairness metrics have been proposed in the literature, each based on different assumptions and goals.
		This chapter focuses on those fairness metrics which are able to evaluate demographic fairness and are applicable to the dermatology context of PASSION.	
		Those are mainly \textit{equalized odds} by \textcite{M63_Hardt_2016} and \textit{subgroup fairness} by \textcite{M79_Kearns_2018}.
		
		
		\subsection{Definition of Fairness in \gls{ML}}
		
		In research, there is currently no common agreement regarding a fairness definition in \gls{ML}. Broadly, fairness \textit{is the absence of bias towards individuals or groups in a decision-making context}. To assess how fair \gls{AI} models are, multiple fairness metrics have been proposed in the literature, each reflecting different interpretations of fairness. The choice of metric largely depends on the specific use case of the application \autocite{Mehrabi_2021}.
		
		\subsection{Fairness Metrics}
		
			\textcite{Mehrabi_2021} summarized the fairness metrics and grouped them into the categories group fairness, subgroup fairness and individual fairness, depending on the main mechanics of the metrics. They are listed in \autoref{tab:fairness_definitions}.
			
			\todo{evtl in anhang wenn es zu viele Seiten werden}
		
			\begin{table}[H]
			\centering
			\begin{threeparttable}
				\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
					\toprule
					\textbf{Fairness Definitions} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
					& \textbf{\gls{ML}} & \textbf{Dermatology} \\
					%	\midrule
					\multicolumn{3}{l}{\textbf{Group Fairness}} \\ 
					Conditional Statistical Parity    & X &   \\
					Demographic/Statistical Parity  & X & \\
					Equal Opportunity& X &   \\
					Treatment Equality & X &   \\
					Test Fairness         & X &   \\
					Equalized Odds     & X &   \\
					%	\midrule
					\multicolumn{3}{l}{\textbf{Subgroup Fairness}} \\ 
					Subgroup Fairness    & X &   \\
					%\midrule
					\multicolumn{3}{l}{\textbf{Individual Fairness}} \\ 
					Counterfactual Fairness     & X &   \\
					Fairness Through Awareness     & X &   \\
					Fairness Through Unawareness        & X &   \\
					%\midrule
					\multicolumn{3}{l}{\textbf{Not Categorized}} \\ 
					Fairness in Relational Domains& X &   \\
					\bottomrule
				\end{tabularx}
			\end{threeparttable}
			\caption{Fairness definitions based on \textcite{Mehrabi_2021}}
			\label{tab:fairness_definitions}
		\end{table}
		
		In the context of PASSION, the fairness metrics which consider both true positives and false positives are particularly relevant. A \textit{true positive} indicates that a disease was detected correctly, while a \textit{false positive} corresponds to a diagnosis of a disease that is not actually present. Including false positives helps to identify cases where individuals from certain demographic groups may be unfairly more likely to receive unjustified diagnoses. This has also been indicated by \textcite{Sabato_2024}.
		
		From the listed group fairness metrics, there is only one that considers true and false positives, which should therefore be used for the evaluation of PASSION. It is equalized odds, as introduced by \textcite{M63_Hardt_2016}: \newline
		"\textit{A predictor $\hat{Y}$ satisfies equalized odds with respect to protected attribute $A$ and outcome $Y$, if $\hat{Y}$ and $A$ are independent conditional on $Y$. \newline
		\(
		P(\hat{Y} = 1 \mid A = 0, Y = y) = P(\hat{Y} = 1 \mid A = 1, Y = y), \quad \forall y \in \{0, 1\}
		\)"} \todo{add formula list} \newline
		In other words, the probability of predicting a positive outcome should be the same across protected and unprotected groups, given the true label $Y$. This ensures that both \gls{TPR} and \gls{FPR} are equal across different demographic groups. If these rates are the same, like in the example of \autoref{fig:eqOdds}, the model satisfies equalized odds, and fairness is achieved. Since equalized odds compares conditional probability distributions across groups, it is a group fairness metrics.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.9\textwidth]{figures/EqualizedOddsIllustration.png}
			\caption{Equalized odds mechanics, inspired by \autocite{M80_Kearns_2019}.}
			\label{fig:eqOdds}
		\end{figure}
		
		Given the specific dermatology use case in the context of PASSION, it is not clear whether individual fairness metrics would be feasible to use. Certain metrics propose to change attributes. This approach is not feasible for the skin type which is passed on to the model implicitly through the picture. Therefore, this thesis focuses on the group fairness metrics for now.
		
		The mechanics of the other fairness metrics are described broadly in \linkapp{app:fairnessMetrics}.
		
		\subsection{Limitations of Group Fairness}
		
		Despite its usefulness, equalized odds and similar group fairness metrics have limitations. These metrics can hide inequalities that exist within more specific subgroups. For example, a model might appear fair when assessed across broad groups such as age or skin type (\autoref{fig:eqOdds}) but still exhibit substantial disparities within subgroups, such as older individuals with darker skin tones (\autoref{fig:eqOddsLimits}) \autocite{M79_Kearns_2018,M80_Kearns_2019}.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.5\textwidth]{figures/EqualizedOddsSubgroupsIssueIllustration.png}
			\caption{Equalized odds violations on subgroups, inspired by \autocite{M80_Kearns_2019}.}
			\label{fig:eqOddsLimits}
		\end{figure}
		
		To address this issue, subgroup fairness metrics have been proposed. These extend group fairness metrics by explicitly evaluating fairness across subgroups. This ensures that fairness assessments do not overlook hidden biases that could affect smaller populations \autocite{M79_Kearns_2018,M80_Kearns_2019}.
		
		Given the demographic focus of this study and the composition of the PASSION dataset, subgroup fairness is particularly important. Therefore, this thesis aims to incorporate equalized odds on subgroups as a core metric for evaluation.
		
		
		\subsection{Limitations of Fairness Evaluation in Multiclass Settings with Equalized Odds}
		Fairness metrics such as equalized odds are originally defined for binary classification problems, typically considering binary labels and binary demographic groups. As a result, fairness libraries like AI Fairness 360 and Fairlearn offer implementations of these fairness metrics only for binary classification tasks \todo{cite relevant sources} \todo{explain fairness libraries and add the individual ones to the glossar}. To evaluate fairness in multiclass settings using these libraries, certain considerations are required. This chapter introduces the two key challenges, handling multiclass labels and multiple subgroups.		
		
		\subsubsection{Multiclass Labels}
		In binary settings, fairness can be evaluated through simple comparisons of false positive and false negative rates. However, in multiclass classification, fairness must account for the full structure of the confusion matrix. \textcite{Sabato_2024} generalizes equalized odds to multiclass classification by defining:
		\textit{"For each \( y, z \in \mathcal{Y} \), the value of \( \mathbb{P}[\hat{Y} = z \mid Y = y, G = g] \) is the same for all \( g \in \mathcal{G} \)."}
		
		In practice, this means the entire confusion matrix must be equal across groups to satisfy strict multiclass fairness under equalized odds \autocite{Sabato_2024}. The similar approach is purposed by \textcite{Putzel_2022}.
		
		More relaxed versions of multiclass equalized odds have also been proposed in the literature. However, researchers argue that such relaxations may not be suitable in all contexts, especially when different types of errors carry different consequences \autocites{Sabato_2024}{Putzel_2022}.
		
		For instance, when the type of misclassification matters, equality of error rates is essential to ensure fairness, as noted by \textcite{Putzel_2022}. Furthermore, as \textcite{Sabato_2024} explicitly states, a fair classifier in healthcare should avoid differences in diagnosis errors for specific diseases across subgroups, since misdiagnoses can lead to different treatment outcomes. Therefore, in PASSION, the strict version of the multiclass equalized odds should be preferred.
		
		\subsubsection{Non-Binary Sensitive Features}
		There can also be non-binary sensitive features leading to multiple subgroups. The original definition of equalized odds does not account for this complexity. To generalize fairness evaluation to such settings, a one-vs-rest strategy can be applied. In this approach, each group is individually compared against the rest of the population \autocite{Nezami_2024}.
		
		
			
		\section{Mitigation Methods}
			 \todo{still to be written}
			 
			 \begin{comment}
			 see text from bias chapter - Further, \gls{AI} engineers need to know what prevention methods are available to reduce the biases \autocite{Mehrabi_2021}.
			 
			 
			
			
		\section{Extensive Sources}
			\subsection{Mitigation Methods Overview}
				\todo{write definitions of pre-in and post-processing, see Methods for fair machine learning below [43, 11, 14]}


				\todo{add stratified split}
				\todo{double check and futher improve groups}
				\begin{table}[H]
				\centering
				\begin{threeparttable}
						\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Mitigation Methods - Unbiasing Data (Pre-Processing)} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						\multicolumn{3}{l}{\textbf{Documentation and Transparency}} \\
						Good Practices while using Data & X\tnote{1,2,3} &   \\
						Datasheets as supporting document for dataset creation method, characteristics, motivations and skews & X\tnote{1,2,3} &   \\
						Datasheets as supporting document for model method, characteristics, motivations and skews & X\tnote{1,4} &   \\
						Dataset (Nutrition) Labels & X\tnote{1,5,6} & X\tnote{18, \todo{add spec source}}   \\
						
						\multicolumn{3}{l}{\textbf{Communication and Reporting}} \\
						Messaging & X\tnote{1,12} &   \\
						
						\multicolumn{3}{l}{\textbf{Bias Detection and Evaluation}} \\
						Test for Simpson's Paradox \todo{Discribe Simpson's Paradox} & X\tnote{1,7,8,9} &   \\
						Detect Direct Discrimination with Causal Models and Graphs & X\tnote{1,10} &   \\					
						Out-of-Distribution Detection in Dermatology Using Input Perturbation and Subset Scanning & & X\tnote{19} \\
						Check confidence interval and p-curve analysis instead of p-value & & X\tnote{17} \\
						 
						\multicolumn{3}{l}{\textbf{Study Design}} \\ 
						Allocation concealment and blinding & & X\tnote{17} \\
						Preventing Direct and Indirect Discrimination & X\tnote{1,11} &   \\
						
						\multicolumn{3}{l}{\textbf{Data Gathering}} \\ 
						Data Collection from diverse sources (incl. primary care clinics) & X\tnote{18} & \\
						Robust standards for external validation & X\tnote{18} & \\
						Preferential Sampling & X\tnote{1,13,14} &   \\
						Geographical Diversity and Inclusion for Dataset creation & X\tnote{16} & \\
						Balanced Representation accross skin tones and genders & & X\tnote{19} \\
						Disparate Impact Removal & X\tnote{1,15} &   \\
						
						\multicolumn{3}{l}{\textbf{Labeling}} \\ 
						Multidimensional Scale for Skin Tones & & X\tnote{19} \\
						
						
						\multicolumn{3}{l}{\textbf{Data Availability and Open Science}} \\ 
						Publish Datasets accessible for the public & & X\tnote{18, \todo{add source}} \\						
						
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M13_}
							\item[3] \autocite{M55_}
							\item[4] \autocite{M110_}
							\item[5] \autocite{M66_}
							\item[6] \autocite{M66Successor_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[7] \autocite{M81_}
							\item[8] \autocite{M3_}
							\item[9] \autocite{M4_}
							\item[10] \autocite{M163_}
							\item[11] \autocite{M62_Hajian_2013}
							\item[12] \autocite{M74_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[13] \autocite{M75_}
							\item[14] \autocite{M76_}
							\item[15] \autocite{M51_}
							\item[16] \autocite{M142_Shankar_2017}
							\item[17] \autocite{Chakraborty_2024}
							\item[18] \autocite{Young_2020}
							\item[19] \autocite{Montoya_2025}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Mitigation Methods - Unbiasing Data - Mentioned in Contextual Research, grouped like in \textcite{Mehrabi_2021}, the author cannot guarantee for completeness}
				\label{tab:mitigation_methods_unbiasing_data}
			\end{table}
				
				
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Mitigation Methods - Fair Classification} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%	\midrule
						
						\multicolumn{3}{l}{\textbf{Satisfy Fairness Definitions}} \\ 
						Satisfy Subgroup Fairness  \todo{unclear if \tnote{*} in \tnote{3} as well, or if \tnote{2} also handles \tnote{*}} & X\tnote{1,2} &   \\
						Satisfy Equality of Opportunity\tnote{*} & X\tnote{1,3,6} & \\					
						Satisfy Equalized Odds\tnote{*} & X\tnote{1,3} &   \\
						Disparate Treatment\tnote{**} & X\tnote{1,4,5} &  \\
						Disparate Impact\tnote{**} & X\tnote{1,4,5} &  \\
						\todo{find out exact method} & X\tnote{1,7} &  \\
						\todo{find out exact method} & X\tnote{1,8} &  \\
						\todo{find out exact method} & X\tnote{1,9} &  \\
						\todo{find out exact method} & X\tnote{1,10} &  \\
						
						\multicolumn{3}{l}{\textbf{Satisfy Fairness and Stability Under Distribution Shifts}} \\ 
						\todo{find out exact method} & X\tnote{1,11} &  \\
						
						\multicolumn{3}{l}{\textbf{Fair Representation Learning (Pre/In-processing)}} \\ 
						Representation Learning by Disentanglement & X\tnote{1,2} &   \\
						Variational Fair Autoencoder & X\tnote{1,3,15} &   \\
						VAE without adversarial training & X\tnote{1,4} &   \\
						Adversial Learning with FairGAN & X\tnote{1,16} &   \\
						Removing correlation between protected and unprotected features with a geometric solution & X\tnote{1,17} &   \\
						
						\multicolumn{3}{l}{\textbf{Algorithmic Adaptions for Fairness}} \\ 
						Modified Discrimination-Free Naive Bayes Classifier & X\tnote{1,12} &  \\
						
						\multicolumn{3}{l}{\textbf{Fairness-Aware \gls{ML} Frameworks}} \\ 
						Fairness-Aware Classification Framework & X\tnote{1,13} &  \\
						Fairness Constraints in Multitask Learning (MTL) Framework & X\tnote{1,14} &  \\
						Decoupled Classification System with Transfer Learning & X\tnote{1,15} &  \\
						
						\multicolumn{3}{l}{\textbf{Preferential Data Selection and Representation}} \\ 
						Wasserstein Distance Measure for Dependence Mitigation & X\tnote{1,16} &  \\
						Preferential Sampling (PS) for Discrimination-Free Training Data & X\tnote{1,17} &  \\
						
						\multicolumn{3}{l}{\textbf{Model Interpretability}} \\ 
						Post-Processing with Attention Mechanism & X\tnote{1,18} &  \\
						Use Brier Score and Response Rate Accuracy & & X\tnote{19, \todo{add clear source}} \\
						some more methods \todo{describe} & & X\tnote{19} \\
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[*] possible to satisfy together
							\item[**] possible to satisfy together
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M147_}
							\item[3] \autocite{M63_Hardt_2016}
							\item[4] \autocite{M2_}
							\item[5] \autocite{M159_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[6] \autocite{M154_}
							\item[7] \autocite{M57_}
							\item[8] \autocite{M78_}
							\item[9] \autocite{M85_}
							\item[10] \autocite{M106_}
							\item[11] \autocite{M69_}
							\item[12] \autocite{M25_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[13] \autocite{M155_}
							\item[14] \autocite{M12_}
							\item[15] \autocite{M49_}
							\item[16] \autocite{M73_}
							\item[17] \autocite{M75_}
							\item[18] \autocite{M102_}
							\item[19] \autocite{Young_2020}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Mitigation Methods - Fair Classification - Mentioned in Contextual Research, grouped like in \textcite{Mehrabi_2021}, the author cannot guarantee for completeness}
				\label{tab:mitigation_methods_fair_classification}
			\end{table}
			
			\todo{check categorization}
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Mitigation Methods - not so relevant for us} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%	\midrule
						\multicolumn{3}{l}{\textbf{Fair NLP}} \\ 
						Fair Word-Embedding & X\tnote{1,5,6,7} &   \\
						Train-Time Data Augmentation & X\tnote{1,8} &   \\
						Test-Time Neutralization & X\tnote{1,8} &   \\
						
						%	\midrule	
						\multicolumn{3}{l}{\textbf{Fair Regression (In-processing)}} \\ 
						Price of Fairness (POF) & X\tnote{1,10} & \\
						XY \todo{check this} and bounded group loss & X\tnote{1,11} & \\
						Decision Tree for Disparate Impact and Treatment & X\tnote{1,12} & \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Structured Prediction (In-processing)}} \\ 
						Reducing Bias Amplification (RBA) as calibration algorithm & X\tnote{1,13} & \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Principal Component Analysis (PCA) (In-processing)}} \\ 
						Fair PCA & X\tnote{1,14} & \\
						
						\multicolumn{3}{l}{\textbf{Graph-Based Fairness Methods}} \\ 
						Community Detection / Graph Embedding  \todo{how to proceed with this} & X\tnote{} & \\
						
						\multicolumn{3}{l}{\textbf{Causal Fairness and Disparate Learning}} \\ 
						Disparate Learning Processes (DLP) & X\tnote{1,9} &   \\
						Causal Approach to Fairness \todo{how to proceed with this} & X\tnote{\todo{add clear source}}  & \\
						Disregard path in causal graph which result in sensitive attributes affecting decision outcome & X\tnote{1} &   \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Removing Sensitive Attributes}} \\ 
						Disregard sensitive attributes in effect on decision-making & X\tnote{1} &   \\						
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M42_}
							\item[3] \autocite{M97_}
							\item[4] \autocite{M112_}
							\item[5] \autocite{M20_Bolukbasi_2016}
							\item[6] \autocite{M58_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[7] \autocite{M169_}
							\item[8] \autocite{M166_}
							\item[9] \autocite{M94_}
							\item[10] \autocite{M14_}
							\item[11] \autocite{M1_}
							\item[12] \autocite{M2_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[13] \autocite{M167_Zhao_2017}
							\item[14] \autocite{M137_}
							\item[15] \autocite{M5_}
							\item[16] \autocite{M90_}
							\item[17] \autocite{M65_}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Mitigation Methods - Others - Mentioned in Contextual Research, grouped like in \textcite{Mehrabi_2021}, the author cannot guarantee for completeness}
				\label{tab:mitigation_methods_others}
			\end{table}
			
			\todo{mention also the IBM AI Fairness 360 toolkit [11] and that authors evaluated their work in benchmark datasets [65], [72], [158], [159]}
			
			
			
			\todo{draft for presentation}
			satisfy Equalized Odds / Subgroup fairness
			highlight allocation concealment and blinding and data collection from diverse sources and Preferential Sampling
			\subsection{Mitigation Methods Overview}
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Mitigation Methods} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						\multicolumn{3}{l}{\bolditalic{Unbiasing Data}} \\
						Documentation and Transparency & X\tnote{1} & X\tnote{3} \\
						Bias Detection and Evaluation & X\tnote{1} & X\tnote{2,4} \\ % simpsons paradoxon, subset scanning, input pertubation
						Study Design & X\tnote{1} & X\tnote{2} \\ % allocation concealment and blinding, preventing direct and indirect discrimination
						Data Gathering & X\tnote{1} & X\tnote{3,4} \\ % data collection from diverse sources, robuster standards, 
						Data Availability and Open Science &  & X\tnote{3} \\
					    Removing Sensitive Attributes & X\tnote{1} &  \\
						\multicolumn{3}{l}{\bolditalic{Fair Classification}} \\
						Satisfy Fairness Definitions & X\tnote{1} &  \\ % satisfy Equalized Odds / Subgroup fairness
						Satisfy Fairness and Stability Under Distribution Shifts & X\tnote{1} & \\
						Fair Representation Learning & X\tnote{1} & \\
						Fairness-Aware \gls{ML} Frameworks & X\tnote{1} & \\
						Preferential Data Selection and Representation & X\tnote{1} & \\
						Model Interpretability & X\tnote{1} & X\tnote{3} \\
						\multicolumn{3}{l}{\bolditalic{For Other \gls{ML} Algorithm Types}} \\
						Fair NLP & X\tnote{1} &  \\
						Fair Regression & X\tnote{1} &  \\
						Structured Prediction & X\tnote{1} &  \\
						Fair Principal Component Analysis & X\tnote{1} &  \\
						Graph-Based Fairness Methods & X\tnote{1} &  \\
						Causal Fairness and Disparate Learning & X\tnote{1} &  \\
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{Chakraborty_2024}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[3] \autocite{Young_2020}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[4] \autocite{Montoya_2025}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Mitigation Methods - Draft}
				\label{tab:mitigation_methods_unbiasing_data_praesi}
			\end{table}
			
			\todo{check categorization}
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Mitigation Methods - not so relevant for us} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%	\midrule
						\multicolumn{3}{l}{\textbf{Fair NLP}} \\ 
						Fair Word-Embedding & X\tnote{1,5,6,7} &   \\
						Train-Time Data Augmentation & X\tnote{1,8} &   \\
						Test-Time Neutralization & X\tnote{1,8} &   \\
						
						%	\midrule	
						\multicolumn{3}{l}{\textbf{Fair Regression (In-processing)}} \\ 
						Price of Fairness (POF) & X\tnote{1,10} & \\
						XY \todo{check this} and bounded group loss & X\tnote{1,11} & \\
						Decision Tree for Disparate Impact and Treatment & X\tnote{1,12} & \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Structured Prediction (In-processing)}} \\ 
						Reducing Bias Amplification (RBA) as calibration algorithm & X\tnote{1,13} & \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Principal Component Analysis (PCA) (In-processing)}} \\ 
						Fair PCA & X\tnote{1,14} & \\
						
						\multicolumn{3}{l}{\textbf{Graph-Based Fairness Methods}} \\ 
						Community Detection / Graph Embedding  \todo{how to proceed with this} & X\tnote{} & \\
						
						\multicolumn{3}{l}{\textbf{Causal Fairness and Disparate Learning}} \\ 
						Disparate Learning Processes (DLP) & X\tnote{1,9} &   \\
						Causal Approach to Fairness \todo{how to proceed with this} & X\tnote{\todo{add clear source}}  & \\
						Disregard path in causal graph which result in sensitive attributes affecting decision outcome & X\tnote{1} &   \\
						
						%	\midrule
						\multicolumn{3}{l}{\textbf{Removing Sensitive Attributes}} \\ 
						Disregard sensitive attributes in effect on decision-making & X\tnote{1} &   \\						
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[1] \autocite{Mehrabi_2021}
							\item[2] \autocite{M42_}
							\item[3] \autocite{M97_}
							\item[4] \autocite{M112_}
							\item[5] \autocite{M20_Bolukbasi_2016}
							\item[6] \autocite{M58_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[7] \autocite{M169_}
							\item[8] \autocite{M166_}
							\item[9] \autocite{M94_}
							\item[10] \autocite{M14_}
							\item[11] \autocite{M1_}
							\item[12] \autocite{M2_}
						\end{minipage}%
						\begin{minipage}{0.33\textwidth}\raggedright
							\item[13] \autocite{M167_Zhao_2017}
							\item[14] \autocite{M137_}
							\item[15] \autocite{M5_}
							\item[16] \autocite{M90_}
							\item[17] \autocite{M65_}
						\end{minipage}%
					\end{tablenotes}
				\end{threeparttable}
				\caption{Mitigation Methods - Others - Mentioned in Contextual Research, grouped like in \textcite{Mehrabi_2021}, the author cannot guarantee for completeness}
				\label{tab:mitigation_methods_others}
			\end{table}
			
			
			
			
		\rawcitationstart
		\subsection{Mitigation Methods Extensive Sources}
			
			\paragraph{Bias Examples and Mitigation Ideas}
			Data bias examples and mitigation ideas
			\begin{itemize}
				\item Bias in \gls{ML} Data - \autocite{M24_Buolamwini_2018} IJB-A / Adience imbalanced (mainly light-skinned subjects) - Bias towards dark-skinned groups (underrepresented). Other instance - when we do not consider different subgroups in the data. Considering only male-female groups not enough, use race to further subdivide gender groups. Only then, clear biases in sub groups can be found, since otherwise part of the groups would  compromise the other group and hide the underlaying bias towards that subgroup \autocite{Mehrabi_2021}
				\rawcitationusedstart
				\item Popular machine-learning datasets that serve as a base for most of the developed algorithms and tools can also be biased—which can be harmful to the downstream applications that are based on these datasets. ... In [\autocite{M142_Shankar_2017}, researchers showed that these datasets suffer from representation bias and advocate for the need to incorporate geographic diversity and inclusion while creating such datasets. \autocite{Mehrabi_2021}
				\rawcitationusedend
				\item Examples of Data Bias in Medical Applications. These data biases can be more dangerous in other sensitive applications. For example, in medical domains there are many instances in which the data studied and used are skewed toward certain populations—which can have dangerous consequences for the underrepresented communities. [98] showed how exclusion of African-Americans resulted in their misclassification in clinical studies, so they became advocates for sequencing the genomes of diverse populations in the data to prevent harm to underrepresented populations \autocite{Mehrabi_2021} \todo{What does sequencing data mean?, is it relevant}
			\end{itemize}
			
			\paragraph{Methods for Fair Machine Learning}
			\begin{itemize}
				\item While this section is largely domain-specific, it can be useful to take a cross-domain view. Generally, methods that target biases in the algorithms fall under three categories \autocite{Mehrabi_2021}
				\item Pre-processing. Pre-processing techniques try to transform the data so that the underlying discrimination is removed [43]. If the algorithm is allowed to modify the training data, then pre-processing can be used [11].\autocite{Mehrabi_2021}
				\item In-processing. In-processing techniques try to modify and change state-of-the-art learning algorithms in order to remove discrimination during the model training process [43]. If it is allowed to change the learning procedure for a machine learning model, then in-processing can be used during the training of a model— either by incorporating changes into the objective function or imposing a constraint [11, 14].\autocite{Mehrabi_2021}
				\item Post-processing. Post-processing is performed after training by accessing a holdout set which was not involved during the training of the model [43]. If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-processing can be used in which the labels assigned by the black-box model initially get reassigned based on a function during the post-processing phase [11, 14].\autocite{Mehrabi_2021}
				\item we concentrate on discrimination prevention based on preprocessing, because the preprocessing approach seems the most flexible one: it does not require changing the standard data mining algorithms, unlike the inprocessing approach, and it allows data publishing (rather than just knowledge publishing), unlike the postprocessing approach. \autocite{M62_Hajian_2013} --> \todo{this is an important point which we should consider for PASSION, also, some more insight in regards of the different phases can be found in this paper}
				
				
				\item From learning fair representations [42, 97, 112] to learning fair word embeddings [\autocite{M20_Bolukbasi_2016}, 58, 169], debiasing methods have been proposed in different \gls{AI} applications and domains. \autocite{Mehrabi_2021} --> seems to refer mostly to NLP domains
				\item Most of these methods try to avoid unethical interference of sensitive or protected attributes into the decision-making process, while others target exclusion bias by trying to include users from sensitive groups. \autocite{Mehrabi_2021}
				\item However, a recent paper [58] argues against these debiasing techniques and states that many recent works on debiasing word embeddings have been superficial, that those techniques just hide the bias and don’t actually remove it. \autocite{Mehrabi_2021}
				\item some works try to satisfy one or more of the fairness notions in their methods, such as disparate learning processes (DLPs) which try to satisfy notions of treatment disparity and impact disparity by allowing the protected attributes during the training phase but avoiding them during prediction time [94].\autocite{Mehrabi_2021}
				\item Some of the existing work tries to treat sensitive attributes as noise to disregard their effect on decision-making, while some causal methods use causal graphs, and disregard some paths in the causal graph that result in sensitive attributes affecting the outcome of the decision.\autocite{Mehrabi_2021}
				\item Different bias-mitigating methods and techniques are discussed below for different domains—each targeting a different problem in different areas of machine learning in detail. \autocite{Mehrabi_2021}
			\end{itemize}
			
			\subparagraph{Unbiasing Data}
				\begin{itemize}
					\item Every dataset is the result of several design decisions made by the data curator. Those decisions have consequences for the fairness of the resulting dataset, which in turn affects the resulting algorithms. In order to mitigate the effects of bias in data, some general methods have been proposed that advocate having good practices while using data, such as having datasheets that would act like a supporting document for the data reporting the dataset creation method, its characteristics, motivations, and its skews [13, 55]. A similar suggestion has been proposed for models in [110].\autocite{Mehrabi_2021}
					\item Authors in [66] also propose having labels, just like nutrition labels on food, in order to better categorize each data for each task. \autocite{Mehrabi_2021}
					\item some work has targeted more specific types of biases. For example, [81] has proposed methods to test for cases of Simpson’s paradox in the data, and [3, 4] proposed methods to discover Simpson’s paradoxes in data automatically. \autocite{Mehrabi_2021}
					\item Causal models and graphs were also used in some work to detect direct discrimination in the data along with its prevention technique that modifies the data such that the predictions would be absent from direct discrimination [163].\autocite{Mehrabi_2021}
					\item in [\autocite{M62_Hajian_2013}] also worked on preventing discrimination in data mining, targeting direct, indirect, and simultaneous effects.\autocite{Mehrabi_2021}
					\item Other pre-processing approaches, such as messaging [74], preferential sampling [75, 76], disparate impact removal [51], also aim to remove biases from the data. \autocite{Mehrabi_2021}
					
					
					\item Image quality. Several barriers to \gls{AI} implementation in the clinic need to be overcome with regards to imaging (Figure 1). These include technical variations (e.g., camera hardware and software) and differences in image acquisition and quality (e.g., zoom level, focus, lighting, and presence of hair). For example, the presence of surgical ink markings is associated with decreased specificity (Winkler et al., 2019), field of view can significantly affect prediction quality (Mishra et al., 2019), and classification performance improves when hair and rulers are removed (Bisla et al., 2019). We have developed a method to measure how model predictions might be biased by the presence of a visual artifact (e.g., ink) and proposed methods to reduce such biases (Pfau et al., 2019). Poor quality images are often excluded from studies, but the problem of what makes an image adequate is not well studied. Ideally, models need to be able to express a level of confidence in a prediction as a function of image quality and appropriately direct a user to retake photos if needed. \autocite{Young_2020} - dermatology
				\end{itemize}
			
			\subparagraph{Fair Classification}
				\begin{itemize}
					\item certain methods have been proposed [57, 78, 85, 106] that satisfy certain definitions of fairness in classification. For instance, in [147] authors try to satisfy subgroup fairness in classification, equality of opportunity and equalized odds in [63], both disparate treatment and disparate impact in [2, 159], and equalized odds in [154]. \autocite{Mehrabi_2021}
					\item Other methods try to not only satisfy some fairness constraints but to also be stable toward change in the test set [69] \autocite{Mehrabi_2021}
					\item The authors in [155], propose a general framework for learning fair classifiers. This framework can be used for formulating fairness-aware classification with fairness guarantees.
					In another work [25], authors propose three different modifications to the existing Naive Bayes classifier for discrimination-free classification.\autocite{Mehrabi_2021}
					\item paper [122] takes a new approach into fair classification by imposing fairness constraints into a Multitask learning (MTL) framework. In addition to imposing fairness during training, this approach can benefit the minority groups by focusing on maximizing the average accuracy of each group as opposed to maximizing the accuracy as a whole without attention to accuracy across different groups. In a similar work [49], authors propose a decoupled classification system where a separate classifier is learned for each group. They use transfer learning to reduce the issue of having less data for minority groups.\autocite{Mehrabi_2021}
					\item In [73] authors propose to achieve fair classification by mitigating the dependence of the classification outcome on the sensitive attributes by utilizing the Wasserstein distance measure.\autocite{Mehrabi_2021}
					\item In [75] authors propose the Preferential Sampling (PS) method to create a discrimination free train data set. They then learn a classifier on this discrimination free dataset to have a classifier with no discrimination.\autocite{Mehrabi_2021}
					\item In [102], authors propose a post-processing bias mitigation strategy that utilizes attention mechanism for classification and that can provide interpretability. \autocite{Mehrabi_2021}
				\end{itemize}
				
			\subparagraph{Fair Regression}
				\todo{only summarize briefly, as PASSION is a classification and not a regression task}
				\begin{itemize}
					\item “price of fairness” (POF) to measure accuracy-fairness trade-offs, 3 penalites: Individual fairness, group fairness and hybrid fairness [14] \autocite{Mehrabi_2021}
					\item In addition to the previous work, [1] considers the fair regression problem formulation with regards to two notions of fairness statistical (demographic) parity and bounded group loss. [2] uses decision trees to satisfy disparate impact and treatment in regression tasks in addition to classification. \autocite{Mehrabi_2021}
				\end{itemize}
			\subparagraph{Structured Prediction}
				\todo{only summarize briefly, as PASSION is a classification task}
				\begin{itemize}
					\item RBA (reducing bias amplification) as calibration algorithm to prevent risk of leveraging social bias, distributions in training data are followed in the predictions. multi-label obeject and visual semantic role labeling classification amplify existing bias in data [\autocite{M167_Zhao_2017}] \autocite{Mehrabi_2021} --> \todo{be careful with this if the approach would be to generate new images for training!!}
				\end{itemize}
			\subparagraph{Fair PCA}
				\todo{only summarize briefly, as PASSION is a classification task with only like 10 features}
				\begin{itemize}
					\item Pincipal Component Analysis (PCA) https://www.geeksforgeeks.org/principal-component-analysis-pca/ --> dimensionality reduction, statistical technic, high-dimensional data into lower-dimensional space while maximising variance in new space -> most important patterns and relationships is preserved
					\item vanilla PCA exaggerate error in reconstruction for one group of people [137] \autocite{Mehrabi_2021}
					\item And their proposed algorithm is a two-step process listed below: (1) Relax the Fair PCA objective to a semidefinite program (SDP) and solve it. (2) Solve a linear program that would reduce the rank of the solution. [137] \autocite{Mehrabi_2021}
				\end{itemize}
			\subparagraph{Community Detection}
				\todo{use this as an example for out of scope text, - Ludovic approved}
				Community detection algorithms are specifically tailored to analyze network data and find connections in such datasets. For example, they can be used to detect groups of people with similar interest in social networks \autocite{Jayawickrama_2021}. This kind of data is not found in the context of PASSION, which is a classification task. Please refer to \textcite{Mehrabi_2021} for more information on bias mitigation in community detection algorithms.
				
			\subparagraph{Causal Approach to Fairness}
				\todo{only relevant, if our variables have a dependency on the variables, e.g. age / gender determines how the disease is presenting itself in the images; check \autocite{Mehrabi_2021} page 18 if relevant}
				
			\subparagraph{Fair Representation Learning}
				https://medium.com/superlinear-eu-blog/representation-learning-breakthroughs-what-is-representation-learning-5dda2e2fed2e
				\begin{itemize}
					\item Variational Auto encoders --> Variational Fair Autoencoder introduced in [97]. Here,they treat the sensitive variable as the nuisance variable, so that by removing the information about this variable they will get a fair representation. They use a maximum mean discrepancy regularizer to obtain invariance in the posterior distribution over latent variables. Adding this maximum mean discrepancy (MMD) penalty into the lower bound of their VAE architecture satisfies their proposed model for having the Variational Fair Autoencoder. \newline
					In [5] authors also propose a debiased VAE architecture called DB-VAE which learns sensitive latent variables that can bias the model (e.g., skin tone, gender, etc.) and propose an algorithm on top of this DB-VAE using these latent variables to debias systems like facial detection systems. \newline
					In [112] authors model their representation-learning task as an optimization objective that would minimize the loss of the mutual information between the encoding and the sensitive variable. The relaxed version of this assumption is shown in Equation 1. They use this in order to learn fair representation and show that adversarial training is unnecessary and in some cases even counter-productive. \newline
					In [42], authors introduce flexibly fair representation learning by disentanglement that disentangles information from multiple sensitive attributes. Their flexible and fair variational autoencoder is not only flexible with respect to downstream task labels but also flexible with respect to sensitive attributes. They address the demographic parity notion of fairness, which can target multiple sensitive attributes or any subset combination of them. \autocite{Mehrabi_2021}
					\item Adversarial Learning - In [90] authors present a framework to mitigate bias in models learned from data with stereotypical associations. using adversarial networks by introducing FairGAN which generates synthetic data that is free from discrimination and is similar to the real data. They use their newly generated synthetic data from FairGAN, which is now debiased, instead of the real data for training and testing. They do not try to remove discrimination from the dataset, unlike many of the existing approaches, but instead generate new datasets similar to the real one which is debiased and preserves good data utility. \autocite{Mehrabi_2021} \todo{address challenges in creating synthetic data in dermatology?}
				\end{itemize}
			
			\subparagraph{Fair NLP}
				\todo{for PASSION irrelevant, if it wants to stick to ResNet50 Architecture \autocite{Gottfrois2024} and not use Visual Encoders, which would make sense bc of the small dataset}
				\begin{itemize}
					\item Word Embedding \todo{potentially relevant, if the labels are used in training, e.g. age / gender determines how the disease is presenting itself in the images; check \autocite{Mehrabi_2021} page 21 if relevant}
					\item Coreference Resolution "Coreference resolution involves identifying when two or more expressions in a text refer to the same entity, be it a person, place, or thing." https://medium.com/@datailm/the-key-to-unlocking-true-language-understanding-coreference-resolution-c01d569e2e87 \todo{irrelevant for the PASSION Context}
				\end{itemize}
				
			\paragraph{comparison of different mitigation algorithms}
				\begin{itemize}
					\item The field of algorithmic fairness is a relatively new area of research and work still needs to be done for its improvement. With that being said, there are already papers that propose fair \gls{AI} algorithms and bias mitigation techniques and compare different mitigation algorithms using different benchmark datasets in the fairness domain. For instance, authors in [65] propose a geometric solution to learn fair representations that removes correlation between protected and unprotected features. The proposed approach can control the trade-off between fairness and accuracy via an adjustable parameter. In this work, authors evaluate the performance of their approach on different benchmark datasets, such as COMPAS, Adult and German, and compare them against various different approaches for fair learning algorithms considering fairness and accuracy measures [65, 72, 158, 159]. In addition, IBM’s \gls{AI} Fairness 360 (AIF360) toolkit [11] has implemented many of the current fair learning algorithms and has demonstrated some of the results as demos which can be utilized by interested users to compare different methods with regards to different fairness measures. \autocite{Mehrabi_2021}
				\end{itemize}			
			
		\subsection{Statistical biases}
			https://data36.com/statistical-bias-types-explained/
			\begin{itemize}
				\item 
			\end{itemize}	
	
		\subsection{Dermatology Bias}
			\begin{itemize}
				\item https://ijdvl.com/biases-in-dermatology-a-primer/ 29 biases, 4 reasons to know about it, 7 mitigation methods \autocite{Chakraborty_2024} - dermatology
				
				\item A recent study reported mean top-1 and top-5 model accuracy of 44.8\% and 78.1\%, respectively, for the classification of 134 diseases (Han et al., 2019b). Most datasets are proprietary, often with minimal description, and datasets collected in dermatology clinics may be skewed toward more complex cases, to those patients with better access to care, or by the choice of camera used in one clinic versus another. Data should be collected from as many diverse sources as possible, including primary care clinics, and robust standards for external validation are needed. \autocite{Young_2020}
				\item There have been successful efforts to support reproducibility and open access. For example, the study by Han et al. (2018a) details the number and characteristics of images from each data source and makes thumbnails of the images publicly available. Additionally, several studies classifying dermoscopic images use the publicly available International Skin Imaging Collaboration archive (Gutman et al., 2016). By making datasets public, it becomes possible to examine them for bias (Bissoto et al., 2019). Alternatively, reporting a model training database’s patient demographics and disease classes would be helpful in predicting model performance on external populations. \autocite{Young_2020}
				\item Metrics of model performance. Standard metrics are needed to assess the performance of different models (Figure 1). Currently, standard performance metrics such as accuracy and area under the receiver operating characteristic and precision recall curves are routinely reported. However, for use in the clinic, studies should additionally describe how well their models deal with uncertainty by reporting (i) the Brier Score, or mean-squared calibration error (Rufibach, 2010), which measures how reliably a model can forecast its accuracy, and (ii) area under the response rate accuracy curve, which measures how capably a model can identify examples it is likely to predict falsely and thus abstain from predicting (Hendrycks et al., 2019) \autocite{Young_2020}
				\item Model interpretability. Acceptance of \gls{AI} in clinical decision-making hinges on being able to understand the decisionmaking process fundamental to its predictions. DL models are inherently difficult to interpret because they are complex, routinely containing millions of learned parameters; interpretation of DL models’ output is an active field of research (Murdoch et al., 2019). One approach for interpreting model diagnoses is contentbased image retrieval, a method for retrieving training images that are visually similar to a test image (Tschandl et al., 2019a). This method may reassure the physician if all the retrieved training images have the same diagnosis as the predicted diagnosis but is less helpful if the test image looks similar to two or more training images with conflicting diagnoses. A second approach is to highlight pixels in an image most relevant for a model’s prediction, using methods such as saliency mapping (Figure 1). However, it is often the case that highlighted pixels correspond to the entire lesion or visually distinctive features that are already obvious to clinicians without indication as to why these pixels are important to the diagnosis. A third approach is to see through the eyes of a model by plotting an activation atlas (Carter et al., 2019), which shows how subtle changes, in particular visual features, may tip the model over into choosing one diagnosis over another. These activation atlases are experimental and have yet to be applied in dermatology. Understanding a model’s predictions and how the prediction is applicable to the patient at hand is necessary to build trust. As \gls{AI} exceeds human performance in various tasks, interpreting models may help to advance scientific knowledge by understanding what the machine sees that is relevant to its predications \autocite{Young_2020}
			\end{itemize}
		\subsubsection{Demographic Bias in Dermatology}
		\paragraph{fairness melanoma detection}
		\begin{itemize}
			\item Some biases can be easily detected and countered, such as through appropriate data curation; for example, having a balanced representation across skin tones and genders in training sets. However, in other cases, biases are hidden and untraceable [9]. \autocite{Montoya_2025}
			\item whether information on demographic diversity (age, gender, race, or ethnicity of patients), clinical diversity (skin type, lesion type, anatomical location of lesion), or image characteristics (source, imaging techniques, resolution, and whether the images were real or artificially generated) \autocite{Montoya_2025}
			\item The most popular skin color scale currently being used for data annotation for image recognition techniques is the Fitzpatrick Skin Tone Scale (FST) [10]which has six skin tones. Dating from the 1970s, it originally featured just 4 light tones and was designed for detecting photo sensitivity for white skin, with two darker tones added later [11]. The Monk Skin Scale was recently developed and still needs testing, but promisingly has 10 tones, 5 light and 5 dark [12].\autocite{Montoya_2025} \todo{highlight this (FST alternatives)}
			\item Fig. 4. Comparison of skin tone scales that can be used for skin cancer detection utilizing \gls{AI}. Recreation of fitzpatrick skin type scale, monk skin tone scale, and sampling of L’Oreal color chart map for reference. \autocite{Montoya_2025} \todo{include this figure}
			\item While this systemic review provides a comprehensive review of the literature on fairness in \gls{AI} for melanoma detection, it is primarily based on existing research. To validate the proposed recommendations or frameworks, continuing work is necessary to complete empirical analysis and experiments. Additionally, the suggested adoption of new skin tone scales, while beneficial, may face practical challenges in implementation. Furthermore, while the paper strongly advocates for specific skin tone scales, it’s important to note that other methods or tools might also effectively address fairness issues in \gls{AI} for melanoma detection. Finally, while the study addresses fairness in \gls{AI}, it could benefit from further exploration of the practical implementation of these recommendations in real-world clinical settings. Potential obstacles and the feasibility of widespread adoption should be considered to ensure that the proposed solutions are not only theoretically sound but also practically viable. \autocite{Montoya_2025} \todo{also mention the limitations regarding FST alternatives}
			\item Recent research [13] adds another axis, skin hue, which is described as ranging from red to yellow. This offers a more complete representation of variations of skin color by providing a multidimensional scale [13]. \autocite{Montoya_2025}
			\item The effect of hue (blue, red, yellow, green) on skin tones adds depth to each face producing a range of undertones (cold, neutral, warm, and olive). In the realm of color theory, the concept of ‘contrast of hue’ emphasizes the distinctiveness among fundamental colors, with primary hues like yellow, red, and blue exhibiting the most pronounced differences [14]. Because skin cancer appears differently on different colored skin, it is important to acknowledge a full range of colors present in both healthy skin and suspicious lesions within datasets used to train skin cancer detection \gls{ML} tools. \autocite{Montoya_2025}
			\item These findings should correlate to \gls{AI} for melanoma detection since the contrast between skin color and skin lesions is a preliminary marker during feature extraction. Although the Fitzpatrick Skin Tone (FST) \gls{FST} measurement scale is not diverse enough and leads to biased \gls{AI} tools, it is continually used and has even been used to test a recently FDA-approved \gls{AI} device for detecting melanoma. \autocite{Montoya_2025}
			\item We advocate for the adoption of improved scales like the Monk and L’Oreal maps. Future studies should ensure equitable representation and testing across skin tones to guarantee \gls{AI}’s effectiveness for all. Please refer to Tables 2 through 7 in the discussion section for further recommendations for curating a diverse dataset, including purpose, ownership, funding, and data annotation, as well as recommendations for each stage of the data life cycle. \autocite{Montoya_2025} \todo{Link for further mitigation methods}
			\item This study found that while using skin tone instead of race for fairness evaluations in computer vision seems objective, the annotation process remains biased by human annotators. Untested scales, unclear procedures, and a lack of awareness about annotator backgrounds and social context significantly influence skin tone labeling. This study exposes how even minor design choices in the annotation process, like scale order (dark to light instead of light to dark) or image context (face or no face, skin lesion presence), can sway agreement and introduce uncertainty in skin tone assessments. ... The researchers emphasize the need for greater transparency, standardized procedures, and careful consideration of annotator biases to mitigate these challenges and ensure fairer and more robust evaluations in computer vision. \autocite{Montoya_2025} - demographic dermatology bias
		\end{itemize}
	\rawcitationend
	\end{comment}
	
	
	\chapter{Ideas and Concepts}
		\baaCriteria{Hier geht es um die Fragestellung, wie Sie die formulierten Ziele der Arbeit erreichen wollen. Sie halten z.B. erste, grobe Ideen, skizzenhafte Lösungsansätze fest. Gibt es mehrere Wege, Ansätze um dieses Ziel zu erreichen, begründen Sie hier, warum Sie einen bestimmten Weg einschlagen. Beispiel für ein Softwareprojekt: Erste Gedanken über eine grobe Systemarchitektur. Ist z.B. eine Microservice-Architektur angebracht? Welche Alternativen bestehen, wo gibt es Problempunkte? Die Umsetzung, die Beurteilung der Machbarkeit und die detaillierte Beschreibung der umgesetzten Architektur sind dann Teil der Realisierung.}
					
		This chapter outlines initial thoughts and conceptual considerations for addressing potential biases in the PASSION project. It sketches the general methodology used in this thesis.
		
		\section{Broad Methodology}
			The evaluation and mitigation of bias in the PASSION model is planned to consists of four stages:
			\begin{enumerate}
				\item \textbf{Literature Review.} A literature review will be conducted to get an overview of what biases, fairness metrics and mitigation strategies are known in medical \gls{AI}.
				
				\item \textbf{Contextualization and Scope Definition.} The findings' relevance for PASSION's \gls{teledermatology} context will be evaluated. Based on this, relevant types of bias, applicable fairness metrics and mitigation methods will be selected. Aspects not feasible to address within the scope of this thesis will documented for future work.
				
				\item \textbf{Baseline Fairness Assessment.} The current PASSION model will be evaluated using the selected fairness metrics. This will provide a baseline for comparison after mitigation methods are applied.
				
				\item \textbf{Mitigation and Evaluation.} Selected mitigation strategies will be implementend individually. Their effect on model fairness and performance will assessed relative to the baseline.
			\end{enumerate}
		
		\section{PASSION Dataset Assessment}
			In order to decide about the scope and feasibility of the findings in the literature review, the dataset must be assessed.
			The PASSION dataset was created to improve the representation of highly pigmented skin, which is underrepresented in many traditional dermatology datasets. Nevertheless, it may still lack adequate representation of specific subgroups. Such gaps in representativeness could potentially lead to biased model outputs. However, as \textcite{Mehrabi_2021} states, this is not necessarily the case. Therefore, a detailed assessed for representativeness can be postponed until the model output indeed proofs to be biased.
			
			Furthermore, the available metadata determines which biases can identified and what mitigation methods are possible. E.g., if metadata on age is missing, fairness with respect to age cannot be assessed.
			
			Therefore, the dataset will be reviewed with regards to:
			\begin{itemize}
				\item Representation of the main groups to get a first impression
				\item Representation of relevant subgroups if the model output proves to be biased
				\item Completeness of metadata relevant for fairness evaluation
				\item Presence of \glspl{proxyVar} that might complicate fairness assessments
			\end{itemize}
			
			These aspects will help determine the extent to which the dataset supports meaningful fairness analysis and subgroup-level model evaluation. It also provides guidance on how to potentially adapt the dataset in the future.
		
			
%		\begin{comment}
%			\todo{should this really be stated here or in the methodology section?}
%			First, I need to gain an overview of what biases, fairness metrics and mitigation strategies are known in general.
%			Then, I must scope the found information, to find what is relevant for PASSION and what is feasible to achieve within the time constraints of this thesis.
%			Before starting an assessment, a baseline needs to be established by computing chosen fairness metrics.
%			Afterwards, the mitigation methods can be applied, and the performance can be compared to the baseline, to find out whether the methods are indeed mitigating the biases.
%			
%			\section{some general mitigation method ideas}
%				\todo{add infos from the midterm presentation}
%			
%				\todo{write things to consider more precisely:}
%				\begin{itemize}
%					\item Divide and Conquer vs. All-In-One-Model
%					\begin{itemize}
%						 \item An algorithm per ethnicity / subgroup running at the same time
%						\item Running 1 Algorithm chosen based on Fitzpatrick skin type
%						\item Running 1 Algorithm which detects first the demographic subgroup (\gls{FST}, gender, age, …) and runs the specific subgroup algorithm afterwards
%						\item Hint Ludovic: Still not of data, maybe also others; often limited because the data is missing, you are missing data from others
%					\end{itemize}
%					\item BLIND performance vs. Including the demographic data
%					\begin{itemize}
%						\item Idea to try if the labels are not relevant for the diagnosis and should only be used for evaluating fairness purposes as some papers suggest 
%						\item Might be obsolete after demographic biases in dermatology research, since melanin response and melanoma risk is different in male and female according to research https://pmc.ncbi.nlm.nih.gov/articles/PMC4797181/
%					\end{itemize}
%					\item Hint Ludovic: Maybe Focal Loss more relevant --> emphasis on data vs. model
%				\end{itemize}
%				\begin{itemize}
%					\item Divide and Conquer vs. All-In-One-Model (either by etnicity x algorithms at a time or one which seperates the imgs first by demographic subgroup (incl. Fitzpatrick skin type))
%					\item BLIND performance vs. Including the demographic data
%				\end{itemize}
%		
%		\end{comment}	
	\chapter{Methods}\label{chap:methodology}
		\baaCriteria{Hier halten Sie fest und begründen, welches Vorgehensmodell Sie für Ihr Projekt wählen. Sie verweisen allenfalls auf die daraus entstandenen, konkreten Terminpläne mit Meilensteinen, welche z.B. unter Realisierung (Kapitel 5) oder im Anhang versorgt sind. Bei Projekten mit einer verlangten wissenschaftlichen Tiefe werden hier die geplanten Forschungsmethoden wie quantitative/qualitative Interviews, Befragungen, Beobachtungen, Feldexperiment etc. beschrieben und begründet. Warum ist in Ihrer Situation ein Interview besser als eine Umfrage? Wer soll interview werden?}
		\baaCriteria{Die gewählten Methoden sind nachvollziehbar und begründet. Eine methodische Übersicht (Methodisches BigPicture) wurde aufgezeigt und Abgrenzungen erläutert.}
		
		
		This chapter describes the methodological approach and project organization used in this thesis. It outlines the selected process model, planned research methods, and relevant conditions. The focus lies on ensuring that the chosen methods are appropriate, transparent, and justified in the context of evaluating and mitigating bias in the PASSION project.
		
		\section{Project Management}
		This chapter illustrates the used process model, how the progress and risk are managed and what technical constraints are available, to get a sense of the constraints and the general plan of this thesis. 
		
		\subsection{Process Model}
		The project follows the waterfall model. This means the work is done sequentially and each sequence is based on the one before \autocite{Petersen_2009}. This model has been chosen for the project, since it provides a solid base for the main project while keeping the project management overhead small.
		This project is separated in two phases:
		
		\textbf{Phase 1 – Literature Review and Methodology Planning.} This phase includes the literature review, the selection and justification of fairness metrics and bias mitigation techniques, and the assessment of the dataset's structure and limitations. Based on these results, a detailed plan for the second phase is developed.
		
		\textbf{Phase 2 – Execution and Evaluation.} In the second phase, the planned assessments and mitigation strategies are implemented. The PASSION model is evaluated against the selected fairness metrics, and improvements are measured and discussed.
		
		The detailed project plan is included in the appendix. \todo{add to appendix}
		
		\subsection{Progress Monitoring and Risk Management}
		To ensure project transparency and timely delivery, bi-weekly status meetings with the advisor are scheduled. Each meeting is prepared beforehand. Discussed are:
		\begin{itemize}
			\item Work completed in the last period
			\item Planned work for the next period
			\item Current project status and comparison with planned schedule
			\item Top three project risks and planned mitigation strategies
		\end{itemize}
		
		Meeting protocols, including the risk reports are included in the appendix. \todo{add to appendix}
		
		\subsection{Technical Constraints}
		Model training is performed on \gls{HSLU}'s \gls{gpuhub} infrastructure, while code development is carried out on a personal notebook. The code is written in Python and builds upon the existing PASSION project architecture.
		
		
		\section{Literature Review}
		The literature review targets known bias types, fairness metrics, and mitigation techniques in medical \gls{AI}, with special attention to \gls{teledermatology} and demographic factors. Sources include scientific publications, surveys, and technical documentation of relevant libraries. The goal is to build a conceptual and methodological foundation for subsequent analysis.
		
		To ensure the thesis follows scientific standards while still being feasible, the literature review is conducted based on the pragmatic method of \textcite{Alake_2021} as suggested by my advisor. First, the focus is on survey and taxonomy papers, which provide an overview over the existing research. Them, more detailed papers in the area of dermatology \gls{AI} is conducted to get more insight in the healthcare context. Such a 2-step approach has also been done by \textcite{Chen_2024}. In general, the papers are filtered by focusing on title, abstract and conclusion. Only relevant papers are read in full. \todo{cite protocol in appendix, week1}
		
		\section{Contextualization and Scope Definition} \label{chap:contextMethod}
		The relevance of the literature findings is evaluated in the context of the PASSION project. This includes analyze the findings from the literature review in terms of their relevance to \gls{teledermatology} and similar healthcare applications, taking into account the available metadata in the PASSION dataset. Limitations due to dataset constraints or the available time are documented for future work.
		
		The relevance will be categorized into the following groups:
		\begin{itemize}
			\item \textbf{High.} Directly applicable to PASSION, both in terms of the \gls{teledermatology} setting and available metadata; likely to provide valuable insights or improvements. 
			\item \textbf{Medium.} Generally relevant to diagnostic \gls{AI}, but requires adaptations of the PASSION metadata or project in general to be feasible.
			\item \textbf{Low.} Related to PASSION, but only limited.
			\item \textbf{Not Applicable.} Not relevant for PASSION due to fundamental differences in domain, type of data, or type of model.
		\end{itemize}
		
		\todo{must de sensitive feature table be split? or the categorization explained?}
		
		Based on this contextual analysis, the highly relevant bias types and mitigation methods are investigated further using the most relevant fairness metrics. The selection process follows domain-specific requirements identified in the literature. Such considerations guide the identification of suitable metrics, which are then justified and evaluated in detail during the execution phase.
		
		This contextual analysis is important, as the context and application of fairness metrics and as well as the effect and therefore importance of potential biases can vary by the use case of the \gls{AI} application \autocite{Mehrabi_2021,Barr_2025}.
		
		\section{PASSION Dataset Assessment}\label{chap:datasetAssessmentMethod}
		The assessment of the PASSION dataset focuses on four core areas:
		
		\begin{itemize}
			\item \textbf{Metadata Completeness.} The metadata is reviewed to verify that all relevant demographic attributes, as identified in the contextualized literature review, are included. Missing attributes limit bias detection and mitigation strategies. They should be added to enable a thorough fairness analysis and bias mitigation. Therefore, potentially missing attributes are listed and passed on to the PASSION team for inclusion in the metadata.
			
			\item \textbf{Presence of \glslink{proxyVar}{Proxy Variables}.} Available metadata attributes are assessed regarding their intended purpose and potential use as \glspl{proxyVar}. If \glspl{proxyVar} are identified, alternatives are proposed to be added to the data instead. This step is essential, as relying on \glspl{proxyVar} may introduce unintended bias into the analysis or model.
			
			\item \textbf{Representation of Main Groups.} To evaluate overall demographic distributions, the proportions of the values for each demographic attribute (age, sex, \gls{FST}) are analyzed to identify over- or underrepresented groups. This provides an initial indication of potential data skews, which then can be compared to the model's fairness assessment results. This grants first insight into whether potential unfairness stems from representation bias or other factors.
			
			\item \textbf{Representation of Relevant Subgroups.}
			If the fairness assessment of model outputs reveals unfairness on subgroup levels, the distribution of the subgroups is examined using the same method as for the main groups. As this is a more detailed analysis than the representation of main groups, it is done later in the process if biases regarding subgroups in the model indeed exist.
		\end{itemize}
		\todo{cite methods}
		
		\section{Reproduce PASSION Results}
		Before starting any evaluation on the model, the PASSION experiments must be reproduced on the \gls{gpuhub}, to ensure, that the code base and the data loading is working the same way as for the initial paper. Only then, the evaluation outcome can be used by the PASSION team.
		
				
		\section{Fairness Assessment}\label{chap:fairnessAssessment}
		The selected fairness metrics are used to assess the output of the current PASSION model, establishing a baseline for the model's fairness. This includes subgroup performance evaluation and identification of any disparities.
		
		Each time after applying a mitigation method, the fairness of the updated model is assessed using the exact same procedure. This consistent evaluation is necessary to enable the comparison between the baseline and each mitigation method.
		
		A mitigation method is considered to have potential for improving the fairness of the PASSION project of it results in a significantly higher fairness than the baseline.
		
		\todo{add more details? Or in explain the detailed approach with Equalized Odds in a seperate (sub-?)chapter? for sure, bring fairlearn into it (smt like Libraries such as \textit{Fairlearn} are used for bias detection and mitigation.)}
		
		\begin{comment}
			\begin{itemize}
				\item \textbf{Quantitative Analysis:} The fairness of the PASSION model is assessed using quantitative fairness metrics such as demographic parity and equalized odds. The use of these metrics enables objective comparison before and after the application of mitigation strategies.
				\item \textbf{Framework-Based Evaluation:} The fairness analysis and mitigation methods are implemented using existing research libraries (e.g., Fairlearn), ensuring that the approach is comparable to state-of-the-art scientific methods.
			\end{itemize}
			
			Qualitative methods (e.g., interviews or surveys) are not applicable in this context, as the project focuses on technical evaluation using quantitative model performance and fairness indicators.
		\end{comment}
		
		
		\subsection{Limitation}
			This method can only provide an initial indication of the potential effectiveness of a mitigation method. To determine whether a mitigation method is scientifically beneficial, it must be tested more systematically. Ideally, each mitigation method should be applied multiple times using different seeds for the random generators. In the best case, the baseline should also be established multiple times using the same set of seeds. This would allow for a statistical significant evaluation of the results. This approach has been used by \textcite{Valentim_2019} who used 30 different seeds for each configuration.
			
			Due to technical limitations and time constraints, this was unfortunately not feasible to do during this thesis. It is advice that the PASSION team executes the experiments further times using the provided scripts, in order to get a more established insight.
		
		\section{Mitigation Method Selection and Evaluation} \todo{consider to split chapter in mitigation method stuff and data split setup}
		The PASSION model uses a predefined train-test split. To prevent test set leakage and overfitting while applying mitigation methods, the training data is further divided into a training and a validation set. 
		
		If a mitigation method can be applied in multiple ways (e.g., with different parameters, configurations, or data splits), all these variants are evaluated using the train-validation split. The variant that performs best on the validation set is then used to evaluate the effectiveness of the mitigation method on the original test set.
		
		This approach ensures that the final test results are comparable across different methods, while keeping the selection process independent of the test data. It follows standard machine learning methodology to prevent overfitting to the test set.\todo{cite AI lectures}
		\begin{comment}
			AI lectures or textbook, e.g., Goodfellow et al., 2016
			Goodfellow, I., Bengio, Y., Courville, A. (2016). Deep Learning. MIT Press.
			Or a standard practice guideline like:
			Varma, S., Simon, R. (2006). Bias in error estimation when using cross-validation for model selection. BMC Bioinformatics, 7(1), 91.
		\end{comment}
		
		Selected bias mitigation strategies are applied to this setup individually, so that the impact on the fairness can be clearly assigned to the tested mitigation strategy. The impact is evaluated relative to the established baseline as described in \linkchap{chap:fairnessAssessment}.
		
		\todo{ensure to also compare model performance - if you did it, also mention here!}
		
		\todo{explain specific mitigation method used}
		
	\chapter{Execution}
		\baaCriteria{Dies ist das Hauptkapitel Ihrer Arbeit! Hier wird die Umsetzung der eigenen Ideen und Konzepte (Kapitel 3) anhand der gewählten Methoden (Kapitel 4) beschrieben, inkl. der dabei aufgetretenen Schwierigkeiten und Einschränkungen.}
		\baaCriteria{Die gewählten Methoden werden systematisch, konsistent und korrekt auf den Kontext der Arbeit angewendet. Die Bearbeitungs- bzw. Forschungsobjekte sind einheitlich benannt, im Kontext dargestellt und sinnvoll in die Arbeit integriert. Praxis- und Erfahrungswissen (z.B. aus Interviews) wird zur Validierung und Ergänzung der erarbeiteten Ergebnisse herangezogen. }
		
		\section{Contextualization and Scope Definition}
		This section applies the information found during the literature review to the PASSION project using the method described in \autoref{chap:contextMethod}. It also scopes what information can be assessed during this thesis and what should be passed on to the PASSION team.
		
		\todo{@Proofreaders, würdet ihr dieses Kapitel hier lassen oder in die Evaluation schieben, basierend auf den Kriterien in blau? Und ist es sinnvoll, wie ich es fürs PASSION dataset assessment gemacht habe?
		Habe das aus dem Research teil herausgelöst weil der Betreuer dazu meinte: I would keep the evaluation for PASSION separate from the state of research and place it in its own subsection in the execution/evaluation.
		In the Methods, you also describe how you perform this evaluation.}
		
		\subsection{Sensitive Features}
			Some of the listed features in \autoref{tab:biases_features} were also mentioned in the dermatology context and/or are included as metadata in the PASSION dataset. Therefore, potential biases associated with them should be evaluated in the PASSION model.
			
			Since PASSION aims to improve classification of skin diseases based solely on image data without any metadata, it does not use these factors as features for training, except for characteristics that are implicitly visible in the images. This is primarily the \textit{skin type} (including the undertone). More broadly defined, the \textit{socioeconomic status} and \textit{geographic location} can also be leaked to the model through the images, due to their impact on disease presentation and progression. Since the model can access these characteristics during training, they can introduce bias and should therefore be closely examined.
			
			\textit{Age} and \textit{sex} are generally not visible in the images. Also, \textit{socioeconomic status} and \textit{geographic location} do not necessarily need to lead to visual effects. However, since they can influence disease prevalence and are prone to bias, the PASSION model should be evaluated for potential bias regarding these characteristics.
			
			The potential impact of \textit{ethnicity} and \textit{disabilities} on visual presentation or prevalence of dermatological conditions has not been assessed in this thesis, due to time constraints. It is recommended that the PASSION team investigates these aspects further.
			
			The other sensitive feature seem not to be further relevant for PASSION.
			
		\section{PASSION Dataset Assessment} \label{chap:datasetAssessmentExecution}
		The practical analysis is conducted according to the methods outlined in \autoref{chap:datasetAssessmentMethod}:
		
		\begin{itemize}
			\item \textbf{Metadata Completeness.}
			The available PASSION metadata listed in \autoref{tab:PASSION_metadata} is compared to the demographic factors which are relevant for bias detection. Missing attributes are listed in \autoref{chap:datasetAssessmentMetadataEvaluation}.
			
			For certain attributes, the impact on dermatology specific use case is not entirely clear based on the literature review. For the attributes sex and age which are used in the PASSION dataset, the author of PASSION was contacted to provide more insight about their impact. This information was incorporated in the literature review.
			
			In order to provide the most complete view possible, all attributes which might have an impact are listed for the PASSION team to double-check with a dermatologist.
			
			\item \textbf{Presence of \glslink{proxyVar}{Proxy Variables}.}
			Since the intended purpose of the variables are not mentioned in the paper, the analysis for \glspl{proxyVar} was more difficult then expected. The result is based on the sensitive features and biases mentioned in the literature.
			
			Also, what the country variable represents in PASSION is not entirely clear based on the documentation. To clarify its meaning, the main author of PASSION was contacted.
			For all variables which appear to potentially be used as a \gls{proxyVar}, recommendations are provided for more precise alternatives for the PASSION team to check.
			
			\item \textbf{Representation of Main Groups.}
			Since there is no \gls{JupyterNotebook} script provided by PASSION to gather the proportions in depth, a python script is created to gather this data, what increased the time effort for the detailed analysis. The script is part of the newly created \texttt{evaluator} class and is meant to be executed standalone.
			It prints the distribution as absolute support and percentage for all values of the attributes country, sex, fitzpatrick, impetig, conditions\_PASSION, and ageGroup. The age group contains the ages binned into 5 year intervals, like it has been done by \textcite{Gottfrois2024} in their distribution analysis.
			Also, it saves the distribution in a csv and prints a plot per attribute. The comparison between the values is done manually for now, since there are not too many values.
			
			\todo{ensure to discuss the evaluator class beforehand somewhere and add command to command in readme(evaluator.run\_split\_distribution\_evaluation)} \todo{consider to mention the age stuff once before hand in a constraints chapter or so}
			
			\item \textbf{Representation of Relevant Subgroups.} The demographic distribution figures of PASSION are briefly analyzed for an initial indication of the representation of age and sex.
		\end{itemize}
		
		
	\chapter{Evaluation and Validation}
		\baaCriteria{Auswertung und Interpretation der Ergebnisse. Nachweis, dass die Ziele erreicht wurden, oder warum	welche nicht erreicht wurden.}
		\baaCriteria{Die Ziele / Forschungsfragen sind dem Umfang der Arbeit entsprechend sehr klar abgegrenzt; sie sind präzise, überprüfbar und nach den Standards der Zielformulierung definiert. Die Zielerreichung wurde systematisch und korrekt validiert.}
		\baaCriteria{Die Herleitung und Bedeutung der Ergebnisse, mögliche Varianten, Gütekriterien und eine Validierung allgemein werden nachvollziehbar diskutiert}
		
		
			\section{PASSION Dataset Assessment}
		The PASSION dataset assessment results are described in this section.
		Overall, the dataset enables a foundational fairness analysis but does not support in-depth bias evaluation without augmentation or careful interpretation.
		
		\subsection{Metadata Completeness and \glslink{proxyVar}{Proxy Variables}} \label{chap:datasetAssessmentMetadataEvaluation}
		Based on the literature regarding sensitive features and potential biases, important metadata is available in the dataset, namingly age, sex and \gls{FST}.
		However, relevant metadata for a thorough fairness assessment and bias mitigation is missing. This limits what biases can be detected.
		
		The missing metadata attributes are:
		\begin{itemize}
			\item socioeconomic status
			\item geographic location / residence of the patient
			\item (type of) the clinic and their medical focus
			\item image quality or other image related information such as the phone used, whether the image contains hair, and so on
			\item ethnicity (if it proves to have an impact on dermatology conditions)
			\item disabilities (if it proves to have an impact on dermatology conditions)
		\end{itemize}
		
		The variable \textit{country} currently could theoretically serve as \gls{proxyVar} for \textit{geographic location}, which clinic the data is from and more broadly even for the \textit{image quality}. It is not clear if those usages are intended. According to the literature review, this should be prevented. \todo{ensure that this is indeed written somewhere in the literature section}
		Since the country only reflects the location of diagnosis, it is insufficient to determine the \textit{geographic location} or residence of the patient. More precise data would be preferable for robust bias analysis.
		Since the data is gathered only from one clinic per country, this \gls{proxyVar} usage is feasible for now. However, more clinics should be included into the data collection process to mitigate medical biases and ascertainment bias. Then, the clinic and some more data about it should be added to the dataset.
		The clinic again might be a \gls{proxyVar} for the picture quality. If this information can be quantified in another way, e.g., the used phone and camera settings, that would further improve the dataset by tackling image biases.
		The country information can still be used in the fairness assessment to see if there are fairness differences in those populations. However, in order to clearly identify related biases, the suggested changes to the metadata would need to be introduced.
		
		It is suggested to add the missing metadata attributes to the dataset. Given the sensitivity of those attributes, ethical considerations must be addressed before extending the dataset.
		
		\subsection{Demographic Representation}		
		The demographic distribution in the PASSION dataset shows clear imbalances across several attributes. The data is available in \linkapp{app:PASSIONdataDistributionAnalysis}.
		
		To summarize:
		
		\begin{itemize}
			\item \textbf{Country.} The dataset is heavily skewed towards samples from Madagascar (59.59\%), while Tanzania is significantly underrepresented (1.39\%). This imbalance may introduce geographic or clinic-specific biases.
			
			\item \textbf{Sex.} Male patients are overrepresented (58.2\%) compared to female patients (41.8\%). No data is available for individuals of other sexes.
			
			This thesis did not explore whether other biological sex differences or gender-affirming hormone therapies have any impact on dermatological conditions, since the main focus for PASSION is on inclusion regarding skin type. However, for a complete fairness evaluation, this factors should be explored in the future.
			
			\item \textbf{\gls{FST}.} The types III to VI are represented, with the distribution ranging from 21.42\% (type III) to 29.4\% (type IV). No data is available for type II and only one sample for type I.  Given PASSION's focus on highly pigmented skin, this distribution is somewhat justified. However, it limits applicability to lighter skin tones and could impair model generalizability. 
			
			An interesting future direction would be to combine PASSION with other dermatology datasets to evaluate fairness and performance across the full spectrum of \gls{FST}. Moreover, due to historical underrepresentation of highly-pigmented skin in dermatology datasets, the performance on types V (25.89\%) and VI (23.23\%) should be examined in more detail, to see if their representation in the dataset must be addressed further.
			\todo{cite https://academic.oup.com/bjd/article-abstract/185/1/198/6600283?redirectedFrom=fulltext, already mention this in dermatology bias section}
			
			
			\item \textbf{Age Groups.} Children aged 0–9 account for over 40\% of the dataset, whereas elderly patients (65+) are nearly absent. Although this skew reflects PASSION’s focus on \gls{pediatric} conditions, the lack of data for seniors may reduce fairness for those age groups.
			
			Nevertheless, PASSION's age-generalization experiments suggest that a model trained on primarily \gls{pediatric} images might generalize reasonably well \autocite{Gottfrois2024}.
			
			\item \textbf{Conditions.} The dataset is dominated by fungal infections (35.02\%), followed by scabies (28.49\%), and eczema (25.05\%). Other conditions account only for 11.43\%. Mo data is available for healthy skin.
			
			The ambiguous "other" category complicates fairness evaluations for specific conditions. Disaggregating this group into defined labels would improve clarity. Additionally, including healthy skin samples could reduce potential bias and enable better calibration of diagnostic models. \todo{find and add healthy-vs-disease bias here}
			
			\item \textbf{Impetigo Indicator.} The impetigo label is present in only 11.6\% of the cases, indicating class imbalance that may affect prediction reliability for this condition.
		\end{itemize}
		
		The \autoref{fig:PASSIONDistrImbalances} illustrates the overrepresentation of male children, based on the figures presented by \textcite{Gottfrois2024}. There are also condition-specific differences in \gls{FST} distribution. If this imbalances significantly affect model fairness, the dataset composition may need to be revised.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.9\textwidth]{figures/PASSIONDatasetDistributionPotentialImbalances.png}
			\caption{PASSION dataset distributions by \textcite{Gottfrois2024} - highlighting potential imbalances}
			\label{fig:PASSIONDistrImbalances}
		\end{figure}
		
		These findings highlight representation disparities across several demographic and clinical factors. Such disparities should be accounted for during training and fairness evaluation, especially when assessing subgroup-specific performance.
		
		It is important to note that the provided analysis only is a high-level overview at the group level. Detailed subgroup representation has not yet been assessed in details. Due to the time limits of this thesis, this was deferred in favor of executing the stratified split experiment.
		
		To enable subgroup-level representation analysis, group-level dataset representation script should be extended accordingly. As the script output will increase substantially, manual comparison may become impractical. Therefore, automating the comparison and generating summaries of the largest disparities is recommended.
		
		
		
	\chapter{Outlook}
		\baaCriteria{Reflexion der eigenen Arbeit, ungelöste Probleme, weitere Ideen.}
		\baaCriteria{Die Ergebnisse und Empfehlungen schaffen einen konkreten Mehrwert für die Auftraggebenden. Einschränkungen und Grenzen werden kritisch diskutiert und die nächsten Schritte im Ausblick festgehalten, so dass die Ergebnisse direkt in der Praxis weiterverwendet und/oder angewendet werden können.}
		
		This chapter summarizes the concrete recommendations to overcome the limitations of the current work. This includes e.g., revising the metadata used in PASSION, and extending the analytical tools used.
		
		It also provides ideas, such as adding more diverse data and combining PASSION with other dermatology datasets to improve bias detection and aim for a more complete dataset.
		
		These measures aim to enhance the practical applicability of the results and support the development of fair, generalizable \gls{ML} models in dermatology.
	
		
		\section{PASSION Dataset Improvements}
		To improve the fairness assessment capabilities of the PASSION dataset, the following dataset improvements are proposed:
		\begin{itemize}
			\item Include the missing metadata attributes identified in \autoref{chap:datasetAssessmentMetadataEvaluation} (e.g., socioeconomic status, clinic type, image quality) to enable a more comprehensive fairness evaluation. Ensure to assess the ethical implications before collecting such data.
			\begin{itemize}
				\item Investigate whether \textit{ethnicity} and \textit{disabilities} influence the presentation or prevalence of dermatological conditions before adding them to the dataset.
			\end{itemize}
						
			\item Clarify the intended purpose of the \textit{country} variable, and replace or supplement it with more precise alternatives, as discussed in \autoref{chap:datasetAssessmentMetadataEvaluation}.
			
			\item Refine the "other" condition category by breaking it down into more specific labels to improve diagnostic granularity and fairness assessment per condition.
			
			\item Incorporate healthy skin samples into the dataset to allow for a more balanced classification task and to mitigate potential bias.
			
			\item Explore whether combining PASSION with other dermatology datasets enhances generalization across the full \gls{FST} range. \todo{add point to consider to use other categorizations}
		\end{itemize}
		
		
		\section{Fairness Assessment Process Improvements}
		The measures to improve the fairness assessment process further are:
		\begin{itemize}
			\item Extend the existing dataset representation script, as described in \autoref{chap:datasetAssessmentExecution}, to support subgroup-level analysis and automated comparison.
		\end{itemize}
		
		
		\section{Fairness Assessment Results Extension}
		\todo{@Proofreaders: habt ihr einen besseren Namen für dieses Kapitel? Es geht mir darum, dass weitere Analysen / Fairness assessments gemacht werden sollten}
		The existing fairness assessment results can be extended with those actions:
		\begin{itemize}
			\item Perform the representation analysis of relevant subgroups, as described in \autoref{chap:datasetAssessmentMethod} using the extended script, to determine whether observed unfairness stems from distribution imbalances at subgroup level.	
			
			\item Evaluate model performance across \gls{FST} types V and VI more closely and take measures if bias exist. \todo{check if this will still be needed}
		\end{itemize}
		
		
		Implementing these measures will enhance the dataset’s ability to support fair, robust, and generalizable \gls{ML} models in dermatology.
	
	
	\chapter{currently working on}
	
	\todo{put this somewhere in the outlook}
	Checkout this paper which suggest further methods and a flowchart to select the right fairness metric \textcite{Barr_2025}
	
	\chapter{writing ongoing TODO REMOVE THIS CHAPTER}\todo{remove this chapter}
	
		\begin{itemize}
			\item \textbf{Tooling Support.}
			While the provided scripts enable basic demographic visualization, they lack functionality for intersectional subgroup analysis or fairness metric evaluation (e.g., \textit{equalized odds}).
		\end{itemize}
	
		\subsection{Bias Evaluation in PASSION}
		\todo{fix this writing}
		\paragraph{Methodology}
		In order to evaluate which biases are there in PASSION dataset, I need to
		\begin{itemize}
			\item reproduce the results of the paper using the PASSION evaluation project with the PASSION data to see a) verify the paper results, b) check what analysis data is available for the evaluation (the paper provides probably a summary), c) check what code can be reused and what needs to be adapted
			\item evaluate what data is missing to do evaluate the fairness and biases
			\item adapt code so that the relevant data is generated to be able to compute the relevant information
			\item GPUHub from HSLU is used, \todo{maybe add some machine information}
		\end{itemize}
		
		
		\rawcitationusedstart
		https://www.mdpi.com/2227-7102/14/2/136
		Equalized Odds (EO)	 𝑃(𝑌̂ =1|𝑌=𝑦,𝑆=1)−𝑃(𝑌̂ =1|𝑌=𝑦,𝑆=0),∀𝑦∈{0,1}
		We extend the fairness metrics, as described in Table 2, for non-binary sensitive attributes by considering a one-versus-rest approach for unfairness calculation. More specifically, to calculate the unfairness gaps, we consider each subgroup as 𝑆=1
		and compare it against the rest 𝑆=0
		(i.e., all other subgroups), one at a time. We mainly focus on racial disparities; however, our proposed approach for auditing fairness and investigating the imputation impact can be extended to use other sensitive attributes. For example, the decision-maker can use “gender” as a sensitive attribute.
		
		
		
		Missing data: Ignoring missing data is not an effective approach to handling missing values, and more importantly, it can result in predictive disparity for minority groups. While many education-related studies have addressed the challenges of missing data, as discussed, little is known about the impact of different imputation techniques on fairness outcomes in the final model. This project aims to address this gap by considering the three aforementioned imputation strategies.
		
		https://proceedings.mlr.press/v108/awasthi20a.html
		BiasY =y(Yb) =Pr h		Yb = 1	Y = y, A = 0i −Pr h	Yb = 1 		Y = y, A = 1i				.
		\rawcitationusedend
		
				
		
		
		\paragraph{Execution}
			In order to run the reproduction of the PASSION results on GPUHub, some minor changes in the loading process for the metadata had to be done. The required changes will be contributed to the code base to make the reproduction easier for others. \todo{do that then ;)}
			All 4 experiments where ran to get a broad overview of the results. The results of the 4 experiments contained the same scores as the ones mentioned in the paper. The scores are given separately for each demographic groups, either based on skin type or gender. There is no data for subgroups available. \todo{add results from age and center experiments}
			For the conditions classification, the provided scores per demographic group in the paper are the average scores over all classes. The variance/deviation of the averages in the scores based on the condition varies between the groups. E.g., the F1 score for \gls{FST} VI is 0.71$\pm$0.11 (total support 87) while for \gls{FST} it is 0.73$\pm$0.04 (total support 254). 
			The detailed information can be found in the attached logs
			% \todo{add logs from the file C:\\Users\\nadja\\OneDrive\\HSLU\_Nadja\\BAA\\baa\_on\_git\\results\\reproducing\_PASSION\_results}.
			
			The available data allows to compute the equalized odds fairness. \todo{describe which fairness methods to use why}
			In order to be able compute the subgroup fairness, the test results need to be split further into the subgroups. \todo{check exactly how to calculate subgroup fairness and whether there is already an algorithm for it, same for equalized odds}
			
					
			To reproduce all 4 experiments, the training ran took roughly \todo{add: it started on 24.4. roughly at 7 o clock, took until ca. 16.4., 13:00, the two other experiments started on 26.4. 17:30} hours since there where no model checkpoints available. This runtime is not feasible for each new training using a mitigation method, especially because the GPUHub seems to cancel the script after roughly 2 days, which means that each experiment needs to be started independently.
			Since this runtime it is not feasible to use for every mitigation run, the following adaptations where made \todo{add improvements described in protokol week 9}
			
			
			checkpoint handling was a bit broken
			runs x and x1 showed, that my code was reproducible while theirs was wrong
			--> running on passion model checkpoint without loading the rest of the infos in the checkpoint
			----> confirmed, that my calculations are reproducible while theirs are not; see commented out file names for comparison
			% experiment_standard_split_conditions_passion__bias_test_BIG_MODEL_reloaded_from_checkpoint.circ vs 		
			% experiment_standard_split_conditions_passion__bias_test_BIG_MODEL.circ
			also tested on another checkpoint, same behaviour, not cached results though
			
			
			no implementation of subgroup fairness found --> calc eq odds for subgroups
			
			
			The results in the PASSION results where reproducable only on the whole model level, but not for the groups. There was no subgroup analysis availbale.
			After adapting the data linkage in the PASSION evaluation code, the results where available when in the PASSION evaluation code. 
			
			
			The smaller model performance was used for the baseline. The Fairness results differ a bit from the bigger model, but it can still serve as a baseline.
			\todo{add somewhere that only the conditions classifier is used in this thesis, the impetig classifier should also be checked. (code is generalized, but must be tested)}
		\paragraph{Evaluation and Validation}
			A first analysis shows issues regarding xy in the big model and y in the small model.
			
			There are some differences in the metrics per class based on the model size. To investigate each class individually would require more effort which should be done later. The provided scripts can be used to generate the required data. \todo{add specific info in the attachement}
			Overall, the balanced accuracy for the small model = 0.69, big model = 0.7
			
							small	big
			Macro F1-Score: 0.69	0.71
			Precision: 		0.68	0.71
			Sensitivity:	0.69	0.71
			
			FST - 5 privileged in both
			6 underprivileged in both
			4,3 priviledged in big, avg or slightly underpriviledged in small
			
			Sex - no bias in small model, big model biased towards women; TPR ↑ (0.73), FPR ~ (0.09), m TPR ↓ (0.69), FPR ~ (0.10)
			
			Age - only slight differences between the models
			0-14; 25-29 priviledged
			20-24; 30-69 underpriviledged
			70+ not represented in test data	
			
			FP and sex - big model: FST 3 only men avg, f priviledged; FST 6 - men and women underprivileged
			small model: 3m priviledged, 3f underprivileged, 4 and 6 m underprivileged, f privileged
			
			
			
			here only the big model analyzed
			FP and age - more or less the same as age alone indipendent of skin type, besides skin type 6 which more often drags them down
			35-54 is more often well of with Skin type 3-5 though
			
			
			Age and Gender - f00-34 privileged! others avg / slightly higher FPR (small model privileged f00-34, m00-15;m25-29, m35-39)
			m 15-59 clearly underprivileged (same in small model)
			
			
			FP, age, gender, often only low support found, which puts more record in the unclear section.  
			
			
			country: Guniea much better on small model
			bacc 0.64, macro prec 0.63, sensivity: 0.65, F1 0.64
			big:
			bacc 0.58, macro prec 0.56, sensivity: 0.58, F1 0.56
						
			madagascar, bigger slightly better
			malawi bigger better
			tanzania exactly the same
			
			Tanzania in both majorly underprivileged
			Malawi majorly privileged
			
			FP and country: small model: Madagascar underprivileged in 4,6; 5 overprivileged;  
			Tanzania, only 4 and 5 FSTs, but both perform badly
			Malawi performs better as there are no records darker skin types
			Guinea and Skin Type 6 perform better than others
			
			big model: Madagascar FST 6 performs badly as well as tanzania; Guinea 6 is a bit worse than in the small model, but still quite good
						
			
			country and age: malawi is always privileged, tanzania under privileged
			madagascar and guinea more or less show the same results as gender alone
			
			
			compare from === Grouping: fitzpatrick, sex, country ===
			
			
			for certain subgroups, also malawi was underperforming.
			
			--> conclusion: sex, skin type and country hold the most clear biases. Since the aim of the dataset is to reduce bias on darker FSTs, especially the skin type issues are crucial - but they could also be linked to the country.
			Limitations: not all subgroup could be investigated in depht, especially the cross categories regarding age.
			
			
			% C:\Users\nadja\OneDrive\HSLU_Nadja\BAA\baa_on_git\results\reproducing_PASSION_results\small_model
			% C:\Users\nadja\OneDrive\HSLU_Nadja\BAA\baa_on_git\results\reproducing_PASSION_results\big_model
			
			
			
			
			
	\subsection{Stratified Split}
		\paragraph{Methodology}
		
		\rawcitationstart
		Furthermore, another version of each dataset was created with all features using a one-hot (or 1-of-K) encoding scheme [5] after being discretised, meaning that they are represented by binary dummy variables. We refer to this as the one-hot encoded version of a dataset.
		\autocite{Valentim_2019}
		The age attribute was discretised into two bins defined by a value greater than or equal to 25, a threshold that was set based on the findings reported by [26]. \autocite{Valentim_2019}
		
		Model Assessment
		We performed five-fold cross-validation with the help of the methods provided by Scikit-learn [8]. In addition to the standard version of cross-validation (normal-cv), the experiments were repeated with stratification (stratified-cv) so as to maintain the class distributions of the original data. Furthermore, each configuration was run with 30 different seeds for the random generators.
		
		We selected fairness metrics which can be applied to the datasets and to the predictions made by the models, so as to be able to compare the unfairness in the predictions to that originally found in the training data. The selected set of metrics includes statistical parity difference (CVS), disparate impact (DI), and the normalised prejudice index (NPI).
		
		The F1-score is more suitable when dealing with imbalanced datasets. However, we also include accuracy in our analysis to facilitate the comparison with previous work. \autocite{Valentim_2019}
		
		Fairness Comparison Between Data and Predictions
		Besides performing our analysis based on the fairness metrics mentioned in III-E, we computed the ratio between the CVS in the predictions and the CVS found in the data subset used to train the models (CVS Ratio), as well as a similar ratio regarding the NPI (NPI Ratio). These ratios give an indication of whether the unfairness in the training data was increased or reduced under each configuration. A value of 1 indicates that the unfairness in the predictions is the same as in the training data, an absolute value greater than 1 means that the unfairness in the predictions is greater, and an absolute value lower than 1 means that the model makes fairer predictions than the procedure which produced the true labels of the training data. A DI Ratio was not computed since it would be difficult to interpret the results. \autocite{Valentim}
		Comparison
		-->  How to compute the ratios between models:
		Let’s say you have two or more models: Model A, Model B, ..., Model N.
		
		And you compute the fairness metric (e.g., CVS) on each model’s predictions over the same dataset.
		
		Then, define a relative fairness ratio as follows:
		
		Model-vs-Baseline Fairness Ratio
		Choose one model as a baseline (e.g., the best, worst, or most standard one), and compare each other model to it:
		CSV / NPI = fairness metrics
		CVS RatioModel𝑋 = CVS (Model X) / CVS(Baseline Model)
​		NPI RatioModel𝑋 = NPI (Model 𝑋) / NPI(Baseline Model)
		
		Interpretation:
		Ratio = 1: Model X has the same fairness as the baseline.
		Ratio > 1: Model X is less fair (greater disparity or bias) than the baseline.
		Ratio < 1: Model X is fairer than the baseline.
		
		This approach is fully compatible with comparisons across models, even if you don’t consider the training data's fairness.
		
		\rawcitationend
		
		
		
		\paragraph{Execution}
		1. evaluating predifined dataset split regarding stratification
		
		
			
		since country, fitzpatrick and sex has been identified to have biased outcomes, they have to be checked most thoroughly 
		
		2. 
		
		\paragraph{Evaluation and Validation}
		1. from the data analysis, \todo{add to appendix} the data is pretty well distributed for most attributes. %C:\Users\nadja\OneDrive\HSLU_Nadja\BAA\baa_on_git\results\reproducing_PASSION_results\analyzing_dataset_split	
		Basically equal: Country, conditions\_PASSION
		Almost equal: most ageGroup and impedig are 
		Bigger differences: fitzpatrick, sex, some ageGroups
		sex: contradictory to the bias towards female, the dataset is skewed to male, with even more males which was trained on. Therefore, this data skew might not be the source of the bias in the model.. 
		
		train: f: 539 (40.74\%) m: 784 (59.26\%)
		test: f: 152 (46.06\%) m: 178 (53.94\%)
		overall: f: 691 (41.8\%) m: 962 (58.2\%)
		
		
		FST types 4 and 5 are overrepresented in training, which could cause the biases on the big model.
		
		
		What I should do:
		1 random split, 1 stratified on condition and country (what probably was the split from passion), 1 stratified with FST condition country, 1 stratified with FST condition country sex 
		
		
		1. change dataset, so that those splits are reflected in validation data
		2. train models with no folds on the train and validation (simply use other split file in the begining)
		3. while training is running investigate how fold splits the data
		
		
		
	
		for the country, the distribution is already almost the same for all splits, with Tanzania clearly underrepresented. On this variable, stratified splitting should be kept for sure. More data should be captured from Tanzania - it could be differences with the data quality which lead to different results.
		
			
			

		% Lists and References
		\newpage




	\todo{probably remove this}
	%\chapter{\glossaryname \todo{probably remove this}}
		%\printglossary[title={}]
	%\todo{Add to ToC of content somehow and fix chapter numbers}
	%\listoffigures
	%\listoftables
	\todo{Add List of Formulas if necessary}
	\todo{add \gls{AI} declarations somewhere}
	
	
	\chapter{\bibname}
		\printbibliography[heading=none]
		
		%\bibliographystyle{ieeetr}
		%\footnotesize\bibliography{references}
		
		
		%----------------------------------------------------------------------------------------
		%	APPENDIX
		%----------------------------------------------------------------------------------------
		\newpage
		\appendix
		\begin{appendices}
			\baaCriteria{Projektspezifisch können weitere Dokumentationsteile angefügt werden wie: Aufgabenstellung, Projektmanagement-Plan/Bericht, Testplan/Testbericht, Bedienungsanleitungen, Details zu Umfragen, detaillierte Anforderungslisten, Referenzen auf projektspezifische Daten in externen Entwicklungs- und Datenverwaltungstools etc.}
			
			\todo{fix appendices chapters, wtf}
			\todo{also move the bibliography potentially after the appendices, idk}
			%\renewcommand{\thechapter}{Appendix \Alph{chapter}}
		
	
			\chapter{PASSION Data Analysis Scripts}\label{app:PASSIONdataAnalysisScripts}
			The PASSION team provides a \gls{JupyterNotebook} with code examples and analysis scripts. They are listed in \autoref{tab:PASSION_scripts} together with their relevance to this thesis. The most relevant scripts are those related to demographic distributions of the chosen attributes, since they help identifying potential data imbalances. Scripts that lay the foundation for further analysis are somewhat relevant, while all other scripts are irrelevant for this thesis.
			
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\hsize=.25\hsize\raggedright}X>{\hsize=.41\hsize}X>{\hsize=.34\hsize}X}
						\toprule
						\textbf{Script Title}       & \textbf{Description} & \textbf{Relevance - Reasoning}       \\ \midrule
						Distribution of \glspl{FST} &
						Counts and visualizes the skin type distribution  &
						\textbf{High} - Insight into demographic distributions \\
						\hline
						Regrouping Malawi and Tanzania to EAS &
						Data aggregation due to dataset size and geographical proximity &
						\textbf{Medium} - Might impact interpretation of the results of the following scripts \\
						\hline
						Linking CSV Data with Image Files & 
						Mapping between data records and images. &
						\textbf{Medium} - Basis for other analyses \\
						\hline
						Extracting and Comparing Subject IDs &
						Dataset verification regarding completeness &
						\textbf{Low} - No insight in regards of demographic distribution \\
						\hline
						Conditions by Country &
						Correlation between clinical conditions and country &
						\textbf{Low} - The attribute \textit{country} is out of scope of this thesis \\
						\hline
						Body Localizations by Conditions &
						Correlation between the condition and primarily affected body parts &
						\textbf{Low} - No insight in regards of demographic distribution \\
						\hline
						Impetigo Cases &
						Total count of impetigo cases and proportion to all cases &
						\textbf{Low} - No insight in regards of demographic distribution\tnote{*} \\
						\bottomrule
					\end{tabularx}
					\begin{tablenotes}
						\footnotesize
						\item[*] Research is divided on which demographic factors influence the prevalence of impetigo \autocites{Romani_2017}{Aleid_2024}.
					\end{tablenotes}
				\end{threeparttable}
				
				\caption{PASSION dataset - existing analysis scripts \autocite{Gottfrois2024}}
				\label{tab:PASSION_scripts}
			\end{table}
			
			\input{appendix_bias_list.tex}
			
			
			\chapter{Fairness Metrics}\label{app:fairnessMetrics}
			 According to \textcite{Mehrabi_2021}, fairness can be achieved on a group level, subgroup level or even for an individual. Group fairness is about treating different groups as equal. Individual fairness tries to achieve similar predictions for similar individuals. Subgroup fairness tries to incorporate the best properties of the other two levels to improve the outcome in larger collections of subgroups \autocite{Mehrabi_2021}. 
			 
			 \autoref{tab:fairness_definitions_appendix} shows the list of fairness definitions, structured in those categories.
			\begin{table}[H]
				\centering
				\begin{threeparttable}
					\begin{tabularx}{\textwidth}{>{\tblWidthDescription}X|>{\tblWidthContext}X|>{\tblWidthContext}X}
						\toprule
						\textbf{Fairness Definitions} & \multicolumn{2}{c}{\textbf{Mentioned in Context of}} \\
						& \textbf{\gls{ML}} & \textbf{Dermatology} \\
						%	\midrule
						\multicolumn{3}{l}{\textbf{Group Fairness}} \\ 
						Conditional Statistical Parity    & X &   \\
						Demographic/Statistical Parity  & X & \\
						Equal Opportunity& X &   \\
						Treatment Equality & X &   \\
						Test Fairness         & X &   \\
						Equalized Odds     & X &   \\
						%	\midrule
						\multicolumn{3}{l}{\textbf{Subgroup Fairness}} \\ 
						Subgroup Fairness    & X &   \\
						%\midrule
						\multicolumn{3}{l}{\textbf{Individual Fairness}} \\ 
						Counterfactual Fairness     & X &   \\
						Fairness Through Awareness     & X &   \\
						Fairness Through Unawareness        & X &   \\
						%\midrule
						\multicolumn{3}{l}{\textbf{Not Categorized}} \\ 
						Fairness in Relational Domains& X &   \\
						\bottomrule
					\end{tabularx}
				\end{threeparttable}
				\caption{Fairness definitions based on \textcite{Mehrabi_2021}}
				\label{tab:fairness_definitions_appendix}
			\end{table}
			
			The specific fairness definitions can be found in \textcite{Mehrabi_2021}. In general, they try to get similar probability outcomes for 'unprotected' or 'protected' groups. This list summarizes how they work:
			\begin{itemize}
				\item \textit{Demographic/Statistical Parity} and \textit{Conditional Statistical Parity}: The parity checks that the likelihood of a positive outcome is the same for both protected groups \autocite{M48_Dwork_2012,Mehrabi_2021}. The conditional version adds legitimate factors before calculating the statistical parity \autocite{M41_Corbett-Davies_2017}.
				
				\item \textit{Equalized Odds}, \textit{Test Fairness}, and \textit{Equal Opportunity}: In all these methods, protected and unprotected groups should have equal rates of positive outcomes when belonging to the positive class. These methods essentially compare the groups' \glspl{TPR}. \textit{Equalized Odds} is a more restrictive since it also checks for similar false positive rates \autocite{M149_Verma_2018,Mehrabi_2021}.
				
				\item \textit{Treatment Equality}: It compares the false negative and false positive rates \autocite{M151_Wang_2014}
				
				\item \textit{Counterfactual Fairness}: This approach is different from the others as it is testing the same individual in both different demographic groups with the intention that the outcome is the same \autocite{M87_Kusner_2017,Mehrabi_2021}. It differs from the first group of fairness metrics since it does not compare the likelihoods of the outcomes for any person in a group, but checks how the exact same individual would be treated if it was in the other group.
				
				\item \textit{Fairness Through Awareness}: This method compares similar individuals based on similarity metrics to get a similar outcome \autocite{M48_Dwork_2012,Mehrabi_2021}
				
				\item \textit{Fairness Through Unawareness}: This measure is ensuring that protected attributes are not explicitly used in decision-making \autocite{M61_Grgic-Hlaca_2016, M87_Kusner_2017}.
				
				\item \textit{Fairness in Relational Domains}: This notion also takes into consideration relational structures between individuals \autocite{M50_Farnadi_2018}.
			\end{itemize}
			
			\chapter{PASSION Dataset Distribution Analysis}\label{app:PASSIONdataDistributionAnalysis}

			The data in \autoref{tbl:PASSIONDatasetDistrAnalysis} shows the distribution of the values of the individual metadata attributes in the PASSION dataset. The data has been generated with a python script \todo{add/refer to python script}. In \autoref{fig:PASSIONDatasetDistrAnalysis}, the data is visualized.
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.9\textwidth]{figures/PASSION_split_all_distributions.png}
				\caption{PASSION dataset distribution analysis on group level}
				\label{fig:PASSIONDatasetDistrAnalysis}
			\end{figure}
			
			\begin{table}[H]
				\centering
				{
					\catcode`\_=12
					\csvautobooktabular{csvs/distribution_PASSION_split.csv}
					\catcode`\_=8
				}
				\caption{Distribution of metadata attributes in the PASSION dataset}
				\label{tbl:PASSIONDatasetDistrAnalysis}
			\end{table}
		\end{appendices}
		
		
		
			
		\glsaddallunused                                % add all unused items to glossary
		\todo{check the gls all unused.}
		
	
\end{document}
